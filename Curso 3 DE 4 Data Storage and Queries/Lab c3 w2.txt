En el laboratorio, implementará un almacén de datos con una arquitectura tipo medallón con Amazon S3, Apache Iceberg y AWS Lake Formation. Se le proporciona un depósito de S3, etiquetado como depósito de lago de datos, que actuará como almacenamiento subyacente para la implementación de Data Lakehouse. El bucket S3 está registrado en Lake Formation, por lo que puede explorar cómo puede establecer permisos de gobierno y detallados para los datos almacenados en el lago de datos mediante Lake Formation. También cuenta con dos sistemas fuente: una base de datos MySQL de RDS que contiene los datos de los modelos clásicos con los que ha trabajado en el curso 1 y un bucket de S3 que contiene un archivo JSON con las valoraciones de los clientes de los productos que se encuentran en los datos de los modelos clásicos. Primero, llevará los datos sin procesar de ambas fuentes a la zona de aterrizaje del depósito del lago de datos. A continuación, procesará cada conjunto de datos, aplicará el cumplimiento del esquema y, a continuación, almacenará los datos procesados en la zona seleccionada del depósito del lago de datos. Por último, usará los datos procesados para crear tablas adicionales que incluyan la información necesaria para sus usuarios finales de análisis y aprendizaje automático, y almacenará estas tablas en la zona de presentación del depósito del lago de datos. También se le proporcionan dos bases de datos en el catálogo de GluData, bases de datos de zonas seleccionadas y bases de datos de zonas de presentación, cada una de las cuales contiene algunas tablas vacías. Para cada objeto que cree en las zonas seleccionadas y de presentación del bucket del lago de datos, lo asociará a una tabla de la base de datos del catálogo correspondiente para que los datos se puedan consultar con Amazon Athena. También elegirás el formato iceberg para algunos de los datos almacenados en la zona seleccionada y todos los datos almacenados en la zona de presentación y, a continuación, explorarás la evolución del esquema y las características de viaje en el tiempo de este formato de tabla abierta. En este vídeo, le proporcionaré una descripción general del formato de datos en cada zona de la casa del lago de datos. Luego, en el siguiente vídeo, analizaré el formato iceberg y la función de gobernanza del servicio de formación de lagos que utilizarás en el laboratorio. Empecemos con los datos sin procesar que ingresarás en la zona de aterrizaje. De la base de datos fuente de MySQL, extraerá cada tabla y la guardará como un archivo CSV en la zona de destino. Esto significa que creará ocho archivos CSV, donde cada archivo corresponde a una de estas tablas y tiene la clave S3 que comienza con la zona de destino, luego RDS y, por último, el nombre de la tabla. Del bucket de origen, extraerás las clasificaciones de los archivos JSON, que consisten en una lista de objetos JSON. Cada objeto contiene el número de cliente, el código del producto y la valoración del producto asignada por este cliente al producto en cuestión. Introducirás el archivo JSON como un marco de datos, añadirás el campo ingest underscore TS, que representa una marca de tiempo en la que el archivo se ingirió en el bucket del lago de datos y, finalmente, almacenarás el marco de datos como un archivo JSON en la zona de destino mediante una clave S3 que comienza con la zona de destino , luego con JSON y, por último, con las clasificaciones. Después de transferir los datos sin procesar a la zona de aterrizaje, extraerá los datos, realizará tres transformaciones en ellos y, a continuación, almacenará los datos procesados en la zona seleccionada. En la primera transformación, te centrarás en procesar los archivos CSV. Extraerás las ocho tablas de la zona de aterrizaje y las convertirás en un marco de datos. Luego, para cada tabla, agregará dos columnas de metadatos: ingest underscore TS, que representa la marca de tiempo en la que se ingirieron los datos en la zona seleccionada, y source, que representa el nombre de la base de datos fuente. También se le proporciona un esquema que deben seguir estas tablas de procesos en la zona seleccionada, en el que se describe el nombre y el tipo de cada columna de cada tabla. Por lo tanto, aplicará el esquema convirtiendo las columnas numéricas en el tipo esperado. Y, por último, almacenará cada tabla como un archivo de parquet utilizando Snappy como algoritmo de compresión y especificará la clave S3 que comienza con la zona seleccionada y, a continuación, el nombre de la tabla. Para cada tabla que crees en la zona seleccionada, escribirás sus metadatos en la tabla de catálogo correspondiente de la base de datos de Glue Catalog etiquetada como zona seleccionada. En la segunda transformación, se centrará en preparar los datos que necesita el equipo de aprendizaje automático mediante la combinación de los datos de calificaciones más recientes con la información de clientes y productos de las tablas CSV. Desde la zona de destino, extraerás las tablas CSV y las calificaciones más recientes del archivo JSON y convertirlas en un marco de datos. A continuación, crearás un nuevo marco de datos que contenga parte de la información de los clientes de la tabla de clientes, parte de la información de los productos de la tabla de productos y las valoraciones de los datos de valoraciones. Añadirá una columna adicional a este marco de datos que contenga la marca de tiempo en la que se procesaron los datos y, por último, almacenará el marco de datos con el formato Iceberg en la zona seleccionada del bucket de Data Lake especificando la clave S3 que comienza con la zona seleccionada, luego las calificaciones de ML y, por último, Iceberg. En el siguiente vídeo, repasaré el formato Iceberg y lo que puede esperar en este camino. También asociará los datos procesados a su tabla de catálogo correspondiente en la base de datos de zonas seleccionadas de Glue Data Catalog. En la tercera transformación, te centrarás solo en extraer las calificaciones más recientes y almacenarlas en formato Iceberg en la zona seleccionada del bucket de Data Lake con la clave S3 que comienza con la zona seleccionada, luego con las calificaciones y, por último, con Iceberg. Si la tabla de valoraciones de la zona seleccionada ya contiene las valoraciones asignadas por un cliente a un producto, con los nuevos datos que ingieras, extraerás los nuevos pares de clientes y productos y actualizarás las valoraciones si el par ya existe. También asociará los datos de clasificación a su tabla de catálogo en la base de datos de zonas seleccionadas del catálogo de datos de Glue. Llevará a cabo estas transformaciones con Glue ETL definiendo los trabajos de ETL para las zonas de aterrizaje y seleccionadas mediante Terraform. No repasaré todos los detalles de los archivos de Terraform proporcionados, pero destacaré algunos componentes. Para obtener una explicación más detallada de los archivos de Terraform, puede volver a visitar los vídeos explicativos del sencillo laboratorio de Data Lake de la lección 1. En los archivos de Terraform, encontrarás tres módulos. Los módulos Landing ETL y Transform ETL contienen los archivos de configuración para los trabajos ETL correspondientes a las zonas de destino y seleccionadas. Utilizará la tabla de modificación de la parte opcional del laboratorio para explorar la evolución del esquema del formato Iceberg. Por ahora, me centraré en estos dos módulos. En el módulo Landing ETL, si compruebas el archivo de configuración de Glue, encontrarás las tareas de Glue que introducen los datos sin procesar en la zona de destino del depósito de Data Lake. También encontrará un bloque que define la información necesaria para conectarse a la base de datos de RDS. En el archivo de configuración de roles de IAM, hay un bloque de datos para el rol que asumirán todos los trabajos de Glue. Tenga en cuenta que se trata de un bloque de datos y esto se debe a que la función Glue ya se ha creado y se le ha proporcionado. En el módulo Transform ETL, encontrarás el archivo de configuración de las tres tareas de Glue que utilizarás para procesar los datos sin procesar de la zona de destino y almacenarlos en la zona seleccionada. En la carpeta Assets, puede encontrar la carpeta que contiene los scripts de Python para cada transformación. Cada script contiene tres partes: extraer los datos de la fuente, procesar los datos y, a continuación, almacenar los datos en el destino de destino. No dudes en hojear estos archivos para obtener una descripción general rápida de cómo se codifica cada transformación. Y en el laboratorio, se le pedirá que complete una pequeña parte del código de estos scripts de Python. El primer archivo, batchTransform.py, corresponde a la tarea de procesar los archivos CSV de la zona de destino, aplicar el esquema a cada tabla y, a continuación, almacenar los datos procesados como archivos de parquet en la zona seleccionada. Por lo tanto, en este archivo, puede encontrar la definición del esquema de cada tabla, definida como un tipo de estructura, cada una de las cuales contiene una lista de campos de estructura que corresponden al nombre de la columna, al tipo de columna y a si la columna puede contener valores nulos o no. Spark proporciona estos objetos de tipo y campo de estructura y los utilizarás para aplicar el esquema a los datos procesados. Aprenderás más sobre Spark en el próximo curso. En los otros dos scripts, extraerá los datos sin procesar de los archivos JSON y los transformará para crear Ratings4ml y RatingsData almacenados en formato Iceberg. Encontrará el conjunto de ajustes de configuración que se utilizan para habilitar Iceberg para AWS Glue. Por lo tanto, al establecer estas configuraciones, puede usar AWS Glue para realizar operaciones de lectura y escritura en tablas Iceberg en Amazon S3, o trabajar con tablas Iceberg mediante el catálogo de datos de AWS Glue. He incluido un enlace en la sección de recursos que incluye más información sobre estas configuraciones. Cuando tengas todos los datos procesados listos en la zona seleccionada, crearás tablas adicionales en la zona de presentación que se compartirán con tus usuarios finales. Para tus usuarios finales de análisis, crearás dos tablas: una que contenga las ventas medias agrupadas por año y mes, y otra que contenga las valoraciones medias por producto. También crearás una tabla adicional que represente la tabla de calificaciones que creaste en la zona seleccionada. Y para los usuarios finales de aprendizaje automático, crearás una tabla que represente la tabla Ratings4ml que creaste en la zona seleccionada. Si bien puedes usar Glue ETL para crear estas tablas en la zona de presentación, utilizarás Athena para crear estas tablas. Como ha asociado tablas de catálogo a los datos de la zona seleccionada, también puede utilizar Athena como herramienta de procesamiento. Este es un ejemplo de la consulta SQL que puede usar con Athena para crear la tabla de calificaciones en la zona de presentación. Empieza la consulta con las palabras clave para crear tablas y, a continuación, elige un nombre para la tabla, que yo llamo valoraciones. Luego, dentro de esta instrucción WITH, especificas el formato de la tabla que deseas crear, que es Iceberg, y la ruta a la zona de presentación en el bucket del lago de datos. A continuación, fuera de los paréntesis, escriba AS y, a continuación, la consulta cuyos resultados desea almacenar en la tabla de presentación. Así que aquí seleccioné todos los registros de la tabla de clasificación de la zona seleccionada. Cuando trabaja con Athena, utiliza el nombre de la base de datos del catálogo y la tabla de catálogo que creó en el catálogo de datos de Glue, que apunta a los datos reales del bucket de S3. Después de escribir esta consulta, la pasará al método StartQueryExecution del paquete AWS Wrangler, donde también especificará el nombre de la base de datos del catálogo que desea asociar a la tabla creada, que en este caso es zona de presentación. Por lo tanto, después de ejecutar esta sentencia, se crearán los datos reales del bucket de S3 y su tabla de catálogo en el catálogo de datos de Glue. Repetirá el mismo proceso para las demás tablas y las almacenará todas en formato Iceberg en la zona de presentación. Esta fue una descripción general del formato de los datos en las zonas de aterrizaje, selección y presentación de la casa de su lago de datos. Acompáñeme en el siguiente vídeo para obtener una descripción general del formato Iceberg y otras funciones de gobernanza de Lake House.

En el vídeo anterior, vio cómo almacenará los datos de sus procesos en formato iceberg en el laboratorio. Ahora veamos el formato iceberg y veamos cómo se organizan estos archivos en el bucket de S3. Tomaré como ejemplo los datos de valoración creados en la zona seleccionada. Cuando lleves los datos de las valoraciones a la zona seleccionada, especificarás esta ruta en el bucket de S3. Si sigues comprobando el archivo que tiene el prefijo iceberg, verás dos prefijos adicionales: metadatos y datos. El prefijo de metadatos representa la capa de metadatos, que contiene los metadatos, la lista de manifiestos y los archivos de manifiesto de los que hablamos en un vídeo anterior. Por otro lado, el prefijo de datos representa la capa de almacenamiento, que contiene los datos reales almacenados en parquet. Este es el contenido de la capa de metadatos justo después de crear los datos. El archivo JSON representa el archivo de metadatos, que contiene información sobre el esquema de la tabla, la ubicación de la tabla tal como está almacenada en S3, la fecha y la hora de la última actualización de la tabla y el UUID de la instantánea actual, que se crea cada vez que se actualiza el contenido de la tabla. Cada vez que realices cambios en los metadatos de la tabla, se creará otro archivo de metadatos. Este archivo Avro que comienza con STAP es otro archivo de metadatos que representa el archivo de lista de manifiestos correspondiente a una instantánea. Apunta a la lista de archivos de manifiesto que contiene metadatos detallados sobre la instantánea. Este archivo de lista de manifiestos apunta a este archivo de manifiesto, que es otro archivo de Avro. Y este es el contenido de la capa de almacenamiento o de datos, que contiene los archivos de datos del parquet. Recuerde que en la parte superior de las capas de metadatos y almacenamiento se encuentra la capa de catálogo, que apunta a los metadatos actuales y ayuda a identificar dónde leer o escribir los datos de una tabla determinada. En el laboratorio, esta capa de catálogo se implementa mediante Glue Data Catalog, que contiene una tabla de catálogo para cada archivo de iceberg que cree en la zona de selección y presentación. Las tablas del catálogo están organizadas en bases de datos de zonas seleccionadas y zonas de presentación, como mencioné anteriormente. En la sección opcional del laboratorio, explorará la función de evolución del esquema de Iceberg. Se le pedirá que añada una nueva columna a la tabla de calificaciones para adaptarse al cambio en el esquema de los datos de entrada del bucket de origen. Aplicarás la transformación y la terraformarás usando el tercer módulo, alterar la tabla. Cuando añada la nueva columna a las tablas de clasificación, solo se modificará el archivo de metadatos y no tendrá que volver a escribir ni actualizar ninguno de los archivos de datos. También explorarás la función de viaje en el tiempo de Iceberg y verás cómo puedes consultar tanto la versión nueva como la antigua de la tabla de clasificación. Por último, explorará cómo puede aplicar permisos detallados a su lago de datos mediante Lake Formation. Como ya mencioné, el lago de datos que se le proporcionó está registrado en Lake Formation y usted actuará como administrador del lago de datos. En las secciones de recursos he incluido enlaces a la documentación de AWS que muestran cómo configurar Lake Formation y asociarlo a su lago de datos. Lake Formation le permite hacer cumplir los permisos en dos niveles: los permisos a nivel de metadatos en los recursos del catálogo de datos, como bases de datos y tablas, y los permisos de acceso al almacenamiento en los datos subyacentes almacenados. Lake Formation le permite a usted, el administrador del lago de datos, conceder a los usuarios o roles de IAM permisos detallados en las bases de datos, tablas, columnas, filas y celdas del lago de datos. Cuando usa Lake Formation para administrar el acceso a sus datos subyacentes, proporciona acceso temporal a un motor analítico integrado, como Amazon Athena o AWS Glue, para acceder a los datos de S3. De este modo, no es necesario redactar una política de IAM detallada para conceder a los usuarios del lago de datos permisos directos para interactuar con los objetos S3 subyacentes. Sin embargo, los permisos que concedes a los usuarios de tu lago de datos que utilizan Lake Formation tienen por objeto aumentar los permisos de IAM habituales, no sustituirlos. El usuario del lago de datos aún debe estar vinculado a una política de IAM que le otorgue acceso al servicio AWS Glue, al servicio Lake Formation y a Amazon Athena. La forma en que funciona es que, con una política de IAM, se aplican permisos amplios a un usuario o rol, pero luego, con Lake Formation, se aplican permisos detallados para otorgarles acceso a objetos S3 específicos. En el laboratorio, se le proporcionan las funciones que asumen el entorno de laboratorio y los recursos de Glue, y un usuario que representa a un miembro del equipo de aprendizaje automático. Los permisos generales para estas identidades de IAM ya se han definido para usted. En el laboratorio, usará Lake Formation para otorgarles permisos detallados para las tablas del lago de datos. En particular, concederá permisos a la función asumida por el entorno de laboratorio para acceder a todas las tablas del catálogo y a los datos almacenados subyacentes. Además, concederás al usuario de aprendizaje automático permisos para acceder únicamente a la tabla de calificaciones para ML desde la zona de presentación. Y, a continuación, comprobarás que no pueden acceder a otras tablas. Con eso, creo que estás listo para probar el laboratorio. Este laboratorio puede ser un poco largo, así que no dudes en saltarte o simplemente hojear la sección opcional si tienes poco tiempo. Cuando termines el laboratorio, nos vemos aquí para hacer un resumen rápido de esta semana.

