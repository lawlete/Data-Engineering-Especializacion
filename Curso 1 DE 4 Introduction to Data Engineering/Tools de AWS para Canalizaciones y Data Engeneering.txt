Ha hablado con las partes interesadas y ha recopilado los requisitos de su sistema. Ahora es el momento de traducir esos requisitos en opciones de herramientas y tecnologías para su sistema. Hay muchas maneras de combinar diferentes herramientas y servicios en la nube para obtener nuestras dos soluciones de formularios. En el proceso de convertir los requisitos del sistema en sistemas de datos reales, debe estar familiarizado con los diferentes servicios para los que se puede utilizar. Creo que Joe realmente ilustra bien este punto en su libro con el ejemplo de elegir un vehículo para viajar de un lugar a otro. Imagínese por un momento que, en lugar de un sistema de datos, se le asignara la tarea de resolver el problema de transportar a todos los miembros de su equipo de datos de un lugar a otro. ¿Cuál de estas dos opciones sería la mejor opción? En primer lugar, tiene un avión gigante que ofrece un alcance de 10 000 millas náuticas, una velocidad máxima de 1 100 kilómetros por hora, capacidad para más de 250 pasajeros y un costo de 225 millones de dólares. Y luego, tienes un Tesla modelo S con una autonomía de 360 kilómetros, una velocidad máxima de 320 kilómetros por hora. Habitación para cuatro pasajeros y un costo de alrededor de 70.000 dólares. ¿Cuál de estos vehículos elegirías para transportar a tu equipo? Bueno, está claro que es una pregunta ridícula. Por supuesto, necesitaría tener una idea clara de sus requisitos. En este caso, por ejemplo, ¿adónde tienes que ir? Si va de Nueva York a París, tal vez necesites el avión. Además, ¿qué tamaño tiene tu equipo? Si, por el contrario, solo vas de Nueva York a Boston con cuatro personas, tal vez el Tesla funcione, pero ¿y si tienes prisa? ¿Necesitas un helicóptero o, si tienes un presupuesto ajustado, quizás cuatro bicicletas? Bueno, creo que entiendes el punto. Además de los requisitos de su sistema, debe estar familiarizado con las tecnologías entre las que puede elegir , para qué se pueden usar y cómo puede combinarlas para hacer lo que necesita hacer. Así que para el sistema de datos con el que trabajarás esta semana, parece que habrá un componente por lotes y un componente de streaming. En este vídeo, explicaré los aspectos básicos de algunos de los servicios de AWS que pueden admitir cargas de trabajo por lotes en AWS y, a continuación, en el siguiente vídeo, analizaremos los servicios de AWS que pueden admitir cargas de trabajo de streaming. Como vio la semana pasada, un enfoque común para el procesamiento de datos por lotes es lo que se conoce como canalización de extracción, transformación, carga o ETL. En el ejercicio de esta semana, tendrá que ingerir datos de un sistema fuente, aplicar algunas transformaciones para que tengan el formato que necesitan los científicos de datos y, a continuación, cargarlos en el almacenamiento y proporcionarles acceso a ellos. Entonces, en este caso, el paradigma ETL parece encajar. Los datos que vas a ingerir son datos tabulares. Supongamos que el sistema de origen con el que trabajará es Amazon Relational Database Service o ARDS para abreviar. Como mencionó Joe anteriormente en el curso, las bases de datos son un sistema fuente muy común con el que trabajarás como ingeniero de datos. Y, en general, no tienes el control del sistema fuente. Por lo tanto, no dedicaremos tiempo aquí a comparar diferentes sistemas de fuentes para datos tabulares. En su lugar, asumiremos que las elecciones que debe tomar se encuentran en los componentes posteriores. Para los próximos pasos de la ingestión y la transformación, hay varios enfoques diferentes que puede adoptar. Por ejemplo, puede simplemente activar una instancia EC2 y escribir varios scripts para conectarse a la base de datos. Ingiera y transforme los datos y, a continuación, envíelos a algún lugar de almacenamiento. Sin duda, esto podría funcionar, pero tenga en cuenta lo que dijo Joe acerca de evitar levantar objetos pesados de forma indiferenciada. Con un enfoque basado en EC 2, usted sería responsable de instalar el software, administrar la seguridad y toda la complejidad que conlleva la implementación de un servidor en la nube. Por lo tanto, en este caso, crear tu propia solución personalizada para una canalización ETL relativamente sencilla podría no ser el mejor uso de tu tiempo. Al comparar las opciones de servicios sin servidor, servidores tradicionales y contenedores, la recomendación de Joe fue considerar primero las herramientas sin servidor y, luego, si no puede obtener lo que necesita, considerar las opciones de contenedor y servidor. Estoy de acuerdo con ese consejo. Así que echemos un vistazo a las opciones sin servidor que tiene en este caso. AWS Lambda fue una de las primeras y sigue siendo una de las herramientas sin servidor más populares de AWS. Con las funciones de Lambda, puede hacer que el código se ejecute en respuesta a un desencadenante o un evento y, para su canalización de ETL de esta semana, sin duda podría considerar la posibilidad de utilizar una función de Lambda para extraer datos del sistema de origen, aplicar transformaciones y enviarlos al almacenamiento. Sin embargo, las funciones de Lambda tienen limitaciones. Cosas como un tiempo de espera de 15 minutos para cada llamada a una función y las limitaciones en la asignación de memoria y CPU para cada función, entre otras, lo que puede significar que tengas que dividir tu tarea y hacer partes más pequeñas para mantenerte dentro de estas limitaciones. Más allá de eso, escribir funciones de Lambda requiere que escribas código personalizado para tu caso de uso, lo que en este caso, una vez más, podría no ser el mejor uso de tu tiempo. En cuanto a las herramientas sin servidor que son específicas para el procesamiento de datos por lotes, hay dos servicios de los que me gustaría hablar. Se trata de Amazon Glue ETL y Amazon EMR sin servidor. En cierto sentido, se podría decir que existe una gran cantidad de superposición entre las cosas para las que se pueden utilizar estos dos servicios. Y, en realidad, puede depender de los matices de su proyecto específico los que determinen qué herramienta puede satisfacer mejor sus necesidades. Sin embargo, en un nivel alto, la diferencia entre los dos se reduce a las ventajas y desventajas entre el control y la comodidad. En pocas palabras, EMR serverless le brinda más control sobre lo que puede hacer, mientras que Glue ETL brinda una experiencia más conveniente. Pero vamos a profundizar en los detalles para que puedas ver lo que quiero decir con eso. EMR se diseñó como una herramienta de big data que admite una amplia gama de marcos como Apache Spark y Apache HIVE. Por lo tanto, si forma parte de un equipo que realiza análisis a escala de petabytes, tal vez utilice Hadoop o necesite la flexibilidad necesaria para incorporar sus propios componentes personalizados, EMR o EMR sin servidor podría ser la elección correcta. Glue ETL también puede gestionar cargas de trabajo de big data, pero las ventajas reales son las funciones adicionales a las que tiene acceso. Por ejemplo, cuando te conectas a un sistema de origen, Glue usa algo llamado rastreadores, que descubren y clasifican automáticamente los datos y crean metadatos en el proceso, incluidos elementos como definiciones de tablas y esquemas. Luego, estos metadatos se utilizan para rellenar el catálogo de datos de Glue, que es un repositorio central que contiene información sobre todos sus activos de datos. Con esta información sobre sus datos ahora en su catálogo de datos, puede usar la herramienta ETL de Glue Visual para diseñar su canalización mediante una interfaz gráfica en la consola de administración de AWS que generará automáticamente el código de Spark que necesita para ejecutar en su canalización. Cuando ejecute su canalización, los servidores mantendrán el catálogo de datos de Glue para realizar un seguimiento de las transformaciones que aplique, y ese catálogo se puede utilizar posteriormente para integrarse más fácilmente con otros servicios de AWS. Hay otras opciones que podría considerar para crear la parte de ingestión y transformación de su canalización de ETL en AWS, pero esas son las principales opciones que quería compartir con usted. En el elemento de lectura que sigue a este vídeo, encontrarás más detalles sobre los servicios que hemos mencionado aquí, así como sobre otros. En lo que respecta a los aspectos de almacenamiento y entrega de la canalización de lotes con la que estás trabajando esta semana, tu elección dependerá principalmente del caso de uso posterior que estés atendiendo. Por ejemplo, si ingiere datos tabulares normalizados y luego aplica transformaciones para modelarlos en un esquema en estrella para el análisis, una opción que podría elegir sería almacenarlos y distribuirlos en otra instancia de RDS. O si desea ejecutar consultas analíticas complejas en conjuntos de datos masivos y aprovechar otras funciones que ofrecen los almacenes de datos, puede optar por almacenar y servir los datos en Amazon Redshift. Se trata de una potente solución de almacenamiento de datos, aunque a un costo significativamente mayor que el de RDS. Esta semana, presentarás un caso práctico de aprendizaje automático, en el que los datos se utilizarán para entrenar un modelo de recomendación. Cuando su consumidor de datos intermedio es otro profesional de datos técnicos que planea manipular los datos e incorporarlos en sus propios sistemas, con frecuencia, la mejor y más económica opción de almacenamiento y servicio es el almacenamiento de objetos en Amazon S3. En el mundo real, para los sistemas de datos creados en AWS, es relativamente común que S3 sirva como una especie de área de preparación como esta, porque S3 es flexible , escalable y relativamente rentable. Le permite almacenar prácticamente cualquier tipo de datos que se integren fácilmente con otros servicios de AWS. En S3, los datos se almacenan en cubos, que son contenedores de objetos. Al crear canalizaciones de datos, es posible que tenga varios depósitos de S3 que se usen en diferentes etapas de su canalización, según la tarea en cuestión. Bueno, podría decir mucho más sobre todos los servicios que hemos analizado aquí. Pero en este momento, creo que tiene lo que necesita en términos de información para el ejercicio de esta semana. En el siguiente vídeo, veremos las herramientas de streaming. Te veré allí.

En el último vídeo, se enteró de algunos de los servicios de AWS entre los que puede elegir cuando se trata de crear canalizaciones por lotes. Esta semana, en el laboratorio, además de un componente por lotes, su sistema también tendrá un componente de transmisión. En este vídeo, analizaremos los servicios de streaming en AWS. Como ha aprendido de Joe, los datos de streaming pueden provenir de varias fuentes diferentes. Pueden ser dispositivos de IoT o datos de transmisión de clics desde un sitio web o una aplicación móvil. Su fuente de transmisión podría ser incluso una base de datos, en el sentido de que podría transmitir de forma continua cualquier cambio o actualización de los datos de la base de datos a través de un proceso llamado captura de datos de cambios o CDC. Y al igual que ocurre con el procesamiento de datos por lotes, una forma en la que podría ingerir datos de streaming consistiría en activar una instancia EC2 y escribir algunos scripts personalizados para ejecutar el CDC o conectarse a cualquier otra fuente de streaming. A continuación, transforme los datos y configúrelos en sentido descendente hasta donde tengan que ir. Al igual que con el procesamiento por lotes, este enfoque basado en EC2 significa que usted será responsable de instalar el software, administrar la seguridad y toda la complejidad que conlleva la implementación de un servidor en la nube. Siguiendo las mismas ideas que analizamos con el procesamiento por lotes, también puedes considerar las funciones de Lambda como una opción sin servidor para crear tu propio sistema de streaming. Pero, una vez más, esto implica escribir código personalizado para cualquier cosa que necesite hacer y las posibles limitaciones de Lambda para su caso de uso particular. Además, interactuar con las fuentes de streaming puede ser más complejo que ejecutar cargas de trabajo por lotes. Puedo decir que probablemente no deberías crear tus propias soluciones de streaming personalizadas con EC2 o Lambda. A menos que esté 100% seguro de que su caso de uso no se puede resolver con las herramientas administradas o de código abierto existentes. Veamos algunos de los servicios de AWS que pueden admitir cargas de trabajo de streaming. El primero es Amazon Kinesis Data Streams. Se trata de un popular servicio de AWS que permite la ingesta de datos en tiempo real. La forma en que funciona es que hay productores de datos que envían datos a Kinesis Data Streams y, por ejemplo, pueden ser datos de registro de servidores web, datos de dispositivos de IoT o datos de transmisiones de clics. Kinesis en sí misma no es exigente con el tipo de datos que envía. Es independiente de los datos. De este modo, puede enviar datos JSON, XML, estructurados o no estructurados a un flujo de datos. Los productores publican los datos en la transmisión y, a continuación, los datos se almacenan en Kinesis durante un período de tiempo configurable. El tiempo de retención predeterminado y mínimo es de 24 horas, pero también se puede ampliar. Los consumidores del flujo de datos pueden extraer entonces los datos almacenados. Varios consumidores pueden extraer los mismos datos y procesarlos de diferentes maneras. Es común que los consumidores de Kinesis cojan los datos y los coloquen en otro lugar, como un servicio de almacenamiento o un almacén de datos, o que los consumidores estén realizando algún análisis en tiempo real de los datos que pasan por la transmisión. Estos consumidores pueden ser aplicaciones de software que extraen datos de la transmisión y los procesan, y esa aplicación podría ejecutarse en servicios informáticos como EC2 o Lambda. Además de Kinesis, otra opción que tiene es Amazon Managed Streaming para Apache Kafka, también conocida como MSK. MSK es un servicio que proporciona prácticamente la misma funcionalidad que Kinesis Data Streams. Apache Kafka en sí es una plataforma de transmisión de código abierto, que es una opción popular para muchos casos de uso de transmisión diferentes. MSK es un servicio totalmente gestionado que facilita la creación y ejecución de aplicaciones que utilizan Apache Kafka para procesar datos de streaming. MSK ejecuta versiones de código abierto de Kafka, lo que resulta útil porque se admiten todas las aplicaciones, herramientas o complementos existentes de la comunidad Apache Kafka. El funcionamiento de MSK consiste en crear primero un clúster de Apache Kafka y el servicio MSK gestiona la pesada tarea de aprovisionar y operar los nodos que ejecutan Kafka por usted. Esto le permite evitar ese trabajo pesado indiferenciado y dedicar más tiempo a la lógica de su aplicación personalizada. Luego, como usuario, interactúa con lo que se denomina el plano de datos de Kafka que MSK administra para crear temas , producir y consumir datos. A continuación, los productores y consumidores de datos se conectan al clúster para enviar y recibir mensajes. Tanto Kinesis Data Streams como MSK pueden ampliarse para gestionar volúmenes de datos a nivel de petabytes procedentes de varias fuentes de datos con una latencia de milisegundos, lo que permite el procesamiento y el análisis en tiempo real. Cuando se trata de elegir entre una y otra, al igual que cuando se trata de elegir entre herramientas similares para otros aspectos de sus sistemas de datos, dependerá de su caso de uso. Pero a un alto nivel, se podría volver a pensar en esto como una compensación entre control y comodidad. Si es la primera vez que utiliza arquitecturas de transmisión de datos, se suele recomendar Kinesis por su relativa facilidad de uso y su reducción de los gastos operativos. Por otro lado, si ya dirige un clúster de Kafka o tal vez tiene experiencia técnica interna en Kafka, o si busca un mayor grado de flexibilidad y control, MSK podría ser la mejor opción. Con respecto al caso práctico de leer los datos de una transmisión y almacenarlos en otro lugar, el siguiente servicio del que quiero hablarte es Amazon Data Firehose. Para dar un poco de contexto sobre por qué existe el servicio Data Firehose en primer lugar, resulta que Kinesis fue lo primero como servicio para sistemas de transmisión de datos. AWS se dio cuenta de que muchos usuarios de Kinesis Data Streams simplemente cogían los datos de la transmisión y los almacenaban en S3 o en otro lugar. Sin embargo, para trabajar con Kinesis de esta manera, es necesario escribir un código personalizado que cree la conexión con la transmisión de datos, lea la transmisión, fragmente los datos y, a continuación, los almacene. Para facilitar todo el proceso, AWS creó el servicio Amazon Data Firehose, que se puede integrar con Kinesis Data Streams y está diseñado para permitirle obtener datos de una transmisión y almacenarlos en un destino como S3 o Redshift, o enviarlos a puntos de enlace HTTP o proveedores de servicios de terceros, como Data Dog o Splunk. La principal ventaja de Data Firehose es que te ayuda a leer más fácilmente los datos de una transmisión y moverlos al almacenamiento sin necesidad de escribir código personalizado ni crear integraciones difíciles por tu cuenta. Además de Kinesis Data Streams, Data Firehose también se integra con más de 20 fuentes de ADS para ingerir datos de streaming, como MSK y otras. Al igual que con todos los demás temas de ADS Cloud que hemos analizado en este curso hasta ahora, queda mucho por saber en lo que respecta a los recursos y servicios de streaming. Pero creo que ya tiene lo que necesita, así que es hora de ponerse manos a la obra con estos servicios. En el próximo cuestionario, determinará qué servicios usar para los componentes por lotes y de transmisión de la canalización de datos para el sistema de recomendación de productos que ha estado analizando a lo largo de este curso. Después de eso, Joe te explicará el ejercicio de laboratorio de esta semana, en el que implementarás este proceso por lotes y streaming. Diviértete y nos vemos en el próximo curso.

