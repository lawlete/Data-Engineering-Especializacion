Ahora que está familiarizado con el ciclo de vida de la ingeniería de datos y las corrientes subyacentes, es hora de echar un vistazo a cómo estos conceptos se traducen en herramientas y tecnologías en la nube de AWS. Primero diré aquí que hay otros proveedores en la nube, incluyendo Azure de Microsoft y la plataforma en la nube de Google, así como otros pequeños proveedores. Y dependiendo de dónde trabajes como ingeniero de datos, podrías estar construyendo en una de estas otras plataformas. Dicho esto, todos los conceptos que estás aprendiendo en estos cursos serán relevantes para tu trabajo, no importa en qué plataforma en la nube estés construyendo. Sólo que algunas de las herramientas y detalles de implementación podrían parecer un poco diferentes. En este momento, AWS es un proveedor de nube líder, y estamos muy contentos de asociarnos con ellos en estos cursos. Para que pueda obtener las habilidades técnicas necesarias para la ingeniería de datos utilizando las mismas herramientas y tecnologías que han sido adoptadas por miles de empresas en todo el mundo. Así que en el siguiente vídeo, Morgan Willis os presentará algunas de las herramientas con las que trabajaréis en los laboratorios a lo largo de estos cursos. Y os explicará dónde encajan esas herramientas en el ciclo de vida de la ingeniería de datos y las corrientes subyacentes. Después, os daré una orientación sobre el ejercicio de laboratorio de esta semana.

Hola de nuevo. Espero que hasta ahora haya disfrutado de la lección sobre el ciclo de vida de la ingeniería de datos y las corrientes subyacentes. Me complace compartir con ustedes cómo estos conceptos cobran vida con las herramientas y tecnologías de la nube de AWS. En este vídeo, analizaré cada una de las etapas del ciclo de vida de la ingeniería de datos, así como las corrientes subyacentes que Joe ha presentado hasta ahora, y las conectaré a estas herramientas de nube específicas que utilizará para crear sus sistemas de datos. En lo que respecta a los sistemas fuente en AWS, como mencionó Joe anteriormente, los sistemas fuente más comunes con los que interactuará son las bases de datos. Por eso, en varios laboratorios de estos cursos, incluido el de esta semana, trabajará con Amazon Relational Database Service, o RDS, como se llama. RDS es un servicio que aprovisiona instancias de bases de datos con el motor de base de datos relacional de su elección, como MySQL o PostgresSQL. El RDS simplifica la sobrecarga operativa que implica el aprovisionamiento y el alojamiento de una base de datos relacional, además de encargarse de tareas como la aplicación de parches y la actualización. En el próximo curso, también trabajará con Amazon DynamoDB, que es una opción de base de datos NoSQL sin servidor. Con DynamoDB, puede crear tablas independientes y el tamaño total de estas tablas es prácticamente ilimitado en todos los elementos de la tabla. DynamoDB tiene un esquema flexible y es ideal para aplicaciones que requieren un acceso de baja latencia a grandes volúmenes de datos, como juegos, IoT, aplicaciones móviles y análisis en tiempo real, donde el modelo de datos puede evolucionar con el tiempo sin necesidad de migraciones complejas. En cuanto a las fuentes de streaming, en la última semana de este curso, tendrá la oportunidad de trabajar con Amazon Kinesis Data Streams, que se configura como un sistema fuente que transmite las actividades de los usuarios en tiempo real desde un blog de una plataforma de ventas. Y aunque no utilizará las colas de mensajes como fuentes en los laboratorios, es posible que utilice algo como Amazon Simple Queue Service (SQS) para gestionar los mensajes al crear sus propias canalizaciones de datos fuera de estos cursos. Otra opción popular para los sistemas fuente de streaming es Apache Kafka, que es una plataforma de streaming de código abierto que puede implementar por su cuenta o utilizar el servicio Managed Streaming for Kafka (MSK) de Amazon, que facilita la ejecución de las cargas de trabajo de Kafka en AWS. Porque la infraestructura subyacente se administra por usted. No usarás Kafka en los laboratorios, pero aprenderás los detalles de Kafka en el segundo curso. En lo que respecta a la ingestión, si lo haces desde una base de datos, puedes utilizar el servicio de migración de bases de datos de Amazon, también conocido como DMS. Con DMS, puede migrar y replicar datos de una fuente a un destino de forma automatizada. Sin embargo, para los laboratorios de estos cursos, utilizará principalmente el servicio ETL de AWS Glue, que ofrece funciones que respaldan los procesos de integración de datos. Y cuando se trata de ingerir datos de una fuente de streaming, utilizará Amazon Kinesis Data Streams y Amazon Data Firehose en los laboratorios. Pero en la naturaleza, puedes usar una de las otras herramientas de ingestión de streaming que mencioné anteriormente, como SQS, Kafka u otras. Con el almacenamiento en la nube, practicará el uso de las opciones tradicionales de almacenamiento de datos, incluido Amazon Redshift, así como el almacenamiento de objetos para un lago de datos en Amazon Simple Storage Service, que se conoce como S3 para abreviar. También analizaremos cómo puede combinar los servicios en lo que se conoce como una disposición independiente para acceder sin problemas tanto a los datos estructurados de su almacén de datos como a los datos no estructurados de un lago de datos de almacenamiento de objetos. Para la etapa de transformación de estos cursos, trabajará con AWS Glue, Apache Spark y DBT, que son herramientas que puede usar en combinación con Glue o como alternativas, según sus necesidades. En lo que respecta al suministro de datos, analizaremos los dos casos de uso principales posibles: primero, el caso de uso de inteligencia empresarial o análisis y, segundo, un caso de uso de inteligencia artificial o aprendizaje automático. Para el análisis, utilizará herramientas como Amazon, Athena o Redshift para consultar datos estructurados y no estructurados. También adquirirás algo de experiencia trabajando con un panel de control en un Jupyter Notebook en el laboratorio de esta semana. Y según la empresa y el equipo con los que trabaje, también puede utilizar herramientas de panel como Amazon QuickSight, Apache Superset y Metabase, ambas opciones de código abierto. Para los casos de uso de inteligencia artificial y aprendizaje automático, proporcionarás datos por lotes para el entrenamiento de modelos y trabajarás con algunas opciones de bases de datos vectoriales para entregar datos a los recomendantes de productos y utilizarlos con modelos de lenguaje de gran tamaño. Es importante tener en cuenta que para cada etapa del ciclo de vida de la ingeniería de datos, existen muchas otras opciones de servicios gestionados y de código abierto. Pero quería mencionar aquí algunos conceptos específicos para que puedas empezar a conectar algunos de los conceptos que has estado aprendiendo con las herramientas y tecnologías con las que practicarás en estos cursos. En el siguiente vídeo, relacionaré cada una de las corrientes subyacentes del ciclo de vida de la ingeniería de datos con los conceptos y las tecnologías de AWS. Nos vemos allí.

Las corrientes subyacentes del ciclo de vida de la ingeniería de datos que ha estado analizando esta semana son la seguridad, la administración de datos, las operaciones de datos, la arquitectura de datos, la orquestación y la ingeniería de software. En lo que respecta a la forma en que aparecen estas corrientes subyacentes en AWS, hay algunos aspectos que son más conceptuales y otros que están más orientados a las herramientas. Por ejemplo, la seguridad como concepto aparecerá de muchas formas en su trabajo en AWS. Mientras que algo como la orquestación consiste más en elegir la herramienta o el servicio adecuados para satisfacer sus necesidades. Por eso, en este vídeo, hablaré un poco sobre cada corriente subyacente en relación con su trabajo en AWS para que pueda empezar a conectar estas ideas. Empecemos por la seguridad. La semana pasada mencioné el modelo de responsabilidad compartida en lo que respecta a la seguridad de los sistemas de datos u otras aplicaciones creadas en la nube de AWS. En resumen, el modelo de responsabilidad compartida establece que AWS es responsable de la seguridad de los centros de datos y los servicios que brindan. Y usted es responsable de la seguridad de los sistemas que construye con esos recursos. Por ejemplo, si almacena sus datos en Amazon S3, es responsabilidad de AWS asegurarse de que el propio servicio S3 sea seguro. Además, es su responsabilidad asegurarse de que los permisos de acceso estén configurados correctamente para que sus datos solo estén disponibles para las personas y las aplicaciones que deberían tener acceso a ellos. Hablando de permisos de acceso, un concepto de seguridad clave en AWS es lo que se conoce como administración de identidades y accesos o IAM. A través de IAM, puede configurar funciones y permisos que controlen el acceso a los recursos de AWS, garantizando que los usuarios y los servicios tengan el acceso necesario para realizar sus tareas de forma segura. Dentro de sus canalizaciones de datos, utilizará las funciones de IAM, que otorgan a los usuarios o las aplicaciones acceso a credenciales temporales que rotan automáticamente y proporcionan los permisos de API de AWS adecuados para varias herramientas o áreas de almacenamiento de datos. La seguridad de la red también es crucial para la seguridad en AWS. Además, necesitará estar familiarizado con servicios y funciones como Amazon Virtual Private Cloud o VPC, así como con los grupos de seguridad, que son firewalls a nivel de instancia que son otro aspecto clave de la implementación de canalizaciones de datos seguras. Con el trasfondo de la administración de datos en estos cursos, utilizará los rastreadores de AWS Glue y los catálogos de datos de Glue, que le permiten descubrir, crear y administrar metadatos para los datos almacenados en Amazon S3 u otros sistemas de almacenamiento y bases de datos. También se familiarizará con Lake Formation, que le ayuda a administrar y escalar de manera centralizada los permisos de acceso a datos detallados. Todos ellos también están relacionados con la seguridad, pero los incluyo aquí en la sección Administración de datos porque tienen que ver específicamente con la privacidad y el descubrimiento de datos. Para las operaciones de datos, en el próximo curso, trabajará con un servicio llamado Amazon CloudWatch que recopila métricas y proporciona funciones de supervisión para los recursos de la nube, las aplicaciones e incluso los recursos locales. También está Amazon CloudWatch Logs, que puede ayudarlo a almacenar y analizar los registros operativos. En el segundo curso, también trabajará con Amazon Simple Notification Service, o SNS, que proporciona un medio para configurar las notificaciones, ya sea entre aplicaciones o por mensaje de texto o correo electrónico, que se activan a partir de eventos dentro de su sistema. También hay muchas herramientas de observabilidad de código abierto que puedes usar en tu propio trabajo, como Monte Carlo o BigEye. Con la orquestación en estos cursos, trabajará con Airflow, que es una herramienta de orquestación que puede implementar como una herramienta de código abierto o usar una versión administrada de AWS. Airflow es actualmente el estándar de la industria, pero debes tener en cuenta las herramientas de orquestación más nuevas, como Dagster, Prefect y Mage, que tienen como objetivo resolver algunos de los problemas para los que Airflow no está diseñado. En lo que respecta a la arquitectura, analizaremos el marco de buena arquitectura de AWS la semana que viene, que es un conjunto de principios y prácticas desarrollados por AWS que pueden ayudarlo a crear sistemas con miras a la eficiencia operativa, la seguridad, la escalabilidad y la sostenibilidad. Y con respecto a la ingeniería de software, puede usar herramientas como CodeDeploy de Amazon, que le permite automatizar la implementación del código, así como varias herramientas de CI-CD. También te encargarás del control de versiones con Git y GitHub. Esta es una descripción general rápida de algunas de las herramientas que utilizará en relación con las corrientes subyacentes del ciclo de vida de la ingeniería de datos. A continuación, Joe lo guiará a través del primer ejercicio en vivo en el que pondrá en marcha una canalización de datos de extremo a extremo en AWS.

