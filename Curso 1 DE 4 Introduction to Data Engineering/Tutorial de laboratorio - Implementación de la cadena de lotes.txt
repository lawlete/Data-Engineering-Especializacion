Ahora que ha visto la conversación con el científico de datos y ha explorado sus opciones de arquitectura en los cuestionarios, es el momento de implementar las arquitecturas por lotes y de streaming para el sistema de recomendación. En primer lugar, implementará la canalización por lotes para servir datos de entrenamiento a los científicos de datos para entrenar un sistema de recomendación. A continuación, configurará una base de datos vectorial para almacenar las incrustaciones de salida del sistema de recomendación. Y, por último, implementará la tubería de streaming que utiliza el sistema de recomendación entrenado y la base de datos de vectores para emitir recomendaciones de productos para un usuario dadas sus actividades de navegación en línea. El laboratorio contiene instrucciones detalladas que muestran cómo crear los recursos utilizando Terraform y cómo interactuar con ellos en el terminal de línea de comandos. No se preocupe si no entiende completamente los detalles. El objetivo principal de este laboratorio es ayudarle a familiarizarse con los componentes por lotes y de streaming de una canalización de datos en AWS. Aquí tiene el diagrama arquitectónico de la canalización por lotes. La primera sección del laboratorio se centra en esta parte de la arquitectura, que transforma los datos y los prepara para su uso en la formación. De forma similar al laboratorio que viste en la semana 2, aquí se te proporciona una base de datos RDS MySQL que contiene el conjunto de datos de modelos clásicos y una tabla adicional que consiste en las valoraciones asignadas por los usuarios a los productos que compraron. Estas valoraciones representarán las etiquetas para el conjunto de datos de entrenamiento y el sistema recomendador se entrenará utilizando un modelo de aprendizaje automático supervisado entre bastidores. Para preparar los datos para la fase de entrenamiento, utilizarás AWS Glue ETL para ingerir los datos de la base de datos MySQL y luego transformar los datos en esta forma. Por último, almacenarás los datos transformados en este bucket S3 etiquetado como lago de datos. Crearás el Glue ETL y el bucket S3 utilizando Terraform. Aquí ya inicié el laboratorio y abrí las instrucciones del laboratorio. Comencemos explorando la tabla de calificaciones proporcionada en la base de datos fuente. Para conectarnos a la base de datos fuente, necesitamos conocer su endpoint. Copiaré este comando de las instrucciones del laboratorio y lo pegaré en la terminal aquí, reemplazando mysqldbname por dec1w4rds y luego ejecutando el comando. Esto devolverá el endpoint de la base de datos. A continuación, me conectaré a la base de datos utilizando el comando mysql. El host es el endpoint que acabo de obtener del comando anterior. El nombre de usuario de la base de datos es admin y la contraseña es adminpwrd. Ahora que la conexión está establecida, elegiré la base de datos de modelos clásicos para ver las tablas que hay dentro. Puedes ver que hay una tabla de valoraciones adicional. Veamos el contenido dentro de esta tabla ejecutando esta consulta, que devuelve las primeras 20 filas de las tablas. Cada fila consta del número de cliente, el código de producto y la valoración del producto. Una vez que hayas terminado de explorar la base de datos, puedes escribir exit para salir de la conexión con la base de datos. Aprenderá más sobre la conexión y el trabajo con bases de datos en el curso 2. En la siguiente parte del laboratorio, utilizaré Terraform para crear los recursos para el batch pipeline. Primero, ejecutaré el script etiquetado setup.sh, que incluye los comandos shell para definir algunas variables de entorno para Terraform. Cambiaré mi directorio de trabajo a la carpeta Terraform. Antes de ejecutar Terraform, vamos a comprobar rápidamente la estructura de esta carpeta. A la izquierda, debajo de Terraform, haré clic en Módulos. En este laboratorio, los recursos necesarios para cada parte del laboratorio están agrupados en carpetas o módulos. Así, si hago clic en ETL, puedes ver los archivos Terraform para Glue. Cuando organices tus archivos Terraform en módulos, necesitarás declarar estos módulos en el archivo Terraform principal definido fuera de los módulos para que puedas pasar valores a cualquier variable de entrada y usar cualquier valor de salida del módulo. Dentro del archivo principal, hay una sección que declara el módulo ETL, que contiene un enlace al módulo y algunos valores que se pasan a sus variables de entrada. Descomentaré la sección eliminando el signo de número en cada fila. Asegúrate de guardar las actualizaciones en el archivo. A continuación, editaré el archivo Terraform de salida y descomentaré la sección que declara las variables de salida del módulo ETL, en concreto el ID del bucket de S3, lago de datos. Vas a obtener una comprensión más profunda de estos diversos archivos Terraform en el Curso 2. Después de guardar las actualizaciones, voy a volver a las instrucciones detalladas. En el terminal, voy a escribir terraform init, luego terraform plan, y finalmente terraform apply. Terraform siempre pedirá confirmación antes de crear los recursos. Una vez creados los recursos, puedes ver los valores de salida. Ahora, vamos a ejecutar el trabajo glue que transformará los datos en la forma deseada. Si tienes curiosidad por ver cómo es el script para este trabajo glue, puedes abrir la carpeta Terraform, hacer clic en Assets, y luego en glue underscore job para ver el script Python que contiene la lógica de transformación. Volveré a las instrucciones y copiaré este comando y lo pegaré en el terminal para iniciar el trabajo glue. Puedes comprobar el estado del trabajo glue ejecutando este comando. Sustituiré el ID de ejecución del trabajo por el ID devuelto por el comando anterior. Así, el estado actual sigue en ejecución, pero si esperas unos minutos, el estado cambiará a Succeeded. Vamos a profundizar en los detalles de AWS Glue en el Curso 4. Ahora, vamos a comprobar que el bucket de S3, data lake, contiene todos los recursos. Vamos a comprobar que el bucket de S3, data lake, contiene los datos de entrenamiento. En la consola, escribiré S3, haré clic en ese servicio, y luego elegiré el bucket que tiene data lake en su nombre. Aquí puedes ver una carpeta etiquetada como Ratings underscore ML underscore Training, que contiene carpetas adicionales, cada una asociada a un número de cliente. En el almacenamiento de objetos, esta forma de organizar los datos se llama partición, y te ayuda a localizar rápidamente la información que está relacionada con cualquier cliente. Hasta ahora, hemos transformado los datos de valoraciones en un formato que se puede utilizar para entrenar el sistema de recomendación. En el siguiente vídeo, echaremos un vistazo a la siguiente parte del laboratorio, donde configuraremos una base de datos vectorial para almacenar los resultados del modelo de recomendación.

En este tutorial en vídeo, repasaremos la sección 2 del laboratorio, donde configurarás una base de datos vectorial que almacenará los resultados del modelo de recomendación. Volvamos al diagrama arquitectónico de la canalización por lotes. Ahora tenemos los datos transformados almacenados en el depósito de S3, Data Lake, que se comparte con los científicos de datos para que puedan usarlos para entrenar el sistema de recomendación. En este laboratorio, no entrenará usted mismo el sistema de recomendación, ya que ese sería el trabajo del científico de datos, sino que se le proporcionará un modelo que ya está capacitado. La salida del modelo se comparte en el bucket de S3 etiquetado como ML Artifacts. Así que comencemos la sección 2 de este laboratorio comprobando el contenido de este depósito de S3. En la consola, volveré a la lista de depósitos disponibles y, a continuación, elegiré el depósito que tiene ML Artifacts en su nombre. Puede ver que el depósito tiene tres carpetas: Embeddings, Models y Scalars. La carpeta Embeddings contiene las incrustaciones de los usuarios y los elementos, es decir, los productos, generados por el modelo. La carpeta Modelo contiene el modelo entrenado que se utilizará para la inferencia. Y la carpeta Escalares contiene los objetos que se usaron en la parte de preprocesamiento del entrenamiento. Por ahora, centrémonos en las incrustaciones. Si haces clic en esa carpeta, verás dos archivos CSV, uno para los artículos o productos y otro para los usuarios. El modelo utilizará esas incrustaciones para encontrar qué productos recomendar a un usuario determinado. Así, por ejemplo, las incrustaciones de artículos se utilizarán para recuperar productos similares a los que un usuario ha colocado en el carrito de compras. El modelo de recomendación calculará primero los vectores de incrustación de los productos en el carrito de compras. A continuación, realizará una búsqueda de similitud sobre las incrustaciones de artículos para encontrar productos similares. El científico de datos te pide que subas los archivos CSV de incrustación de artículos y de incrustaciones de usuarios a una base de datos vectorial para que la recuperación de productos similares sea más eficiente. Así que veamos los pasos para crear la base de datos vectorial y cargar las incrustaciones. Siguiendo las instrucciones de la sección 2, abriré el archivo main.tf y descomentaré la sección que declara el módulo VectorDB. Luego, en el archivo outputs.tf, descomentaré las variables de salida de este módulo VectorDB. Cuando cree esta base de datos, se le devolverán estos archivos de salida, como el nombre de usuario, la contraseña, el host o el punto final de la base de datos, para que pueda usarlos para establecer una conexión con la base de datos. Asegúrese de guardar las actualizaciones en los archivos principal y de salida. Ahora podemos crear los recursos asociados a la base de datos vectorial. En la terminal, ejecutaré terraform init, luego terraform plan y finalmente terraform apply. Tras confirmar que desea crear estos recursos, la creación de la base de datos de PostgreSQL puede tardar unos 7 minutos. Por cierto, Terraform no sobrescribirá ningún recurso que se haya creado anteriormente. Los mantendrá y se centrará en crear los nuevos recursos. Una vez creada la base de datos, podrá encontrar el nombre de host o el punto final. Como necesito esta información para conectarme a la base de datos vectorial, copiaré este enlace y lo pegaré en una nota aparte para más adelante. También necesito la contraseña y el nombre de usuario, pero son sensibles al mercado, así que tendré que ejecutar estos comandos según las instrucciones del laboratorio y copiar la contraseña y el nombre de usuario en una nota aparte. Ahora vamos a añadir las incrustaciones a la base de datos vectorial. Haré clic en la carpeta SQL de la izquierda y, a continuación, abriré el archivo SQL. Aquí puede encontrar algunas instrucciones SQL que transfieren las incrustaciones de elementos y usuarios del bucket de MLArtifact S3 a la base de datos vectorial. En este archivo, debe especificar el nombre del bucket. Vamos a la consola, a la lista de depósitos y, a continuación, copiemos el nombre completo del depósito de MLArtifacts. Luego, en el archivo SQL, peguemos el nombre del bucket en estas dos sentencias SELECT. Asegúrese de guardar el archivo. Ahora tendrá que conectarse a la base de datos vectorial y, a continuación, ejecutar estas sentencias SQL. Para conectarme a la instancia de la base de datos, copiaré este comando de la sección 2 de las instrucciones de laboratorio y, a continuación, sustituiré el host por el nombre de host de la base de datos que guardé antes. Se te pedirá que introduzcas la contraseña, así que, de nuevo, solo tienes que utilizar la que hayas reservado. Parecerá que no escribiste nada, pero cuando presiones enter, se leerá la contraseña. Ahora, para trabajar en la base de datos particular llamada Postgres, ejecutaré este comando en la terminal. De nuevo, se te pedirá que introduzcas la contraseña, así que usa la misma de las instrucciones anteriores. Ahora, para ejecutar las sentencias SQL desde el script SQL incrustado, ejecutaré este comando en la terminal. Luego ejecutaré este comando para ver todas las tablas disponibles. Me desplazaré hacia abajo con la tecla de flecha para encontrar las tablas que contienen las incrustaciones y, a continuación, escribiré q para salir. Cuando termines, puedes cerrar la conexión escribiendo barra q. Ahora que hemos configurado la base de datos vectorial y subido las incrustaciones de modelos en ella, en el siguiente vídeo te explicaré la parte del proceso de streaming de este laboratorio, que se utilizará para pasar los datos de usuarios y productos al sistema de recomendaciones y, a continuación, pasar las recomendaciones de productos a un bucket de S3. ¡Nos vemos allí!

Como recordatorio, al final del último vídeo, habíamos entrenado un sistema de recomendación y una base de datos vectorial lista para ser utilizada en la tubería de streaming para generar y almacenar recomendaciones de productos. Aquí está el diagrama arquitectónico del flujo de trabajo de streaming. En el lado izquierdo, se puede ver una función lambda etiquetada como inferencia de modelo. Esta función lambda utilizará el modelo entrenado almacenado en S3 y los embeddings de la base de datos vectorial para proporcionar las recomendaciones basadas en la actividad online del usuario. Así es como funciona. AWS Kinesis Data Streams recibirá la actividad online del usuario desde los logs de la plataforma de ventas. Data Firehose leerá entonces los eventos desde Kinesis Data Streams y actuará como un servicio de entrega para cargar los datos en el bucket de S3 etiquetado como el bucket de recomendaciones. Antes de cargar los datos en S3, Data Firehose invocará la función lambda etiquetada como transformación de flujo para extraer las características del usuario y del producto de los flujos de datos y, a continuación, pasará estas características a la inferencia del modelo de la función lambda para obtener las recomendaciones. Data Firehose cargará finalmente las recomendaciones al bucket de recomendaciones de S3. La función lambda model inferences ya está implementada y te la hemos proporcionado, pero su configuración no está completa. Así que antes de implementar el flujo de trabajo de streaming, tendrás que seguir las instrucciones de la sección 3 del laboratorio para configurar las variables de entorno y la función lambda para que pueda conectarse a la base de datos de vectores. Para ello, necesitaremos de nuevo el host de la base de datos, el nombre de usuario y la contraseña, que ya he apartado en una nota aparte del vídeo anterior. En la consola, abriré el servicio lambda, luego elegiré la función lambda que tiene model inference en su nombre. Desplazaré hacia abajo y haré clic en la pestaña de configuración, luego a la izquierda, elegiré environmental variables y haré clic en edit. Aquí pegaré el host de la base de datos, la contraseña y el nombre de usuario que aparté previamente, y no olvides guardar estas actualizaciones. Por último, podemos seguir las instrucciones de la sección 4 para implementar el streaming pipeline. En el archivo main.tf, descomentaré esta sección final que declara un módulo de streaming. Haré lo mismo en el archivo outputs. Después, ejecutaré los tres comandos de Terraform, terraform init, terraform plan, y terraform apply. Esto debería crear el firehose y las funciones lambda de transformación de flujos. Ten en cuenta que terraform no creará los flujos de datos de Kinesis y las recomendaciones S3 bucket porque ya se te proporciona como uno de los recursos del laboratorio. Cuando inicies el laboratorio, un proceso se ejecutará en segundo plano y transmitirá automáticamente algunos eventos a los flujos de datos de Kinesis. Después de crear la tubería de transmisión, la manguera de incendios de datos comenzará a leer automáticamente los eventos de los flujos de datos de Kinesis, invocará la función lambda para transformar los datos en recomendaciones y, finalmente, transferirá estas recomendaciones al bucket de S3. Aprenderás más sobre el diseño subyacente de los flujos de datos de Kinesis en el curso 2. Para la tarea final de este laboratorio, echemos un vistazo al contenido del bucket de S3. En la consola, busque S3 y, a continuación, elija este bucket que tiene recomendaciones en su nombre. Dentro de este bucket, verá que los datos están particionados por año, mes, día y hora. De nuevo, la partición ayuda a S3 a localizar datos específicos más rápidamente. También puedes consultar los logs de las funciones lambda de transformación. En la consola, busca lambda y, a continuación, haz clic en la lambda de transformación. Haré clic en la pestaña monitor y, a continuación, en View CloudWatch Logs. Estos son los logs de la función generados mientras realizaba la transformación. Ahora te toca a ti probar este laboratorio. Asegúrate de seguir las instrucciones del laboratorio con atención y vuelve a ver estos vídeos si es necesario. Una vez que hayas completado todas las tareas del laboratorio, no olvides enviar el laboratorio en la página Instrucciones de configuración del laboratorio. Ten en cuenta que el entorno del laboratorio caducará al cabo de dos horas. De nuevo, no te preocupes si alguno de los pasos del laboratorio no te queda del todo claro. Desarrollarás un conocimiento más profundo de las herramientas en futuros cursos. Después de esto, te veré de nuevo aquí para concluir este curso.

