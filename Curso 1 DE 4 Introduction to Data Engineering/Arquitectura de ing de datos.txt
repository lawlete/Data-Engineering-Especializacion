La semana pasada, analizamos brevemente la arquitectura de datos y vimos cómo, en su trabajo como ingeniero de datos, tendrá más éxito si puede pensar como un arquitecto, incluso si el arquitecto de datos no es técnicamente su función. Esta semana, profundizaremos en lo que significa crear una buena arquitectura de datos. En primer lugar, analizaremos cómo la arquitectura de datos se ajusta al contexto más amplio de la arquitectura empresarial, es decir, la arquitectura de toda la organización. A continuación, analizaremos algunos ejemplos de arquitectura específicos y veremos cómo puede empezar a pensar en traducir las necesidades de las partes interesadas en opciones tecnológicas para sus sistemas de datos. También compartiré contigo algunos principios rectores que puedes tener en cuenta mientras aprendes a pensar como un arquitecto. En el laboratorio de esta semana, evaluará las ventajas y desventajas en lo que respecta a aspectos como el costo, el rendimiento, la escalabilidad y la seguridad de una arquitectura de datos real en la nube de AWS. También tendrá la oportunidad de explorar el marco bien diseñado de AWS, que es un conjunto de principios complementarios que pueden ayudarlo a diseñar sistemas de datos sólidos y eficaces. Al final de esta semana, dispondrá de un conjunto de herramientas que le servirán en todas las etapas de su proceso de ingeniería de datos. Acompáñame en el siguiente vídeo para empezar.

Antes de entrar en los detalles de la arquitectura de datos, me gustaría alejarme un momento y ver cómo la arquitectura de datos encaja en el contexto más amplio de lo que se denomina arquitectura empresarial. Lo cual, sin duda, puede ser un concepto un tanto vago y abstracto. En cuanto a la definición de arquitectura empresarial, no existe un consenso claro y diferentes grupos la han definido de forma diferente. Pero esta es una definición que establecimos en el libro sobre los fundamentos de la ingeniería de datos. La arquitectura empresarial es el diseño de sistemas para respaldar el cambio en una empresa, logrado mediante decisiones flexibles y reversibles tomadas a través de una evaluación cuidadosa de las compensaciones. Ahora, puede que estés pensando: oye, espera un momento, ¿no vimos ya esta definición en el contexto de la arquitectura de datos la semana pasada? Y sí, tendrías razón. De hecho, la definición de arquitectura de datos que vio la semana pasada fue que la arquitectura de datos es el diseño de sistemas para respaldar las cambiantes necesidades de datos de una empresa. Se logra mediante decisiones flexibles y reversibles tomadas a través de una evaluación cuidadosa de las compensaciones. Dada la similitud entre estas dos definiciones, puede ver que la arquitectura de datos está muy bien alineada con el contexto de la arquitectura empresarial y se ajusta a él. De hecho, la arquitectura empresarial abarca varios dominios, y se puede pensar que estos dominios incluyen cuatro áreas principales. La primera es la arquitectura empresarial, que se aplica a la estrategia de producto o servicio y al modelo de negocio de una empresa. La segunda es la arquitectura de aplicaciones, que describe la estructura y la interacción de las aplicaciones clave que satisfacen las necesidades empresariales. En tercer lugar, tiene una arquitectura técnica, que se refiere a los componentes de tecnología de software y hardware que se requieren para respaldar la implementación de sistemas y aplicaciones empresariales. Y, por último, está la arquitectura de datos, que, como ya ha visto, consiste en respaldar las cambiantes necesidades de datos de la empresa. Por lo tanto, puede pensar en la arquitectura de datos como un elemento de la arquitectura empresarial y, de esta manera, puede empezar a ver cómo su trabajo como ingeniero de datos se relaciona directamente con los objetivos de más alto nivel y la estructura de su organización. Como la estructura de una organización puede cambiar con el tiempo, esto lleva a otro concepto importante, la gestión del cambio, que es la base de la arquitectura empresarial y de datos. Puede esperar que las necesidades de su organización evolucionen constantemente, y su arquitectura de datos tendrá que adaptarse a esos cambios. A Jeff Bezos, ex director ejecutivo de Amazon, se le atribuye la idea de puertas unidireccionales y bidireccionales, ya que se aplica a cualquier decisión que se tome en una organización. Una puerta de un solo sentido es una decisión que es casi imposible de revertir. La puerta se cierra y se bloquea detrás de ti, y no hay forma de volver atrás. Como un ejemplo simple de este concepto, puedes pensarlo de esta manera, digamos que si rompes un huevo para cocinarlo, no puedes cambiar de opinión más tarde y descascarar el huevo. Así que romper un huevo es una decisión de ida y vuelta. O cuando este concepto se refiere a una organización. Amazon, por ejemplo, podría haber decidido en algún momento vender AWS y todos sus servicios en la nube, o simplemente cerrarla. Y después de tomar esa medida, sería casi imposible para Amazon reconstruir una nube pública con la misma posición en el mercado. Por otro lado, una puerta de doble sentido es una decisión fácilmente reversible, por lo que puedes entrar por la puerta y, si te gusta lo que ves, puedes continuar. O si no te gusta lo que ves, puedes volver a entrar por la puerta. Por ejemplo, cuando se trata de elegir cómo almacenar los datos en un sistema de almacenamiento de objetos como S three, puede elegir entre una variedad de clases de almacenamiento en función de sus requisitos de rendimiento, acceso a los datos y costos. Un ejemplo de una decisión bidireccional sería si optara por almacenar sus datos en la clase de almacenamiento estándar en S three. Más adelante, si tus necesidades de almacenamiento cambian, puedes hacer la transición a cualquier otra clase de almacenamiento pagando una tarifa. Por lo tanto, esta decisión es reversible. Dado que los riesgos asociados a cada decisión bidireccional suelen ser bajos, las organizaciones pueden tomar fácilmente más de este tipo de decisiones reversibles, iterando rápidamente y mejorando la forma en que recopilan, usan y almacenan los datos. La toma de decisiones flexible y reversible es fundamental para la arquitectura empresarial y de datos. Al tratar de tomar decisiones bidireccionales siempre que sea posible en su arquitectura, podrá gestionar mejor los cambios en lo que respecta a su organización. Si te encuentras ante lo que parece ser una decisión unidireccional, ve si puedes dividirla en una serie de decisiones más pequeñas en las que cada decisión individual sea una puerta bidireccional. Por lo tanto, la arquitectura no consiste simplemente en trazar un mapa de los procesos de datos o TI empresariales y mirar vagamente hacia un futuro utópico lejano. Los arquitectos deben resolver activamente los problemas empresariales y crear nuevas oportunidades. Si piensas como un arquitecto en tu papel de ingeniero de datos, crearás soluciones técnicas que existan no solo por sí mismas, sino también para apoyar directamente los objetivos empresariales. A continuación, analizaremos los principios de una buena arquitectura de datos. Pero antes de hacerlo, quiero destacar un fenómeno muy interesante conocido como ley de Conway que afecta a la arquitectura de todos los sistemas de datos. El siguiente vídeo es opcional y no voy a ponerte a prueba con la ley de Conway, así que si quieres saltarte el paso, está perfectamente bien. Pero creo que le voy a hacer un flaco favor si no menciono al menos la ley de Conway en estos cursos, porque puedo garantizar que esto influirá en los tipos de sistemas que construya.

A lo largo de estos cursos, quiero que tenga en cuenta que los diversos principios y patrones de arquitectura que discutimos se aplicarán a diferentes escenarios. Según el tipo de arquitectura y sistemas que esté creando para respaldar los objetivos de su organización. Sin embargo, hay un principio rector muy importante que ha afectado a todas las arquitecturas y sistemas que he conocido. Quiero presentarles lo que se conoce como la ley de Conways. La ley de Conway fue mejor descrita por su autor Melvin Conway, quien afirmó que cualquier organización que diseñe un sistema producirá un diseño cuya estructura sea una copia de la estructura de comunicación de la organización. Ahora, puede parecer una afirmación muy difícil de hacer, pero así es como funciona en la práctica. Imagine una empresa con cuatro departamentos diferentes: ventas, marketing, finanzas y operaciones. Ahora bien, si estos departamentos operan en silos relativamente aislados, sus patrones de comunicación también estarán aislados y aislados. Cuando se trata de crear arquitecturas y sistemas de datos, inevitablemente construirán cuatro sistemas relativamente aislados y en silos. En otras palabras, las ventas desarrollarán un sistema de datos, el marketing, construirán otro, las finanzas, un tercero, y las operaciones, otro sistema más. Entiendes la idea. Si, por el contrario, los cuatro departamentos de la misma organización se comunican de forma interfuncional, comparten ideas y colaboran entre departamentos, los sistemas de datos que creen reflejarán la cultura de la colaboración y la comunicación interfuncionales. Una vez más, me doy cuenta de que esto puede sonar extraño, pero por extraño que parezca, la ley de Conway es notablemente coherente en todos los tipos de organizaciones. La principal conclusión para usted como ingeniero de datos es que, cuando se trata de entender, ¿qué tipo de arquitectura de datos va a funcionar para su organización? Primero debe prestar atención y comprender la estructura de comunicación de la organización. Incluso si intenta construir una arquitectura de datos que choca con la estructura de comunicación de su empresa, está destinado a tener problemas. Si está interesado en obtener más información sobre la ley de Conways, he incluido un enlace en la sección de recursos al final de la semana. Acompáñeme en el siguiente vídeo para ver más de cerca los principios clave de una buena arquitectura de datos.

La semana pasada, mencioné brevemente nueve principios a tener en cuenta en tu enfoque de la arquitectura de datos. Aquí están de nuevo para refrescar tu memoria. Volveremos sobre estos principios a lo largo de la especialización. Esta semana trata sobre la arquitectura de datos. Y por eso me gustaría dedicar un poco más de tiempo a los detalles de cada uno de estos principios antes de entrar en algunas arquitecturas específicas. En cierto sentido, estos principios están todos relacionados entre sí. Pero para los propósitos de esta discusión, me gustaría dividirlos en tres grupos, que puedes ver aquí. Para mí, lo que estos dos principios del primer grupo tienen en común es que ambos se refieren a cómo la arquitectura de datos afecta a otros equipos e individuos de tu organización. El segundo grupo de principios trata sobre el hecho de que la arquitectura de datos es un proceso continuo y que puedes esperar que tu arquitectura evolucione con el tiempo. Este tercer grupo es un conjunto de prioridades tácitas pero bien entendidas que subyacen a cualquier arquitectura de datos, es decir, que tienes que estar pensando en cosas como el coste, la seguridad, la escalabilidad y los modos de fallo para cualquier sistema que construyas. Por supuesto, hay otras formas en las que podrías pensar en combinar o relacionar estos principios. Pero aquí, los discutiré en estos tres grupos. En este vídeo, hablaré sobre el primer par de principios que se relacionan con cómo la arquitectura de datos afecta a otros equipos e individuos de tu organización. En los próximos dos vídeos, veremos los otros dos grupos. Aquí he elegido componentes comunes sabiamente, y la arquitectura es liderazgo. Uno de los principales trabajos de un arquitecto de datos, y potencialmente parte de tu trabajo como ingeniero de datos es elegir componentes comunes y prácticas que puedan ser utilizadas ampliamente en toda una organización. Los componentes comunes pueden ser cualquier cosa que tenga una amplia aplicabilidad dentro de una organización, incluyendo cosas como almacenamiento de objetos, sistemas de control de versiones, observabilidad, sistemas de monitorización y orquestación, y motores de procesamiento de datos. Las plataformas en la nube son un lugar ideal para adoptar componentes comunes. Por ejemplo, la separación de almacenamiento de datos en sistemas de datos en la nube significa que puede servir datos a diferentes equipos de la organización con una capa de almacenamiento compartida que permite a los usuarios consultar los datos para su caso de uso particular. Cuando se eligen componentes comunes sabiamente, se convierten en parte del tejido de la organización. Facilitando la colaboración en equipo y rompiendo silos. Esto no quiere decir que siempre habrá componentes comunes que puedan proporcionar la solución adecuada para cada equipo. Elegir sabiamente los componentes comunes significa identificar casos de uso en los que los equipos puedan beneficiarse del uso de las mismas herramientas y prácticas de datos sin crear obstáculos a la productividad aplicando ciegamente un enfoque único para todos los sistemas de datos. Como ingeniero de datos, puede practicar el liderazgo en arquitectura identificando los componentes comunes adecuados en consulta con los miembros de su organización. A medida que te conviertas en un senior y asumas más responsabilidades, puedes ser el mentor de otros y proporcionar la formación adecuada en torno a estos componentes también. Como he dicho antes, como Ingeniero de datos, también recomiendo que busques la tutoría de arquitectos de datos en tu organización o en otro lugar porque, con el tiempo, puedes ocupar el papel de arquitecto tú mismo. Únete a mí en el siguiente vídeo para echar un vistazo al segundo grupo de principios, que tratan sobre la construcción de flexibilidad en tu arquitectura.

En un video anterior, mencioné la idea de las puertas unidireccionales y bidireccionales, donde las puertas unidireccionales representan las decisiones que se toman y que son difíciles o imposibles de revertir, mientras que las puertas bidireccionales representan decisiones reversibles. Los sistemas basados en decisiones reversibles le permiten diseñar arquitectura en todo momento. Por mencionar otro concepto que surgió de Amazon, el llamado mandato de API que llegó en forma de correo electrónico de Jeff Bezos a todos los empleados de Amazon en 2002. Puede obtener más información sobre los detalles de este mandato siguiendo el enlace de la sección de recursos al final de esta semana. Pero la esencia de este mandato era la siguiente. Todos los equipos deben usar interfaces de servicio, también conocidas como interfaces de programación de aplicaciones o API, para comunicarse y ofrecer datos y funciones. El mandato también terminó cuando Jeff Bezos dijo que quienes no siguieran este mandato serían despedidos. Se nota la seriedad con la que se tomó esta idea de las API en Amazon. Lo que esto significaba era que, independientemente del tipo de lío complicado que tuviera un equipo en particular en sus propios sistemas, tendría que proporcionar todos los datos, funciones y comunicaciones a otros equipos, a través de una interfaz de servicio estable y predecible. Esto permitió a los equipos de Amazon funcionar juntos como un sistema poco acoplado en el que los equipos individuales se conectaban entre sí a través de estas interfaces de servicio, y cualquier reconfiguración o reorganización dentro de un equipo determinado no afectaba a los demás equipos. La otra parte del mandato de la API consistía en que todas las interfaces de servicio o API tenían que crearse desde cero para que, finalmente, fueran públicas para los desarrolladores del mundo exterior. Esta reorientación hacia las interfaces de servicio sentó las bases de lo que eventualmente se convertiría en Amazon web services o AWS, que gran parte del mundo usa ahora como plataforma en la nube. En este siguiente grupo de principios, he tomado decisiones reversibles, he construido sistemas poco acoplados y siempre me he dedicado a la arquitectura. Como ya sabes, esas decisiones reversibles son puertas de doble sentido, decisiones que puedes deshacer fácilmente si no te gusta el resultado. Una forma clave de garantizar que sus decisiones sean reversibles es construir su sistema de datos a partir de componentes poco acoplados. Al estar vagamente acoplado en el contexto de la arquitectura. Me refiero a los componentes que se pueden intercambiar con relativa facilidad sin tener que rediseñar todo el sistema. Con un sistema creado a partir de componentes poco acoplados y decisiones reversibles, siempre tendrá la posibilidad de diseñar arquitecturas en todo momento. Como ya hemos mencionado, la arquitectura de datos debe respaldar las necesidades de datos cambiantes de su organización, y eso significa que la arquitectura de datos en sí misma también debe poder evolucionar. Como ingeniero de datos, su trabajo no consiste solo en crear sistemas que satisfagan las necesidades de datos de su organización en la actualidad, sino también en tener la vista puesta en el futuro para poder adaptarse constantemente a los cambios y requisitos empresariales, así como a la tecnología disponible. Acompáñeme en el siguiente vídeo para analizar el último grupo de principios y centrarse en las mejores prácticas a la hora de comprender el coste, la seguridad, la escalabilidad y los modos de fallo de los sistemas que construye.

Además de crear sistemas de datos que satisfagan las necesidades de las partes interesadas, rompa los silos entre los equipos y evolucione con las necesidades cambiantes de su organización. Si eso aún no es suficiente, también debes anticipar lo que sucede cuando las cosas van mal. Créeme, las cosas van a salir mal. En el siguiente grupo de principios, planifiqué el fracaso, diseñé para la escalabilidad, prioricé la seguridad y adopté FinOps. Cuando se trata de los modos de fallo en sus sistemas, es mejor adoptar un enfoque práctico y cuantitativo, tal como lo haría con cualquier otro aspecto del rendimiento de su sistema. Ahora podemos definir de manera más explícita lo que se entiende por términos que describen las métricas de su sistema, como la disponibilidad, la confiabilidad y la durabilidad de sus sistemas de datos. La disponibilidad, a menudo denominada tiempo de actividad, es el porcentaje de tiempo que se espera que un servicio o componente esté en estado operativo. Si observa las diferentes clases de almacenamiento y el almacenamiento de objetos de Amazon S3, por ejemplo, verá que su disponibilidad oscila entre el 99,5% y el 99,99% a lo largo del año. Ahora bien, el 99,5% y el 99,99% pueden parecer cifras de alta disponibilidad e incluso muy similares, pero tenga en cuenta que una disponibilidad anual del 99,5% significa que puede esperar que su sistema de almacenamiento no esté disponible durante un total de 44 horas al año. Si bien la disponibilidad del 99,99% significa que puede esperar aproximadamente una hora de inactividad al año. Desafortunadamente, nunca es posible garantizar una disponibilidad del 100%, ya que los escenarios de falla pueden incluir cortes de energía inesperados o daños en los dispositivos de red. Sin embargo, según las necesidades de su sistema, puede elegir una clase de almacenamiento con la disponibilidad que necesite. La confiabilidad es similar a la disponibilidad, pero es, en cambio, la probabilidad de que un servicio o un componente en particular desempeñe la función prevista dentro de un intervalo de tiempo determinado, dentro de estándares de rendimiento bien definidos. La durabilidad se refiere a la capacidad de un sistema de almacenamiento para soportar la pérdida de datos debido a fallas de hardware , errores de software o desastres naturales. En la nube, la durabilidad es crucial, ya que las empresas confían en los servicios en la nube para almacenar datos críticos y acceder a ellos. Por ejemplo, Amazon S3 cuenta con una durabilidad muy alta de 99.99999999. Probablemente sean demasiados 9 para decirlo en voz alta, pero son un total de 11 9 segundos de durabilidad de los datos. Esto significa que la pérdida de objetos en Amazon S3 es extremadamente rara. Relacionados con los conceptos de disponibilidad, confiabilidad y durabilidad, están el objetivo de tiempo de recuperación o RTO, por supuesto, y el objetivo de punto de recuperación o RPO, sin duda. El RTO es el tiempo máximo aceptable para una interrupción del servicio o del sistema. Para establecer un RTO para su aplicación, debe considerar el impacto en los clientes internos y externos si esta aplicación no está disponible. Luego puede, por ejemplo, decidir cuál es la clase de almacenamiento S3 para cumplir con lo subjetivo. El RPO, por otro lado, es una definición del estado aceptable después de la recuperación. Por ejemplo, cuando se habla de un sistema de almacenamiento de datos, el RPO puede hacer referencia a la pérdida de datos máxima aceptable que su sistema puede tolerar debido a un ultraje. Determinar el RTO y el RPO de los sistemas que construye le ayudará a elegir componentes con las especificaciones de disponibilidad, confiabilidad y durabilidad adecuadas para satisfacer sus necesidades. Ese es un aspecto de lo que significa planificar para el fracaso. Otra forma en que su sistema puede fallar es a través de brechas de seguridad. Por eso, el principio de planificación para el fracaso y el principio de priorizar la seguridad van de la mano. Ya hemos hablado un poco sobre cultivar una cultura de seguridad y el principio del mínimo privilegio. Aquí también me gustaría presentar lo que se denomina seguridad de confianza cero. Para entender de qué se trata la confianza cero, es útil echar un vistazo a lo que podríamos llamar un enfoque de seguridad más tradicional, que se conoce como seguridad perimetral reforzada. Adoptar un enfoque perimetral reforzado equivale a construir un gran muro, como este, alrededor de los sistemas, en el que no se confíe en todos y cada uno de los que están fuera del muro, mientras que se confía en todos y cada uno de los que están dentro del muro, en el sentido de tener acceso a datos y sistemas confidenciales. El problema de un enfoque perimetral reforzado es que los atacantes solo necesitarían atravesar la pared para obtener acceso sin restricciones a todos sus datos y sistemas. En la era de la nube, la idea de construir un perímetro reforzado tiene el problema adicional de que realmente no existe un perímetro físico con datos y sistemas que se conecten a través de Internet. La confianza cero, por el contrario, significa que cada acción requiere autenticación, y usted construye su sistema de manera que no se confíe en ninguna persona o aplicación, interna o externa, de forma predeterminada. En su lugar, el acceso se concede solo cuando es necesario. Su sistema también puede fallar cuando hace cosas como incurrir en grandes costos imprevistos o perder oportunidades de obtener ingresos. Por ejemplo, cuando se trata de costos imprevistos, me refiero a cosas como la ejecución accidental de costosos servicios en la nube, de modo que todo el presupuesto anual se consume en un mes o menos. Lo crea o no, ¿he visto esto pasar muchas veces? O, por el contrario, una oportunidad perdida puede ser que se produzca un aumento repentino de la demanda de tus productos y que todo tu sistema se bloquee porque no estabas bien preparado para escalar rápidamente. De esta manera, los principios de adoptar FinOps y diseñar una arquitectura para la escalabilidad se conectan con el principio de planificación para el fracaso. Como ingeniero de datos, debe pensar en las estructuras de costos de los sistemas en la nube. Por ejemplo, cuando se ejecuta un clúster distribuido, ¿cuál es la combinación adecuada de instancias EC2 a pedido de AWS frente a instancias puntuales? Por cierto, localice las instancias o las instancias EC2 no utilizadas que están disponibles con un gran descuento en AWS. ¿Cuál es el enfoque más apropiado para ejecutar un trabajo diario considerable en términos de rentabilidad y rendimiento? En la era de la nube, la mayoría de los sistemas de datos se pagan por uso y son fácilmente escalables. Los sistemas pueden funcionar con un costo por modelo de consulta, costo por modelo de capacidad de procesamiento u otra variante del modelo de pago por uso. Ahora es posible aumentar la escala para obtener un alto rendimiento y luego reducirla para ahorrar dinero. Sin embargo, el enfoque de pago por uso hace que el gasto sea mucho más dinámico. El nuevo desafío para los ingenieros de datos es administrar los presupuestos , las prioridades y la eficiencia al crear y mantener sus sistemas. La principal conclusión aquí es que si planifica para el fracaso o diseña una arquitectura para la escalabilidad, prioriza la seguridad y adopta FinOps, estará mejor posicionado para satisfacer las necesidades de su organización, no solo cuando los sistemas que está creando funcionen según lo esperado, sino también cuando ocurran fallas. A continuación, profundizaremos en los detalles de algunos enfoques de arquitectura específicos para diferentes tipos de sistemas de datos. Acompáñeme en el siguiente vídeo para ver más de cerca las arquitecturas por lotes.

La semana pasada analizamos brevemente los conceptos de lotes frente a streaming en el contexto de la ingesta de datos. Ahora me gustaría centrarme en cómo aparecen algunos de estos conceptos en algunos patrones de arquitectura bien establecidos con los que se encontrará como ingeniero de datos. El objetivo aquí es hacerle pensar en algunas de las desventajas e implicaciones de las diferentes decisiones que puede tomar con sus arquitecturas. En este vídeo, analizaremos más detenidamente las arquitecturas por lotes y, en el siguiente vídeo, analizaremos las arquitecturas de streaming. La arquitectura de datos por lotes es lo que podríamos llamar el enfoque tradicional del procesamiento de datos, en el que se ingieren, transforman y almacenan los datos en lotes o fragmentos. El procesamiento por lotes es más práctico cuando el análisis en tiempo real no es fundamental. Por lo general, un lote de datos consiste en datos recopilados durante un período de tiempo fijo, tal vez en el transcurso de un día. Y, por ejemplo, en una empresa de comercio electrónico, un analista de datos podría estar interesado en analizar el historial de ventas de un producto en particular a diario. Por lo tanto, puede configurar una arquitectura por lotes en la que ingiera y procese estos datos una vez al día. Entonces, ¿qué aspecto tendría esto? Este podría ser el principio de lo que se denomina canalización de extracción, transformación, carga o ETL, en la que primero se ingieren o extraen lotes de datos de una o varias fuentes, tal vez en un área de ensayo. A continuación, aplique algunas transformaciones para limpiar, estandarizar y modelar los datos y, a continuación, cárguelos en un almacén de datos para su almacenamiento y servicio. También hay una variación de este patrón, conocida como extraer, cargar y transformar, o ELT. Con ELT, la idea es que, después de ingerir los datos, los cargue en su almacén de datos y, a continuación, realice transformaciones directamente dentro del almacén de datos. Esta arquitectura o patrón ELT se está haciendo más popular hoy en día, dada la mayor potencia computacional de muchos almacenes de datos en la nube modernos y otras abstracciones de almacenamiento. Lo que sucede a continuación, ya sea que trabaje con una arquitectura ETL o ELT, es que entregará datos para casos de uso posteriores, que representaré aquí en el lado derecho del almacén de datos. Por lo general, pueden ser análisis o aprendizaje automático, pero otra posibilidad, como he mencionado anteriormente, es que el caso de uso final sea el denominado ETL inverso, en el que se realizan algunos análisis y, a continuación, los datos del proceso se devuelven a los sistemas de origen que se encuentran al principio de la canalización de datos. También podría tener una capa adicional aquí entre su almacén de datos y su caso de uso final para algo llamado data mart. Un data mart es un subconjunto más refinado de un almacén de datos que se centra en un departamento, función o área empresarial específicos. Un data mart está diseñado para ofrecer análisis e informes. Por lo tanto, es posible que tenga un centro de datos centrado en las ventas , otro para el marketing y otro para las operaciones. Este tipo de configuración puede facilitar el acceso a los datos para los analistas y para quienes necesitan crear informes. Con los data marts, también puede proporcionar una etapa de transformación adicional más allá de la que proporcionan las canalizaciones iniciales de ETL o ELT. Estas transformaciones adicionales pueden ser cosas como uniones adicionales entre tablas o agregaciones que pueden ayudar a mejorar el rendimiento de las consultas en tiempo real. Estos son algunos ejemplos de arquitecturas típicas de procesamiento por lotes. Si está configurando una arquitectura como esta para su organización, hay varias cosas que podría considerar en términos de los principios de una buena arquitectura de datos. Por ejemplo, si atiende varios casos de uso final para diferentes equipos o departamentos, ¿cómo podría elegir componentes comunes para su almacén de datos y canalizaciones de datos que ayuden a facilitar la colaboración y la interoperabilidad entre los equipos? Cuando se trata de planificar un fallo, hay que pensar en lo que ocurre, por ejemplo, si un sistema de origen se desconecta o cambia el esquema de datos ascendente. Conectarse con los propietarios del sistema fuente sería un excelente primer paso hacia la creación de un sistema que pueda gestionar los cambios en la fuente. También buscaría las especificaciones de disponibilidad y confiabilidad para cada uno de los componentes de su cartera y descubriría cómo incorporar flexibilidad a su sistema. Por ejemplo, si más adelante decides cambiar la cadencia de la ingesta o si esperas que el volumen de cada lote de datos varíe drásticamente con el tiempo para adoptar las finops, también realizarás un análisis de coste-beneficio para entender qué tipo de ventajas y desventajas podrías tener en cuenta en lo que respecta al rendimiento de los diferentes componentes de tu sistema, así como el tipo de valor que puedes ofrecer a la empresa en diferentes escenarios. Tendremos en cuenta estos principios a lo largo de estos cursos y recordaremos que las tecnologías que elija al crear sus sistemas de datos siempre presentarán un determinado conjunto de riesgos, así como oportunidades que pueden agregar valor a su organización. A continuación, analizaremos algunas arquitecturas de streaming comunes. Nos vemos en el siguiente vídeo.

Como mencioné la semana pasada, se puede pensar que los datos se producen en una serie de eventos, y estos eventos pueden ser clics en un sitio web o mediciones de sensores o algo más. En este sentido, en su origen, casi todos los datos podrían caracterizarse como un flujo continuo de este tipo de eventos. Es decir, los datos generalmente se producen y actualizan de forma continua con canalizaciones de datos por lotes, que vimos en el último vídeo. Si está esperando a que se acumulen los datos, en un intervalo de tiempo predefinido o una vez que los datos alcancen un determinado umbral de tamaño, puede procesar un lote de datos, por lo que solo está procesando un flujo de datos en una serie de fragmentos. O una canalización de datos en streaming, por otro lado. Está ingiriendo y proporcionando datos a los sistemas posteriores de forma continua y casi en tiempo real. Cuando digo casi en tiempo real, quiero decir que los datos están disponibles para los sistemas posteriores poco tiempo después de su producción. Posiblemente menos de un segundo después. En el sentido más simple, puede pensar que un sistema de transmisión está compuesto por un productor, un consumidor y un corredor de transmisión. El productor es la fuente de datos, y pueden ser datos de flujo de clics provenientes de una aplicación o mediciones provenientes de un dispositivo de IOT, y luego está un consumidor. Podría ser, por ejemplo, un servicio o una aplicación que procesará los datos, o podría ser un lago de datos o un almacén, y entre el productor y el consumidor se encuentra el agente de streaming que coordina los datos entre los productores y los consumidores, y luego, más allá del consumidor, puede haber algún caso de uso final de análisis en tiempo real o aprendizaje automático. Entre principios y mediados de la década de 2010, la popularidad de trabajar con datos de streaming se disparó con la aparición de Kafka como una plataforma de transmisión de eventos altamente escalable y otros marcos de procesamiento de transmisiones, como Apache Storm y Samza, para la transmisión y el análisis en tiempo real. Estas tecnologías permitieron a las empresas realizar nuevos tipos de análisis y modelado en grandes cantidades de datos, como la agregación y clasificación de usuarios y las recomendaciones de productos. Esta nueva demanda de soluciones de transmisión de datos no significó que el procesamiento por lotes desapareciera. En cambio, significaba que los ingenieros de datos tenían que averiguar cómo conciliar los datos por lotes y en streaming en una sola arquitectura. Lo que se conoce como arquitectura Lambda fue una de las primeras respuestas populares a este problema. En una arquitectura Lambda, tiene sistemas de transmisión y servicio por lotes que funcionan de forma independiente, y el sistema de origen transmite datos simultáneamente a dos destinos. Digo uno para el procesamiento de flujos, en el que los datos del proceso pueden almacenarse en una base de datos sin SQL, y otro para el procesamiento por lotes, que puede utilizar un almacén de datos para transformar y almacenar los datos procesados y agregados con fines analíticos. La capa de servicio de esta arquitectura proporciona entonces una vista combinada al agregar los resultados de las consultas de las capas de lotes y streaming. Quería mencionar aquí la arquitectura Lambda para que la conozcan, pero esta arquitectura presenta varios desafíos y problemas, como la administración de sistemas paralelos con diferentes bases de código, entre otras cosas. En muchos sentidos, la tecnología y las prácticas han ido más allá de la arquitectura Lambda. Sin embargo, la arquitectura Lambda sigue siendo un buen punto de referencia para los diseños y las herramientas de arquitectura de streaming que aparecieron después. Como respuesta a las deficiencias de la arquitectura Lambda. De hecho, uno de los autores originales de Apache Kafka, J Kreps, propone una alternativa llamada arquitectura Kappa. La idea central de la arquitectura Kappa es utilizar una plataforma de procesamiento de flujos como columna vertebral para todo el manejo, la ingesta, el almacenamiento y el servicio de datos. Esto facilita una verdadera arquitectura basada en eventos, lo que significa que, en lugar de esperar a que los sistemas comprueben periódicamente si hay actualizaciones cuando ocurren cosas y cuando se producen datos, la información se envía automáticamente a los consumidores relevantes que necesitan la actualización para que estos consumidores puedan reaccionar de manera más inmediata a esta información. Con la plataforma de procesamiento de transmisiones como columna vertebral, puede aplicar el procesamiento en tiempo real leyendo la transmisión de eventos en vivo. Al mismo tiempo, puede configurar el procesador de transmisión para que conserve una cierta cantidad de datos históricos a medida que lee la transmisión en vivo. Esto le permite aplicar eficazmente el procesamiento por lotes cuando lo desee al reproducir grandes fragmentos de los datos retenidos del mismo flujo de datos. Bueno, Lambda ha caído en desgracia y Kappa nunca fue ampliamente adoptado. Ambas arquitecturas sirvieron de inspiración y sentaron las bases para superar el desafío central de unificar el procesamiento de datos por lotes y en streaming. Uno de los problemas principales de la administración del procesamiento por lotes y streaming es la unificación de varias rutas de código. Hoy en día, los ingenieros buscan resolver esto de varias maneras. Google desarrolló el modelo de flujo de datos y el marco Apache Beam que implementa este modelo. La idea central del modelo de flujo de datos es ver todos los datos como eventos. Las transmisiones de eventos en tiempo real en curso contienen datos ilimitados. Los lotes de datos son simplemente flujos de eventos limitados, y los límites proporcionan una ventana natural, por lo que el procesamiento en tiempo real y por lotes puede realizarse en el mismo sistema utilizando un código casi idéntico. Apache Flink y otras herramientas de procesamiento de transmisiones se utilizan ampliamente en la actualidad, y analizaremos estas y otras herramientas similares en este curso. En la ingeniería de datos actual, la filosofía del lote como un caso especial de transmisión es ahora más generalizada que nunca. En su trabajo como ingeniero de datos, puede esperar enfrentarse a los desafíos de administrar las canalizaciones por lotes y de streaming. A medida que se enfrente a estos desafíos, deberá tener en cuenta los principios de una buena arquitectura de datos a la hora de elegir los componentes de sus sistemas, crear soluciones para lograr flexibilidad y escalabilidad y anticipar posibles modos de falla. Una cosa en la que debe pensar, independientemente del sistema que esté creando, es el cumplimiento. En resumen, el cumplimiento significa garantizar que sus sistemas de datos cumplan con las leyes, los reglamentos y los acuerdos de privacidad y las políticas de términos de servicio de su propia organización. Acompáñeme en el siguiente vídeo para hablar sobre la arquitectura para el cumplimiento.

Vale, antes de empezar, solo quiero salir y decirlo. El cumplimiento normativo es probablemente el tema más aburrido de estos cursos. No creo que nadie quiera hablar realmente de leyes y reglamentos, especialmente cuando podríamos hablar de trabajar con datos y trabajar con tecnologías interesantes, ¿verdad? Así que estoy de acuerdo, y también creo que lo decepcionaría si no dedicara al menos un tiempo a hablar sobre cómo el cumplimiento normativo se ajusta a su función como ingeniero de datos. Y esto se debe principalmente a que una de las formas más espectaculares en las que sus sistemas de datos pueden fallar es infringir las regulaciones y provocar que su organización sea demandada e incurra en multas cuantiosas. Esto sí sucede. Entonces, ¿de qué tipo de regulaciones estoy hablando con respecto a los datos? Un reglamento importante es el Reglamento General de Protección de Datos, o GDPR, que se promulgó en la Unión Europea en 2018. En resumen, el GDPR tiene que ver con la protección de la privacidad y los datos personales de las personas. Sin embargo, lo que constituye datos personales según el GDPR es relativamente amplio, e incluye no solo la información de identificación personal (PII), sino también otra información que podría usarse colectivamente para identificar a una persona. Para cumplir con el RGPD, debe asegurarse de contar con el consentimiento adecuado de las personas de las que recopila datos, así como de la capacidad de eliminar los datos de manera oportuna. Si una persona quiere que se eliminen sus datos de sus sistemas ahora, puede que esté pensando: ¿qué pasa si mi empresa no tiene su sede en la UE? ¿O qué pasa si no atendemos a clientes en la UE? Bueno, técnicamente, sí, la ubicación de su empresa y sus clientes al menos influirá en la determinación de si las regulaciones se aplican a usted. Sin embargo, desde la promulgación del GDPR, docenas de países de todo el mundo, así como estados individuales de los EE. UU., han adoptado regulaciones similares. Como ingeniero de datos, será responsable de construir sistemas que no solo cumplan con las normativas actuales, sino también con las del mañana. Y podrían ser nuevas leyes que se promulguen en el lugar donde usted opera actualmente o regulaciones que ya estén en vigor en las áreas a las que se expande su empresa. En el futuro. Será su responsabilidad mantener sus sistemas actualizados para cumplir con el conjunto de normas que se aplican a su empresa. El enfoque inteligente consiste en crear sistemas que cumplan con las normas modernas de protección de datos, como el RGPD, incluso si las normativas locales son menos estrictas. Y para crear sistemas flexibles y poco acoplados que le permitan adaptarse a los cambios normativos. Además de la ubicación de su empresa y sus clientes, la industria en la que opera también puede tener su propio conjunto de regulaciones. Si trabajas con datos de atención médica en EE. UU., por ejemplo, tendrás que cumplir con la Ley de Portabilidad y Responsabilidad del Seguro Médico, o la ley HIPAA, en relación con los datos confidenciales de los pacientes. Se han promulgado leyes similares con respecto a los datos médicos en muchos países del mundo. O si trabajas con datos financieros en tu organización, tendrás que cumplir con la ley Sarbane Oxley de EE. UU. o leyes similares en otros lugares que exigen ciertas prácticas de presentación de informes financieros y mantenimiento de registros. Por lo tanto, la conclusión principal aquí es que, independientemente de dónde se encuentre en el mundo o en qué industria se encuentre, existen leyes y regulaciones que se aplicarán a los sistemas que construya. Como ingeniero de datos, una forma de aportar valor a su organización es evitar las demandas y multas que conlleva el incumplimiento de las normativas necesarias. No dedicaremos mucho tiempo a los detalles del cumplimiento normativo en el resto de estos cursos, pero al menos quería que conociera este aspecto en su función de ingeniero de datos para que pudiera tenerlo en cuenta junto con los demás principios de una buena arquitectura de datos. En la siguiente lección, veremos cómo elegir las tecnologías adecuadas para su arquitectura. Te veré allí.

En la lección anterior, analizamos qué significa diseñar una buena arquitectura de datos y por qué es importante. Hago hincapié en que debe tener en cuenta las ventajas y desventajas entre las diferentes opciones de diseño y crear sistemas flexibles con un acoplamiento flexible que puedan adaptarse a las cambiantes necesidades de datos de su organización. En esta lección, me centraré en cómo elegir las herramientas y tecnologías adecuadas para crear este tipo de arquitectura. En el campo de la ingeniería de datos, no faltan opciones cuando se trata de herramientas y tecnologías para realizar el trabajo. De hecho, todo lo contrario. Como ingenieros de datos, en cambio, sufrimos una vergüenza de riqueza. Ya sea que esté considerando soluciones para la ingestión, el almacenamiento, la transformación o el servicio, encontrará opciones que incluyen el código abierto, el código abierto administrado , los servicios de software propietario y mucho más. Al tomar estas decisiones, es importante tener en cuenta su objetivo final, es decir, ofrecer productos de datos de alta calidad que cumplan con los requisitos de los usuarios finales. En otras palabras, su arquitectura de datos es el qué, el por qué y el cuándo para satisfacer las necesidades de datos de la empresa. Las herramientas y tecnologías que elija para hacer de esa arquitectura una realidad o el cómo hacerlo. En este punto, puede que estés pensando: bueno, claro, eso suena lógico. Elija las herramientas que le permitirán obtener un resultado exitoso. Eso es genial Gracias, Joe. Bueno, desafortunadamente, hay varias maneras en las que esto puede salir mal, y de eso es de lo que hablaremos en esta lección para que estés preparado para el éxito. Primero analizaremos la ubicación, es decir, las ventajas y desventajas entre construir sus sistemas en las instalaciones, en la nube o en algún híbrido de los dos. A continuación, analizaremos la optimización de costos y cómo puedes decidir si crear tus propias herramientas para ciertas cosas o comprar soluciones listas para usar, teniendo en cuenta aspectos como el tamaño y las capacidades de tu equipo y el tipo de actividades que realmente generan valor en tu empresa. Hablaremos sobre cómo crear para las necesidades actuales y, al mismo tiempo, prestar atención a las posibles necesidades futuras de su organización. Analizaremos todas estas cosas en el contexto de los principios de una buena arquitectura de datos que vio en la lección anterior y las corrientes subyacentes del ciclo de vida de la ingeniería de datos que vio la semana pasada. Acompáñame en el siguiente vídeo para empezar.

No hace mucho, tal vez solo hace dos décadas o algo así. La creación de sistemas de datos locales era realmente la única opción para cualquier tipo de necesidad de almacenamiento o procesamiento de datos. Esto se debía simplemente a que las plataformas de datos en la nube modernas aún no existían. El sistema local es aquel en el que la empresa posee y mantiene el hardware y el software para toda la pila de datos. Esto significa que una empresa es responsable desde el punto de vista operativo del aprovisionamiento, el mantenimiento, la actualización y el escalado del hardware y el software que se ejecuta en él. Hoy en día, muchas empresas construyen todos sus sistemas de datos en la nube. Con los sistemas de datos en la nube, el proveedor de la nube, como AWS, por ejemplo, es responsable de crear y mantener el hardware y los centros de datos para satisfacer las necesidades de los clientes. Si está creando sus sistemas de datos en la nube, básicamente está alquilando los recursos de procesamiento y almacenamiento necesarios para su sistema. Lo bueno de la computación y el almacenamiento en la nube es que puedes escalar fácilmente para satisfacer la demanda o reducirlos para ahorrar costos cuando no los necesitas. No necesita mantener ni aprovisionar ningún hardware, y puede cambiar de opinión con relativa facilidad acerca del tipo exacto de herramientas o tecnologías que desea utilizar en su sistema. Bueno, muchas empresas ahora están optando por construir sistemas de datos completamente en la nube. Otros aún mantienen sistemas locales o algún tipo de sistema híbrido con algunos componentes locales y otros en la nube. El impulso de la industria definitivamente apunta a que más empresas elijan la nube en lugar de los sistemas de datos locales. O migrar de las instalaciones a la nube. Esto se debe a todas las ventajas obvias que ofrece la nube en términos de flexibilidad y escalabilidad. Sin embargo, hay algunas empresas que eligen o se ven obligadas a mantener algunos o todos sus sistemas de datos en las instalaciones debido a la naturaleza de su negocio o a las normativas o a problemas de seguridad y privacidad de los clientes. Como ingeniero de datos actual, es posible que trabajes en una empresa que tenga algunos sistemas locales o en una que esté migrando de las instalaciones a la nube. En estos cursos, nos centraremos exclusivamente en la creación de sistemas de datos en la nube. Esto se debe a que, para la gran mayoría de los casos de uso empresarial actuales, crear sus sistemas de datos en la nube es la mejor opción. La industria avanza en la dirección de una mayor cantidad de servicios en la nube y menos en las instalaciones. Como aspirante a ingeniero de datos, creo que es mejor dedicar su tiempo a aprender a crear sistemas de datos en la nube. Acompáñeme en el siguiente vídeo para obtener más información sobre otra tendencia en la ingeniería de software y datos: el paso de los sistemas monolíticos a los modulares.

Otro concepto de ingeniería de datos que quiero abordar brevemente aquí, cuando hablamos de sistemas que contienen dependencias rígidas y flexibilidad limitada frente a aquellos que están estrechamente acoplados y son flexibles, es la idea de arquitectura monolítica frente a arquitectura modular. Los sistemas monolíticos son sistemas autónomos que se componen de componentes estrechamente acoplados. En el desarrollo de software, las arquitecturas monolíticas han sido un pilar tecnológico durante décadas, en las que grandes equipos trabajan juntos para ofrecer una única base de código funcional. Todos los componentes de esa base de código de producto de software se crearían e implementarían como una sola aplicación. Una ventaja de un sistema monolítico es la simplicidad. Tiene toda la funcionalidad en un solo lugar. Esto significa que es fácil entender un sistema monolítico. En lugar de tratar con docenas o cientos de tecnologías, solo tiene una tecnología y, por lo general, un lenguaje de programación principal. Los monolitos son una excelente opción si desea simplicidad y razonamiento sobre su arquitectura y sus procesos. Sin embargo, los monolitos también son muy difíciles de mantener a medida que crecen, ya que un monolito se compone de componentes estrechamente acoplados, por lo que si necesita actualizar un componente, es posible que también tenga que actualizar otros componentes. A menudo es necesario volver a escribir una aplicación completa. Por ejemplo, una vez trabajé en una empresa que tenía un proceso de ETL monolítico que tardaba al menos 48 horas en funcionar. Ahora, si algo se rompía en alguna parte de este oleoducto, había que reiniciar todo el proceso del oleoducto. Mientras tanto, los ansiosos usuarios empresariales intermedios esperaban los informes, que ya tenían dos días de retraso por defecto y, a menudo, llegaban mucho más tarde. El equipo responsable de este oleoducto deseaba desesperadamente reemplazar este oleoducto, pero habría requerido semanas de inactividad para todo el sistema. Avanzaron cojeando y aceptaron un rendimiento inferior al óptimo porque actualizar el sistema era una tarea demasiado abrumadora y costosa. Por el contrario, los sistemas modulares constan de componentes débilmente acoplados. En lugar de depender de un enorme monolito que combine todas las funcionalidades de una aplicación, los sistemas modulares se basan en dividir la aplicación en áreas de preocupación independientes. En el desarrollo de software, los sistemas verdaderamente modulares surgen con el auge de los microservicios. Con los microservicios, en lugar de combinar los componentes que corresponden a varios servicios en una sola entidad desplegable, cada servicio se implementa como una sola unidad. Ingeniería de datos moderna. Las tecnologías de procesamiento de datos se han desplazado hacia un modelo modular al proporcionar un sólido soporte para la interoperabilidad. Esto significa que la mayoría de las herramientas de procesamiento de datos disponibles en la actualidad se pueden integrar fácilmente con herramientas que respaldan las otras etapas del ciclo de vida de la ingeniería de datos. Por ejemplo, los datos almacenados en el almacenamiento de objetos en un formato estándar como Parquet se pueden combinar con cualquier herramienta de procesamiento que admita el formato Parquet. La capacidad de intercambiar herramientas a medida que la tecnología cambia es inestimable. Le ayuda a crear una buena arquitectura de datos al permitir la toma de decisiones flexibles y reversibles y la mejora continua. Durante el resto de esta lección, analizaremos los detalles de las distintas decisiones que tomará para implementar su arquitectura. Cosas como la optimización de costos, si se opta por soluciones de código abierto o maná y todo lo demás. Una vez más, presentaré estas ideas desde una perspectiva modular y centrada en la nube, porque estas son las direcciones en las que se dirige la ingeniería de datos como campo. Acompáñeme en el siguiente vídeo para ver la optimización de costes.

Como dije antes, en cada etapa del ciclo de vida de la ingeniería de datos, tendrá múltiples herramientas y tecnologías entre las que elegir para realizar el trabajo. Cada una de estas opciones tiene un costo, y no me refiero solo al precio y al software o servicio al que te estás suscribiendo. También hay un costo asociado con la implementación, es decir, pagar a un equipo para que dedique el tiempo necesario a instalar y mantener el sistema, y también hay costos de oportunidad, lo que significa que al elegir una herramienta, al menos durante un tiempo se aprovecha la oportunidad de elegir otra. Por lo tanto, sus elecciones de herramientas y tecnología tendrán un impacto significativo en su presupuesto. Como ingeniero de datos, su trabajo consiste en proporcionar un retorno positivo de la inversión que su organización realiza en sus sistemas de datos. En este vídeo, analizaremos los costos desde tres puntos de vista principales. Primero me centraré en el costo total de propiedad, o TCO para abreviar. Después de eso, analizaremos rápidamente el costo total de oportunidad de propiedad o TOCO o TOCO para abreviar. Por último, volveremos a hablar de FinOps, que abordamos brevemente la semana pasada. El costo total de propiedad, o TCO, es el costo total estimado de una iniciativa o proyecto de solución durante todo su ciclo de vida. El término TCO no es específico de la ingeniería de datos, sino que es un término empresarial general que se utiliza para describir la inversión total en algún proyecto, incluidos los costos directos e indirectos de los productos y servicios que se utilizan. Esto incluye cosas como la adquisición de hardware y software, el costo de administración, mantenimiento y reparaciones, así como cualquier capacitación requerida. Por lo tanto, cuando se trata de sistemas de datos, costos directos o costos tangibles y fáciles de identificar que se atribuyen directamente al desarrollo de un producto de datos. Por ejemplo, sus costos directos incluyen los salarios del equipo que trabaja en la iniciativa o la factura de AWS por todos los servicios utilizados, así como las tarifas o los costos de licencia de las suscripciones de software. Sus costos indirectos, casi conocidos como gastos generales, o gastos que no se atribuyen directamente al desarrollo de un producto de datos. Por ejemplo, podrían ser los costos incurridos debido al tiempo de inactividad de la red, el soporte de TI continuo o la pérdida de productividad de ciertos miembros del equipo. Es importante incluir los costos indirectos al estimar el costo total de propiedad, ya que estos costos pueden ser importantes. Cuando se trata del costo del hardware y el software, estos gastos generalmente se dividen en dos grandes grupos. El primer grupo son los gastos de capital o CapEx para abreviar. El CapEx es el pago realizado para comprar activos fijos a largo plazo. Este tipo de gasto para los sistemas de datos era común antes de la existencia de las plataformas en la nube. Las empresas realizarían un pago inicial para comprar hardware y software y luego los instalarían en los centros de datos. Estas inversiones iniciales, por lo general de cientos de miles a millones de dólares o más, se tratarían como activos de CapEx que se depreciarían lentamente con el tiempo. Hoy en día, con el cambio a la nube, muchas empresas están creando sistemas de datos con un CapEx prácticamente nulo. El otro tipo importante de costos son los gastos operativos o OpEx, para abreviar. Los gastos operativos son un gasto asociado con la ejecución de las operaciones diarias, por lo que se distribuyen a lo largo del tiempo. En los sistemas de datos, los gastos operativos suelen aparecer como un gasto de pago por uso en forma de tarifas de suscripción recurrentes o el costo de usar un servicio de nube en particular. En el contexto de la construcción en las instalaciones frente a los sistemas de datos basados en la nube, la construcción en las instalaciones generalmente implica un gran costo de capital. Bueno, los sistemas basados en la nube pueden ser casi en su totalidad OpEx. Ahora, antes de que existieran las plataformas en la nube, un enfoque centrado en los gastos operativos no era realmente una opción para proyectos de datos de gran tamaño. Esto ha cambiado ahora con la llegada de la nube, ya que los servicios de plataforma de datos permiten pagar según un modelo basado en el consumo. Las inversiones en hardware a largo plazo para proyectos de datos inevitablemente quedarán obsoletas, por lo que le sugiero que primero adopte un enfoque de OpEx centrado en la nube y elija tecnologías flexibles de pago por uso para sus canalizaciones de datos. A diferencia del TCO, lo que yo llamo costo total de propiedad de oportunidad o TOCO, abreviado TOCO, es el costo de las oportunidades perdidas en las que incurre al elegir una herramienta o tecnología en particular. Puede ser más difícil de cuantificar, pero básicamente significa que cualquier elección que hagas excluye inherentemente otras posibilidades. Si eliges la pila de datos A, que incluye un determinado conjunto de tecnologías para crear tus canalizaciones de datos, habrás elegido las ventajas de la pila de datos A en lugar de todas las demás opciones, excluyendo de manera efectiva, por ejemplo, la pila de datos B, C, D, etc. En este caso, el costo total de oportunidad de propiedad es el costo de permanecer cautivo de la pila de datos A y dejar de beneficiarse de otras pilas de datos. En el caso de que la pila de datos A resulte ser la mejor opción posible, enhorabuena. El costo total de oportunidad de propiedad es esencialmente cero. Sin embargo, en realidad, las herramientas y tecnologías de ingeniería de datos evolucionan muy rápidamente. Incluso si hoy toma las mejores decisiones posibles, aún necesitará hacer evolucionar sus sistemas de datos en el futuro. Si algunos componentes de la pila de datos A, que eran las mejores opciones para ayer, ahora se han vuelto obsoletos, el cambio a un componente diferente o a una pila completamente diferente tendrá un costo. Para garantizar que el coste total de propiedad se reduzca al mínimo, tendrá que crear sistemas flexibles y poco acoplados que sean fáciles de actualizar a medida que cambien sus necesidades de datos y evolucione el panorama de herramientas y tecnologías. Una forma de hacerlo es reconocer por adelantado qué componentes de las canalizaciones de datos tienen más probabilidades de cambiar. En otras palabras, separar las tecnologías inmutables de las tecnologías transitorias. Las tecnologías inmutables son aquellas que han resistido la prueba del tiempo. En el almacenamiento en la nube, se trata de cosas como el almacenamiento de objetos y las redes. O como otro ejemplo, SQL como lenguaje de consulta existe desde hace décadas y no va a desaparecer pronto. Las tecnologías transitorias, o al menos las que tienen más probabilidades de serlo, son aquellas que son nuevas en la vanguardia y en áreas de la pila de datos que evolucionan rápidamente, como el procesamiento de transmisiones, la orquestación y la inteligencia artificial, por ejemplo. Cuando se trata de pensar en las opciones tecnológicas en el contexto de la optimización de costos. El concepto de FinOps está estrechamente relacionado con el TCO y el TOCO. FinOps consiste en minimizar los costos asociados con sus sistemas de datos, su TCO y su TOCO y, al mismo tiempo, maximizar sus oportunidades de generación de ingresos. ¿Cómo se hace eso? En resumen, puede elegir servicios basados en la nube que le permitan adoptar un enfoque centrado en los gastos operativos con tecnologías flexibles de pago por uso, así como opciones modulares que le permitan iterar , mejorar y crecer rápidamente. El siguiente. Nos centraremos un poco más en el tema de elegir las herramientas y tecnologías adecuadas y, al mismo tiempo, optimizar los costos. Acompáñeme en el siguiente vídeo para ver las ventajas y desventajas entre crear sus propios componentes del sistema de datos y comprar soluciones listas para usar.

Hasta ahora, hemos estado hablando de elegir herramientas y tecnologías para su arquitectura y he hecho hincapié en que, en general, construir en la nube con servicios flexibles de pago por uso será la mejor opción para la gran mayoría de las empresas, cuando se trata de servicios comunes, como, por ejemplo, el almacenamiento de objetos. Usar un servicio en la nube como el S3 de Amazon es una opción mucho mejor que intentar crear tu propia solución personalizada de almacenamiento de objetos. Sin embargo, según los requisitos de su sistema, es posible que necesite crear y personalizar usted mismo ciertas herramientas o tecnologías. Por ejemplo, muchos equipos optan por construir sobre marcos de código abierto para obtener exactamente la solución que necesitan. En otros casos, un equipo puede optar por crear su propia solución o personalizar los marcos de código abierto para evitar las tarifas de licencia o simplemente evitar estar a merced de un proveedor. De hecho, prácticamente para todas las etapas del ciclo de vida de la ingeniería de datos, dispondrá de una variedad de opciones a la hora de elegir herramientas y tecnologías. Por supuesto, para cualquier herramienta que necesites, puedes construir algo desde cero. En algunos casos, esta puede ser tu única opción si estás intentando hacer algo que nadie más ha hecho antes. Sin embargo, en la mayoría de los casos, esto no se recomienda a menos que esté seguro de que no existe una solución para lo que está intentando hacer. Crear tecnologías desde cero cuando ya existen soluciones listas para usar puede equivaler a reinventar la rueda, por así decirlo. Me oirá referirme a esta actividad como trabajo pesado indiferenciado en el sentido de que es un trabajo arduo y, en última instancia, probablemente no añada valor a su organización. Cuando se trata de soluciones existentes, estas incluyen opciones totalmente de código abierto, así como opciones comerciales de código abierto de los proveedores, que son esencialmente una versión gestionada de alguna herramienta de código abierto. Además , también puede haber software y servicios propietarios que no sean de código abierto entre los que elegir. A la hora de elegir entre estas opciones, hay varios parámetros clave que debes tener en cuenta. En primer lugar, si opta por una solución totalmente de código abierto, ¿su equipo tiene el ancho de banda y las capacidades para implementar y mantener ese sistema? Muchas herramientas de código abierto tienen una gran comunidad de apoyo donde puedes obtener ayuda si la necesitas. Pero si formas parte de un equipo pequeño, tal vez incluso de uno solo, es posible que un servicio gestionado de código abierto o propietario se adapte mejor a tus necesidades. Esto se debe a que le permite crear y administrar todo su sistema de datos sin tener que preocuparse por la implementación y el mantenimiento de un solo componente. Más allá de eso, incluso si su equipo tiene la experiencia y el ancho de banda para crear desde cero e implementar una solución de código abierto. ¿De verdad vale la pena? A primera vista, crear algo por ti mismo o usar una solución de código abierto puede parecer un ahorro de costos porque estás evitando las tarifas de licencia. Sin embargo, el coste total de propiedad, como mencionamos en el último vídeo, es mucho más que los costes de licencia. También incluye el costo del equipo necesario para construir y mantener el sistema. Además de los costos, el otro punto importante a considerar es si la creación y el mantenimiento de una solución personalizada o de código abierto realmente proporciona valor a su organización. En otras palabras, ¿obtiene alguna ventaja al crear su propio sistema o utilizar el código abierto en comparación con lo que obtendría con un servicio gestionado, o simplemente se trata de un trabajo pesado indiferenciado? En otras palabras, solo trabajo duro que no proporciona ningún valor adicional. Para la mayoría de los equipos, y especialmente los equipos pequeños, que crean canalizaciones de datos y se preguntan si deben crear las suyas propias, utilizar el código abierto o comprar herramientas comerciales de código abierto o patentadas. Mi sugerencia sería buscar primero soluciones de código abierto o comerciales de código abierto y, si no puede obtener lo que necesita, considere comprar una solución propietaria. En cualquier caso, hay muchos excelentes servicios modulares disponibles para elegir. Esto permitirá que su equipo se concentre en las oportunidades únicas que proporcionan el mayor valor a su organización. Acompáñeme en el siguiente vídeo para ver las diferencias entre las denominadas opciones sin servidor y las opciones de herramientas y tecnologías de servidor.

Para alojar cualquier aplicación de software, necesita un servidor, que es esencialmente un ordenador. O un conjunto de ordenadores que impulsan su aplicación al proporcionar CPU, memoria o RAM, así como almacenamiento en disco y, tal vez, una GPU y una red. Los servidores proporcionan recursos informáticos a través de una red, la mayoría de las veces a través de Internet. En lo que respecta a las herramientas en la nube, según el servicio que esté considerando, es posible que deba configurar y administrar los recursos informáticos necesarios para ejecutar la aplicación. Y en otros casos, es posible que pueda elegir entre una o más de las tres opciones informáticas siguientes. Servidor con o sin servidor En este vídeo, repasaré las diferencias y compensaciones entre estas tres opciones. Si opta por la versión de servidor de un servicio, será responsable de configurar y administrar el servidor, por ejemplo, una instancia Amazon EC2 en la que se ejecutará el servicio. Incluye la actualización del sistema operativo, la instalación o actualización de paquetes, paquetes de software, redes, escalado y seguridad. A diferencia de un servidor, un contenedor es una unidad más modular que reúne el código y todas sus dependencias en un paquete que puede ejecutarse en un servidor. Por lo tanto, una máquina virtual tradicional envuelve todo el sistema operativo. Un contenedor es más liviano, ya que solo se empaqueta en un espacio de usuario aislado, como un sistema de archivos y algunos procesos. Por lo tanto, en el caso de una solución en contenedores, seguirías siendo responsable de configurar los elementos esenciales del código y las dependencias de la aplicación. Sin embargo, el sistema operativo subyacente, las redes y todo lo demás se proporcionarían aparte de las opciones de servidores y contenedores para los servicios. Cada vez es más común en el mundo de las herramientas de datos en la nube encontrar el terminal sin servidor para describir un servicio en particular. Si está familiarizado con la forma en que funcionan las computadoras, una palabra como «sin servidor» puede sonar un poco extraña, por ejemplo, cómo se ejecuta el software sin un servidor, ¿verdad? Bueno, resulta que el término sin servidor en realidad no significa que no haya servidor. Simplemente significa que la configuración y el mantenimiento del servidor no son su responsabilidad para ese servicio en particular. Puede interactuar con la aplicación sin administrar los servidores entre bastidores ni preocuparse por las instalaciones y dependencias de los paquetes. Así que el servidor está esencialmente oculto para ti, como he dibujado aquí. Por lo general, las tecnologías sin servidor se ejecutan en contenedores, por lo que estos servicios se pueden escalar automáticamente. Tienen disponibilidad y tolerancia a fallos integradas, y ofrecen facturación de pago por uso. Sin embargo, en el caso de los servicios sin servidor, los contenedores en los que se ejecutan también se abstrajeron. De este modo, las tecnologías sin servidor pueden permitirle dedicar menos tiempo a preocuparse por su infraestructura informática y más tiempo a centrarse en el desarrollo de productos de datos. En el laboratorio de la semana anterior, trabajó con varios servicios sin servidor, como Amazon, Athena y AWS Glue. La tendencia sin servidores comenzó con toda su fuerza con el lanzamiento de AWS Lambda en 2014, un servicio que permite ejecutar código en respuesta a un evento. Con la promesa de ejecutar pequeños fragmentos de código según sea necesario sin tener que administrar un servidor. Las opciones sin servidor se han disparado en popularidad y diversidad, y ahora van mucho más allá de la simple ejecución de fragmentos de código bajo demanda. Las principales razones de esta popularidad son el costo y la conveniencia. En lugar de pagar el coste de un servidor, puedes pagar un poco cada vez que se ejecute el código o cuando utilices un servicio en particular. Entonces, ¿cuándo tiene sentido usar servicios sin servidor? Como ocurre con muchos otros servicios en la nube, depende. Como ingeniero de datos, debe comprender los detalles de los precios de la nube para poder predecir el costo de sus implementaciones sin servidor y decidir si es más rentable que la opción de servidor. Por ejemplo, si observa los precios de AWS Lambda, descubrirá que usar el servicio en un entorno con altas tasas de eventos puede resultar catastróficamente caro. Al igual que en otras áreas de sus canalizaciones de datos, es fundamental modelar y monitorear los servicios que usa, sin servidor o de otro tipo. Es posible que necesite monitorear directamente para determinar las tasas reales de eventos, la duración y el costo por evento en un entorno real. Por lo tanto, puede modelar el costo total de un servicio sin servidor en comparación con alguna otra alternativa. Además de eso, las plataformas sin servidor en la nube tienen límites de ejecución, frecuencia, simultaneidad y duración. Si su aplicación no puede funcionar perfectamente dentro de estos límites, es hora de considerar un enfoque orientado a contenedores para que pueda pensar en ello de esta manera. Serverless funciona mejor para cargas de trabajo y tareas simples y discretas. No va a funcionar tan bien si tiene muchas partes móviles o necesita mucha potencia de procesamiento o memoria. En ese caso, considera usar contenedores y un marco de orquestación del flujo de trabajo de contenedores como Kubernetes. Para la mayoría de las aplicaciones modernas de ingeniería de datos en la nube, puede realizar el trabajo con herramientas sin servidor o en contenedores. Por lo tanto, recomendaría utilizar primero la tecnología sin servidor y, si es posible, luego los contenedores y la orquestación. Y una vez que haya superado estas opciones sin servidor. Acompáñeme en el siguiente vídeo para concluir esta lección con un vistazo a cómo las corrientes subyacentes del ciclo de vida de la ingeniería de datos entran en juego a la hora de elegir herramientas y tecnologías para su arquitectura de datos.

Para concluir esta lección, me gustaría abordar brevemente cómo entran en juego cada una de las corrientes subyacentes del ciclo de vida de la ingeniería de datos a la hora de elegir las herramientas y tecnologías para crear su arquitectura de datos. La semana pasada analizamos cada una de estas seis corrientes subyacentes: seguridad, administración de datos, operaciones de datos, arquitectura de datos, orquestación e ingeniería de software, en relación con el ciclo de vida de la ingeniería de datos. Ahora analizaremos más detenidamente cómo estas corrientes subyacentes se relacionan con la elección de los componentes individuales para su arquitectura de datos. En lo que respecta a la seguridad, las diferentes herramientas tienen diferentes características de seguridad. Es importante entender cuáles son esas funciones y asegurarse de implementar la tecnología de autenticación adecuada, así como otras prácticas recomendadas de las que hablamos. Una cosa importante a tener en cuenta es usar solo software y herramientas desarrolladas por organizaciones acreditadas y comunidades de código abierto confiables. Hay casos conocidos de organizaciones o estados nacionales que lanzan herramientas de datos que contienen componentes sospechosos, básicamente software espía que compromete sus canales de datos. No voy a decir nada más sobre eso ahora, pero la conclusión es que debes asegurarte de que sabes de dónde vienen tus herramientas. Si se trata de una herramienta de código abierto, echa un vistazo al código y asegúrate de entender cómo se implementa. Con la administración de datos, no siempre está claro cómo se implementan determinadas prácticas de gobierno de datos, y es una buena idea preguntar a la empresa o comunidad que proporciona la herramienta cómo se gestiona la gobernanza. Por ejemplo, ¿cómo se protegerán sus datos contra las infracciones tanto externas como internas? ¿Cómo cumple la herramienta con el RGPD y otras normas de privacidad de datos? ¿O cómo proporciona la herramienta la verificación de la calidad de los datos? Cuando se trata de operaciones de datos, la elección de las herramientas adecuadas consiste principalmente en comprender qué funciones ofrecen en términos de automatización y supervisión. Si está buscando una opción de servicio administrado, asegúrese de entender el acuerdo de nivel de servicio (SLA) del proveedor que describe sus garantías en cuanto a confiabilidad y disponibilidad. Con la arquitectura de datos, como hemos mencionado a lo largo de esta semana de materiales, es necesario analizar cómo una herramienta determinada proporciona modularidad e interoperabilidad con otras herramientas. La buena modularidad e interoperabilidad permiten flexibilidad y un acoplamiento flexible entre los componentes. En cuanto a la orquestación, el panorama de la ingeniería de datos está dominado actualmente por Apache Airflow, que se puede implementar como una herramienta gestionada o de código abierto. También hay otras ofertas como Prefect, Dagster y Mage, que también están ganando popularidad. Si está buscando herramientas de orquestación, tenga en cuenta que el espacio está evolucionando rápidamente y un conocimiento profundo de sus propios objetivos de arquitectura de datos lo ayudará a determinar qué herramientas de orquestación se adaptan mejor a sus necesidades. Cuando se trata de ingeniería de software, la gran pregunta es: ¿cuánto quieres hacer? Lo que quiero decir con esto es, dada su evaluación del ancho de banda y la experiencia de su equipo, así como del tipo de actividades de desarrollo que realmente agregan valor a su organización. ¿Quiere crear su propia herramienta, optar por una opción de código abierto o suscribirse a una solución comercial de código abierto o patentada? Lo principal que se debe evitar es el trabajo pesado indiferenciado, es decir, el trabajo arduo que no agrega valor a la organización. Consulte primero las herramientas de código abierto y comerciales de código abierto. Si no pueden satisfacer tus necesidades, busca herramientas propietarias. Eso es todo para esta breve descripción general de cómo cada una de las corrientes subyacentes del ciclo de vida de la ingeniería de datos entra en juego a la hora de elegir las herramientas y tecnologías para implementarlas en su arquitectura de datos. Una vez más, hemos hablado mucho sobre la ingeniería de datos de forma un tanto abstracta. Sé que puede parecer que hasta ahora nos hemos centrado mucho en la teoría, y probablemente te entusiasme empezar a practicar estos conceptos que has aprendido. Eso es genial, porque pronto habrá mucha práctica práctica. Únase a mí en la siguiente lección para analizar el marco bien diseñado de AWS y evaluar las opciones de arquitectura para su propia arquitectura de datos. En AWS.

Hasta ahora, en este curso, ha estado aprendiendo sobre los fundamentos de la arquitectura de datos. Y las numerosas consideraciones que debe tener en cuenta al diseñar y elegir herramientas para su propia arquitectura de datos. En esta lección, empezaremos por analizar el marco bien diseñado de AWS. Este marco se compone de un conjunto de principios y prácticas recomendadas que le ayudarán a crear arquitecturas sólidas y escalables en AWS. El marco bien diseñado complementa el conjunto de principios y prácticas de los que ya hemos hablado en este curso. De hecho, al escribir el libro sobre los fundamentos de la ingeniería de datos y articular los principios clave, nos pareció que eran los más importantes para diseñar una buena arquitectura de datos. Mi coautor Matt Housley y yo nos inspiramos en el marco bien diseñado de AWS, así como en otras fuentes. Si realiza una búsqueda rápida del marco de arquitectura AWS well, encontrará todo tipo de recursos excelentes. Incluye un documento técnico centrado en casos de uso específicos y una herramienta que AWS proporciona para ayudarlo a aplicar este marco. De hecho, el marco en sí mismo podría ser el tema de todo un curso. Para los fines de este curso, en el siguiente video, Morgan le dará una introducción a los seis pilares clave de este marco. Y le mostrará dónde puede ir para obtener más información y practicar más con el marco. Y después de eso, nos vemos en el siguiente vídeo para ver un recorrido por el ejercicio de laboratorio de esta semana. Tendremos la oportunidad de aplicar los principios de una buena arquitectura de datos en la nube de AWS.

Cuando crea sistemas en AWS, especialmente cuando acaba de empezar, puede resultar abrumador. Hay muchísimas opciones entre las que elegir y diferentes maneras de crear soluciones en AWS. Entonces, ¿cómo sabes si lo estás haciendo bien? Bueno, de eso se trata el marco de buena arquitectura de AWS: evaluar y mejorar las soluciones para que pueda hacerlo bien cuando cree en AWS. En este vídeo, le proporcionaré una descripción general del marco de buena arquitectura y le indicaré más recursos para que pueda utilizar este marco para evaluar cualquier cosa que cree en AWS. Empecemos con un poco de contexto. En AWS, no solo brindamos servicios para crear sistemas en la nube. También trabajamos en estrecha colaboración con miles de clientes de todo el mundo para ayudarlos a crear las mejores soluciones en la nube para respaldar y administrar sus negocios. Los arquitectos de soluciones de AWS, los expertos en la materia y otro personal de AWS llevan décadas trabajando directamente con los clientes en una amplia variedad de necesidades empresariales y casos de uso, lo que les ha brindado mucha experiencia a la hora de hacer las cosas de la manera correcta. A partir de esa experiencia colectiva, AWS ha creado el marco de buena arquitectura, que comprende un conjunto de prácticas recomendadas y estrategias básicas para diseñar sistemas en la nube. El marco de buena arquitectura de AWS incluye seis pilares clave: excelencia operativa, seguridad, confiabilidad, eficiencia del rendimiento , optimización de costos y sostenibilidad. A continuación, describiré brevemente cada uno de estos pilares y, si está interesado, puede seguir los enlaces de la sección de recursos al final de esta semana para obtener más información. El primer pilar es la excelencia operativa. Este pilar se centra en cómo puede desarrollar y ejecutar sus cargas de trabajo en AWS de manera más eficaz, supervisar sus sistemas para obtener información sobre sus operaciones y mejorar continuamente sus procesos y procedimientos para ofrecer valor empresarial. El siguiente es el pilar de la seguridad, que se centra en cómo aprovechar las tecnologías de la nube para proteger sus datos, sistemas y activos. Ya ha analizado el trasfondo de seguridad del ciclo de vida de la ingeniería de datos con Joe, y la idea aquí es la misma. Debe emplear las herramientas adecuadas para proteger sus sistemas y fomentar una cultura de seguridad en su equipo. El siguiente es el pilar de la fiabilidad. Los sistemas confiables son aquellos que realizan la función prevista de manera correcta y consistente y pueden recuperarse rápidamente de una falla. Por lo tanto, este pilar abarca todo, desde el diseño para garantizar la confiabilidad hasta la planificación para los fallos y la adaptación al cambio. El pilar de la eficiencia del rendimiento se centra en adoptar un enfoque basado en datos para crear arquitecturas de alto rendimiento. En lo que respecta a la eficiencia del rendimiento de su sistema, evaluará la capacidad de un conjunto de recursos informáticos para cumplir de manera eficiente los requisitos del sistema, así como la forma en que puede mantener esa eficiencia a medida que la demanda cambia y las tecnologías evolucionan. El siguiente pilar es la optimización de costos. Este es bastante sencillo y está estrechamente relacionado con los puntos que Joe señaló a principios de esta semana sobre la adopción de FinOps. En pocas palabras, la optimización de costos significa crear sistemas para ofrecer el máximo valor empresarial al precio más bajo posible, y AWS ofrece una gama de servicios, incluidos el explorador de costos y el centro de optimización de costos de AWS, donde puede hacer comparaciones y obtener recomendaciones sobre cómo optimizar el costo de sus sistemas. Por último, tenemos el pilar de la sostenibilidad. Si bien aspectos como el rendimiento, la escalabilidad, la seguridad y el costo pueden ser lo más importante a la hora de crear sistemas de datos, también es importante tener en cuenta el impacto ambiental de las cargas de trabajo que ejecuta en la nube. El pilar de sostenibilidad se centra en reducir el consumo de energía y aumentar la eficiencia en todos los componentes de su sistema. Ahora bien, es importante tener en cuenta que estos seis pilares del marco de buena arquitectura no proporcionarán diseños específicos que pueda copiar y aplicar a sus soluciones. Por el contrario, puede considerarlos como un conjunto de principios y preguntas que lo ayudarán a mantener conversaciones productivas sobre sus soluciones existentes y a diseñar y operar sistemas confiables, seguros, eficientes, rentables y sostenibles en la nube. Es casi como tener acceso a su propio arquitecto de soluciones de AWS personal, que puede ayudarlo a analizar las ventajas y desventajas de las diferentes opciones de arquitectura. Como mencioné anteriormente, puede seguir los enlaces de la sección de recursos al final de esta semana para obtener más información sobre cada uno de los pilares y explorar la herramienta Well-Architected que le permite evaluar sus propias arquitecturas para detectar posibles riesgos y oportunidades de mejora. También hay aplicaciones específicas del marco de buena arquitectura llamadas Lenses que puede explorar. Básicamente, una lente es una extensión del marco de buena arquitectura de AWS que se centra en un área , sector o conjunto de tecnologías en particular y proporciona orientación específica para esos contextos. Cada lente tiene su propio conjunto de preguntas, mejores prácticas, notas y plan de mejora. En particular, te recomiendo que consultes la lente del análisis de datos, que se centra en consideraciones específicas de los datos. La lente del análisis de datos lo guiará a través de la evaluación de sus arquitecturas de datos en cuanto a escalabilidad, seguridad, rendimiento y costo. Puede ayudarlo a evaluar sus arquitecturas de datos actuales, identificar áreas de mejora e implementar estrategias que se alineen con las mejores prácticas de la industria. A continuación, es su turno de intentar aplicar los principios de los que hemos estado hablando esta semana a su propia arquitectura de datos en AWS. En los próximos vídeos, Joe te explicará los ejercicios de laboratorio de esta semana y nos vemos de nuevo la semana que viene.

