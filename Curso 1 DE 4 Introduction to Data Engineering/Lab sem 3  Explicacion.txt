En el laboratorio de la semana pasada, creó esta canalización de datos para ingerir, transformar y entregar datos al analista de datos de su empresa. Ahora, su empresa quiere ofrecer algunos paneles integrados a sus clientes y compartir algunos datos analíticos a través de plataformas de intercambio de datos de terceros. Por lo tanto, se le ha encomendado la tarea de compartir estos datos transformados con un público más amplio. Para entregar estos datos a clientes externos, supongamos que creó una aplicación web que se ejecuta en estas instancias de EC2. En este laboratorio, trabajará con esta aplicación web para garantizar que sea capaz de escalar para satisfacer las necesidades de sus clientes, utilice los recursos informáticos de manera eficiente y esté diseñada de manera segura y confiable. También supervisará el rendimiento de esta aplicación con Amazon CloudWatch y, al mismo tiempo, aplicará los principios de una buena arquitectura de datos y seguirá el marco de buena arquitectura de AWS. Antes de empezar con el laboratorio, analicemos más de cerca estas instancias de EC2 para comprender la arquitectura subyacente de la aplicación web. Este es el diagrama arquitectónico de la aplicación web. Esta aplicación web está diseñada en base a una arquitectura de tres niveles que consta de un nivel de datos, un nivel lógico y un nivel de presentación. Este tipo de arquitectura de tres niveles es una forma habitual de implementar soluciones de aplicaciones web. El bucket de S3 que ve aquí a la izquierda representa el nivel de datos del que su aplicación web extrae los datos. Las instancias EC2 y el balanceador de cargas del medio representan el nivel lógico, que aloja la lógica de la aplicación que procesa las solicitudes del cliente. El nivel lógico consulta los datos del nivel de datos y devuelve los resultados al nivel de presentación que se encuentra a la derecha, que consiste en una interfaz que muestra los resultados a los clientes que interactúan con la aplicación web a través de sus dispositivos. En este ejemplo, puede pensar en la página web que muestra los paneles analíticos a los clientes como la capa de presentación. Vamos a ampliar el nivel lógico del medio. Aquí hay dos componentes principales: el grupo Amazon EC2 Auto Scaling y el balanceador de carga de aplicaciones (ALB). El grupo Auto Scaling está formado por una colección de instancias EC2 que ejecutan la misma lógica de aplicación. Se utilizan para aumentar las capacidades informáticas de la aplicación. Por lo tanto, en lugar de tener una única instancia de EC2 que procese todas las solicitudes de los clientes, estas solicitudes se distribuyen entre las instancias de EC2. El escalado automático significa que la cantidad de instancias EC2 puede aumentar o disminuir en función de la cantidad de solicitudes de los clientes a la aplicación web. El grupo está configurado para comenzar con dos instancias, cada una lanzada en una zona de disponibilidad diferente para mejorar la disponibilidad y la confiabilidad de la aplicación. El grupo Auto Scaling debe estar asociado a un balanceador de carga de aplicaciones que ayude a distribuir el tráfico entrante entre las instancias EC2 y sirva como punto de contacto único para los clientes. En este laboratorio, interactuará principalmente con los recursos informáticos en los que se ejecuta la lógica de la aplicación. Simulará el tráfico de la aplicación web para evaluar su escalabilidad y, a continuación, supervisará los recursos informáticos y la actividad de la red en la aplicación mediante Amazon CloudWatch. Configurará las instancias EC2 para permitir la eficiencia del rendimiento y la optimización de costos, y ajustará las opciones de seguridad del balanceador de carga para controlar el tráfico entrante a la aplicación. Para comenzar el laboratorio, abrirás el elemento de laboratorio en Coursera y esperarás a que los recursos del laboratorio estén listos. Una vez que se cargue el entorno de laboratorio, encontrará las instrucciones del laboratorio en esta asignación C1 W3 del archivo Markdown, que puede abrir haciendo clic primero con el botón derecho en el archivo y, a continuación, seleccionando Abrir vista previa. Para este laboratorio, seguirá las instrucciones del laboratorio que aparecen aquí en Markdown y utilizará la consola de administración de AWS para supervisar y actualizar la configuración de la aplicación web. Para acceder a la consola, primero ejecutará este comando en la terminal. Obtendrá este enlace que puede abrir en una nueva pestaña del navegador para acceder a la consola de AWS. La primera sección de las instrucciones aquí repasa la introducción que presenté en este video. En el siguiente vídeo, lo guiaré por las secciones 2 y 3 del laboratorio, donde explorará y supervisará los recursos informáticos de su aplicación. Te veré allí.

En este vídeo, le proporcionaré una vista previa de las tareas descritas en las secciones 2 y 3 de las instrucciones de laboratorio, en las que se centrará en supervisar las instancias EC2 del grupo de escalado automático y las actividades de red a través del balanceador de carga de aplicaciones. El grupo de ajuste de escala automático y el balanceador de carga de aplicaciones están disponibles para que no tengas que crearlos. Para acceder a ellos desde la consola, puede escribir EC2 aquí en la barra de búsqueda y, a continuación, hacer clic en ese servicio. Aquí, en el panel izquierdo, encontrará una sección para las instancias EC2 reales y, en la parte inferior del panel, encontrará las secciones para el equilibrio de carga y el ajuste de escala automático. Para la primera tarea de este laboratorio, que corresponde a la sección 2 de las instrucciones del laboratorio, necesitará encontrar la dirección de la página web de la aplicación. Para eso, debe verificar el balanceador de carga, ya que sirve como el principal punto de contacto para los clientes. Por lo tanto, en Equilibrio de carga, harás clic en los balanceadores de carga para ver el balanceador de carga de tu aplicación. Debajo del nombre DNS, copiarás la dirección de la página web y pegarás el enlace en otra pestaña. Esta página representa la interfaz a través de la cual los clientes interactúan con la aplicación y se supone que debe mostrar un panel para los clientes, pero para simplificar las cosas, se muestra un mensaje sencillo que muestra algunos detalles de la instancia EC2 que aloja la lógica de la aplicación. Mantendré esta pestaña abierta, porque necesitaremos esta información más adelante en el laboratorio. Ahora, cuando los clientes usen tu aplicación web, recibirás algunas solicitudes y tráfico entrante que deben procesarse. Por ejemplo, al abrir la página web de la aplicación se realiza una solicitud HTTP que el balanceador de cargas recibe y, a continuación, la entrega a una instancia EC2 para su procesamiento. Para garantizar que la aplicación pueda soportar las demandas entrantes, tendrá que supervisar el tráfico entrante y el uso de los recursos informáticos de su aplicación web. La observabilidad y el monitoreo son una práctica importante dentro de DataOps y se enmarcan en el pilar de excelencia operativa del marco de buena arquitectura de AWS. Por lo tanto, en la sección 3 de este laboratorio, utilizaremos Amazon CloudWatch para realizar un seguimiento de las métricas de rendimiento de su aplicación web. En primer lugar, tendrás que simular parte del tráfico entrante al sitio web mediante una herramienta de código abierto llamada Apache Benchmark. Puedes usar esta herramienta para realizar una prueba de esfuerzo, que implica enviar una gran cantidad de solicitudes a tu aplicación web. A continuación, supervisará el uso de la CPU de sus instancias EC2 y la actividad de red en Amazon CloudWatch. Primero instalemos la herramienta Apache Benchmark con AWS CloudShell, que es un servicio de línea de comandos que está disponible directamente desde la consola de AWS. Por lo tanto, desde la consola, puede buscar y abrir CloudShell. A continuación, copiará el comando de instalación de las instrucciones de laboratorio y lo ejecutará en este terminal. El siguiente comando de las instrucciones de laboratorio generará la prueba de esfuerzo mediante el envío de varias solicitudes HTTP. En este comando, puede usar la opción «-n» para especificar el número total de solicitudes HTTP y la opción «-c» para especificar el número de solicitudes simultáneas. Así que aquí estamos enviando 7000 solicitudes en total, y se enviarán 50 a la vez. Para el último argumento de este comando, debe reemplazarlo por la dirección real de su página web. Volveré a la pestaña que muestra la página web de la aplicación, copiaré la dirección y la pegaré aquí para reemplazar el último argumento. Las solicitudes ya se están enviando, por lo que puedo comprobar el uso de la CPU y las actividades de red de las instancias EC2 del grupo de escalado automático. Desde la consola, buscaré EC2. Luego, en el lado izquierdo, me desplazaré hacia abajo hasta la sección de grupos de escalado automático, abriré el grupo proporcionado, iré a la pestaña de monitoreo y elegiré EC2. Aquí me centraré en estos tres gráficos. La primera muestra la utilización de la CPU para procesar las 7.000 solicitudes. Puede observar el aumento de los recursos informáticos necesarios para procesar el tráfico entrante. También puede observar un patrón similar en estos dos grupos que supervisan las actividades de red entrantes y salientes en bytes. Transcurridos unos minutos, una vez que se hayan procesado las 7000 solicitudes, si actualiza la interfaz de usuario, verá una disminución en la utilización de la CPU y las actividades de red. En este vídeo, recorrimos las secciones dos y tres del laboratorio para obtener la dirección de su aplicación web, simular el tráfico entrante y supervisar el uso de la CPU y las actividades de la red. Acompáñeme en el siguiente vídeo para explorar los aspectos de seguridad, disponibilidad, escalabilidad y costes de su aplicación web.

Como recordatorio, en el vídeo anterior simulamos parte del tráfico de su aplicación web y supervisamos el uso de la CPU y las actividades de la red. En este vídeo, le proporcionaré una vista previa de las tareas descritas en las secciones 4 a 6 de las instrucciones de laboratorio para que pueda empezar a pensar en cómo aplicar los principios de una buena arquitectura de datos para garantizar la seguridad, la disponibilidad y la escalabilidad de sus aplicaciones y, al mismo tiempo, optimizar los costos. Como se indica en el principio de priorizar la arquitectura de seguridad y en el pilar de seguridad del marco de arquitectura Will de AWS, siempre debe priorizar la seguridad al crear cualquier aplicación o sistema de datos. Para controlar el acceso a la aplicación web en este laboratorio, puedes configurar el balanceador de cargas para que solo reciba ciertos tipos de solicitudes y bloquee otras. En las redes, cuando un cliente envía una solicitud al servidor web, necesita la dirección y el número de puerto. Un número de puerto es un identificador virtual que las aplicaciones utilizan para diferenciar los tipos de tráfico, de modo que cada puerto se pueda asociar a un proceso específico. Para facilitar las cosas a los programadores, de forma predeterminada se asignan números de puerto específicos a las solicitudes populares. Así, por ejemplo, el puerto 80 está asignado a las solicitudes HTTP. Sin embargo, si las reglas de seguridad del balanceador de cargas no están configuradas correctamente, los clientes externos aún pueden acceder a otros puertos. Para la aplicación web utilizada en este laboratorio, las instrucciones del laboratorio indicaron que algunos datos privados podrían filtrarse a través del puerto 90 del balanceador de cargas debido a configuraciones incorrectas. Para verificarlo, iré a la pestaña que muestra la página web, copiaré la dirección y la pegaré en otra pestaña. Pero ahora añadiré dos puntos 90 para especificar el puerto 90. El mensaje aquí muestra que hay algunos datos privados que se muestran a través de este puerto y que terceros pueden acceder a ellos fácilmente. Para solucionar este problema, debes ajustar las reglas de seguridad conocidas como grupos de seguridad del balanceador de cargas. Abramos EC2 desde la consola, naveguemos hasta la sección Red y seguridad del panel izquierdo y, a continuación, hagamos clic en Grupos de seguridad. Puede ver dos grupos de seguridad aquí. La primera está asociada a las instancias EC2 del grupo de escalado automático y la segunda está asociada al balanceador de cargas. Como mencioné anteriormente, el balanceador de cargas actúa como el principal punto de contacto para los clientes y debe configurarse para recibir tráfico de Internet externo, que es lo que ajustará en esta sección del laboratorio. Pero antes de ajustarlos, verifiquemos que los grupos de seguridad de las instancias EC2 estén configurados para recibir solo tráfico del balanceador de cargas. Si haces clic en el ID del grupo de seguridad de EC2 y te desplazas hacia abajo para comprobar las reglas de entrada, verás que esta regla tiene el ID del grupo de seguridad del balanceador de carga como fuente, lo que significa que las instancias de EC2 solo pueden recibir tráfico del balanceador de cargas según lo previsto. Muy bien, volvamos y ajustemos las configuraciones del grupo de seguridad del balanceador de cargas. Cuando abras el grupo de seguridad del balanceador de cargas, encontrarás esta regla que especifica un montón de ceros como origen, lo que significa que acepta solicitudes de todas las direcciones IP y también acepta todos los números de puerto posibles para el rango de puertos. Esto significa que cualquier fuente puede acceder a un balanceador de cargas mediante cualquier puerto, lo cual es una regla muy abierta. En otras palabras, con una configuración como esta, su aplicación web está abierta a cualquier dirección IP y número de puerto para la Internet pública, y eso no es lo que desea. Solucionemos esto restringiendo las solicitudes HTTP solo al puerto 80. Haré clic en Editar reglas de entrada y, a continuación, en Agregar reglas. En Intervalo de puertos, especificaré el puerto 80 y, en Origen, seleccionaré Todas las direcciones IP especificando este bloque. A continuación, eliminaré la primera regla y, a continuación, haré clic en Guardar reglas. Ahora puede ver la regla actualizada con 80 como rango de puertos. Verifiquemos que el puerto 90 ya no sea accesible desde la Internet pública. Abriré una pestaña, ingresaré la dirección de la aplicación y agregaré dos puntos 90. Es posible que siga viendo el resultado anterior porque los navegadores web pueden almacenar en caché los resultados. Pero si sigues actualizando la página web, te darás cuenta de que ya no puedes acceder a la página con los datos privados y, si esperas lo suficiente, recibirás este mensaje de error que dice que no se puede acceder al sitio. Así es como puedes ajustar los controles de seguridad de tu aplicación web. A continuación, pasemos a la sección 5 de las instrucciones de laboratorio y exploremos el aspecto de confiabilidad de la aplicación. El diagrama de arquitectura nos indica que el grupo de escalado automático está configurado para comenzar con dos instancias EC2, cada una lanzada en una zona de disponibilidad diferente. Para comprobarlo, voy a ir a la pestaña que muestra la página web de la aplicación. El mensaje que se muestra muestra la dirección IP interna de la instancia EC2 que procesó las solicitudes HTTP, así como su zona de disponibilidad. Actualizaré esta página y ahora verá la IP y la zona de disponibilidad de la segunda instancia EC2. Esto significa que la solicitud la procesa una instancia EC2 diferente cada vez. El diseño de aplicaciones para abarcar múltiples zonas de disponibilidad está relacionado con el pilar de confiabilidad de las soluciones basadas en la nube y con el principio de planificación para el fracaso del que hablamos esta semana. De esta forma, si algo sale mal en una de las zonas de disponibilidad, el EC2 hospedado en otra zona aún puede procesar la solicitud. Por último, analicemos otros dos aspectos de la aplicación, la optimización de costos y la escalabilidad, que se tratan en la sección 6 de las instrucciones de laboratorio. Para adoptar FinOps, debe asegurarse de utilizar los recursos correctos de manera eficiente. Recuerde que la semana pasada hay diferentes tipos de instancias EC2, cada una con una potencia de procesamiento y una capacidad de memoria diferentes. Y, por supuesto, cuanto más potente sea la instancia EC2, más tendrá que pagar por el uso por horas. Actualmente, utilizamos microinstancias T3, pero si son demasiado potentes y costosas para su aplicación, puede cambiarse a las nanoinstancias T3 para reducir los costos. Para ello, debe ir al grupo de escalado automático y modificar la plantilla de lanzamiento, que contiene la información de configuración para lanzar las instancias EC2. En la consola de AWS, buscaré EC2 y, a continuación, me desplazaré hasta encontrar los grupos de escalado automático en el panel izquierdo. Haré clic en el nombre del grupo y buscaré la sección de plantillas de lanzamiento. Esta sección contiene la información de configuración de las instancias EC2 actuales, por lo que puede ver que el tipo de instancia actual es T3 micro. Vamos a editar esta plantilla. Aquí haré clic en crear una versión de plantilla de lanzamiento para crear una nueva versión de la plantilla de lanzamiento existente. Desmarcaré esta casilla en la guía de escalado automático y me desplazaré hacia abajo hasta la sección de tipos de instancia y buscaré T3.nano. Se dará cuenta de que la diferencia entre las nanoinstancias y las microinstancias es su capacidad de memoria y, por supuesto, su precio. Seleccionaré T3.nano y luego haré clic en crear versión de plantilla. Una vez que la plantilla se haya creado correctamente, volvamos a los grupos de ajuste de escala automático y, a continuación, volvamos a hacer clic en el nombre del grupo. En la sección de plantillas de lanzamiento, haré clic en editar. Luego, en el menú desplegable de versiones, seleccionaré la última opción. No olvides hacer clic en actualizar en la parte inferior. Acabo de actualizar las configuraciones de la plantilla de lanzamiento para todas las instancias EC2 futuras que se crearán como parte de este grupo. Pero también queremos terminar con las instancias T3 micro EC2 actuales que aún se están ejecutando. Para ello, accederé a la sección de instancias del panel izquierdo y seleccionaré todas las instancias EC2 que se están ejecutando actualmente. Luego haré clic con el botón derecho en cualquiera de ellos y haré clic en terminar instancia. El estado de las instancias EC2 ahora se está cerrando. Transcurridos unos minutos, una vez que actualice la interfaz de usuario, el estado de las instancias EC2 debería aparecer como terminadas. También observará que se han creado una o dos nuevas instancias T3 nano. Esto se debe a que el grupo de escalado automático tiene una capacidad deseada de dos instancias, lo que significa que siempre comenzará con dos instancias. Por lo tanto, si solo ves una instancia ejecutándose ahora, después de un tiempo, si actualizas la interfaz de usuario, verás dos nanoinstancias T3 ejecutándose. Para la última tarea de la sección seis de las instrucciones de laboratorio, aplicará el principio de la arquitectura para la escalabilidad aumentando y reduciendo sus recursos mediante la propiedad de escalado automático de los grupos de escalado automático. Con los grupos de escalado automático, solo pagas por lo que usas. Por lo tanto, cuando la demanda disminuya, el escalado automático de AWS eliminará automáticamente cualquier recurso adicional de EC2 para ayudarlo a evitar gastos excesivos. En este momento, la función de ajuste de escala automático no está habilitada. Para habilitar el escalado automático, tendrás que crear una política de escalado en la que especifiques las métricas y los valores de umbral que activen el proceso de escalado automático. Por lo tanto, en el panel izquierdo de la instancia EC2, naveguemos hasta la sección del grupo de escalado automático, hagamos clic en el nombre del grupo y vayamos a la pestaña de escalado automático. Aquí haré clic en la política de creación de escalado dinámico y le daré a la política el nombre que aparece en las instrucciones del laboratorio. Para el tipo de métrica, elegiré el recuento de solicitudes del balanceador de carga de aplicaciones por objetivo. Y para el grupo objetivo, elegiré el grupo del puerto 80. A continuación, elegiré 60 como valor objetivo, lo que significa que cuando haya más de 60 solicitudes HTTP, el grupo de ajuste de escala automático puede ampliarse para añadir más instancias a fin de gestionar el aumento de la carga. Y cuando haya menos de 60 solicitudes, el grupo de escalado automático se reducirá al reducir la cantidad de instancias. Y, por último, estableceré el calentamiento de la instancia en 60 segundos. Este tiempo de preparación hace referencia al período durante el cual las instancias recién lanzadas pueden inicializarse por completo antes de considerarlas en servicio para las métricas de evaluación del escalado automático. No olvides hacer clic en crear en la parte inferior. Ahora vamos a realizar una prueba de esfuerzo más intensa con Apache Benchmark enviando 1 millón de solicitudes en lugar de 7000. Haré clic en el icono de cloud shell que ves aquí en la parte inferior para poder usar una línea de comandos para realizar la solicitud y comprobar el estado de las solicitudes mientras superviso las métricas de rendimiento de la aplicación. Escribiré este comando y sustituiré el último argumento por la dirección de la aplicación. La prueba tardará unos minutos en ejecutarse. Mientras tanto, vayamos al servicio grupal de escalado automático y hagamos clic en monitoreo y luego en EC2. Aquí puede ver alguna actividad en el uso de la CPU y también en las métricas de la red. Tras unos minutos, puedes cambiar a la pestaña de actividad y comprobar que se han lanzado instancias adicionales para gestionar el aumento del tráfico. Así que ese fue un adelanto de lo que puede esperar del laboratorio de esta semana. Cuando pase por el laboratorio, asegúrese de seguir las instrucciones con cuidado y puede volver a visitar estos vídeos explicativos del laboratorio si es necesario. Una vez que haya completado todas las tareas del laboratorio, no olvide enviar el laboratorio para que cuente para completar este curso. Tenga en cuenta que el entorno de laboratorio caducará después de dos horas, así que asegúrese de enviarlo antes de que se acabe el tiempo. Después de eso, nos vemos de nuevo aquí para terminar el contenido de esta semana.

