Antes de que pruebes el primer laboratorio de la especialización, he preparado tres vídeos cortos para explicarte qué esperar de este ejercicio de laboratorio. En este primer vídeo, te presentaré el escenario del laboratorio y te daré una visión general de las tareas que realizarás como ingeniero de datos. Luego, en los dos vídeos siguientes, te explicaré cómo configurar el laboratorio y te daré una vista previa del contenido real del laboratorio. En este laboratorio, explorará una canalización de datos de extremo a extremo en AWS que abarca todas las etapas del ciclo de vida de la ingeniería de datos. Para su sistema fuente, se le proporcionará una base de datos relacional. Tras explorar esta fuente de datos, incorporará los datos a su canalización de datos, donde los transformará y cargará en el sistema de almacenamiento antes de entregarlos finalmente a los usuarios finales. En la etapa de transformación, convertirá la estructura de los datos de origen en un formulario que sea más fácil de entender y más rápido de consultar para el usuario final. El laboratorio contiene instrucciones detalladas que muestran cómo crear y ejecutar los recursos de su canalización de datos. Si no comprende completamente los detalles de cada etapa, aprenderá más sobre todas las herramientas involucradas en cada etapa en los próximos cursos de la especialización. Por ahora, el objetivo principal de este laboratorio es ayudarlo a empezar a interactuar con la canalización de datos en AWS. El escenario de este laboratorio es que trabajes como ingeniero de datos en un minorista especializado en modelos a escala de automóviles clásicos y otros vehículos. Su cliente almacena los datos con fines históricos y la información de sus clientes en una base de datos relacional que consta de estas tablas. Clientes, productos, líneas de productos, pedidos, detalles de pedidos , pagos, empleados y oficinas. Se le otorga acceso a esta base de datos y se le pide que cree una canalización de datos para transformar estos datos y entregarlos a un analista de datos del equipo de marketing. El analista de datos está interesado en analizar los registros históricos de compras para comprender, por ejemplo, qué líneas de productos tienen más éxito y cómo se distribuyen las ventas en los diferentes países. Su trabajo consistirá en extraer los datos que el analista necesita de la base de datos, transformarlos en una estructura que sea más fácil de entender y más rápida de consultar con fines analíticos y, a continuación, almacenarlos en un sistema de almacenamiento independiente al que tenga acceso el analista. La transformación de sus datos en una estructura para su uso posterior como esta se conoce como modelado de datos, que trataremos en detalle en el curso 4 de la especialización. En este laboratorio, se le proporcionan el script de transformación y la estructura de los datos transformados. Así es como se estructurarán sus datos transformados al final. Esto se conoce como esquema estelar. La tabla central contiene algunas medidas relacionadas con un pedido de venta que el analista de datos podría estar interesado en agregar, como el número total de ventas o el precio promedio. Las tablas circundantes proporcionan más contexto, como las ubicaciones de los clientes y los detalles de los pedidos, lo que permite realizar agregaciones más específicas, como el número total de ventas por país o las cantidades máximas solicitadas para cada línea de productos. En la terminología de un modelo de datos de esquema estelar, esta tabla central se conoce como tabla de hechos y las tablas circundantes se denominan dimensiones. Este es el diagrama arquitectónico de la canalización con la que trabajarás. La instancia de base de datos MySQL de RDS representa un sistema de origen que contiene las tablas. Este recurso se le proporcionará en el laboratorio. Luego tiene AWS Glue, que es una herramienta que le permite ingerir datos de la base de datos de origen y aplicar transformaciones sobre la marcha a los datos ingeridos. En el laboratorio, definirá un trabajo de unión, que consiste en conectarse a la base de datos de RDS, extraer los datos sin procesar, transformar los datos mediante su modelado mediante el esquema en estrella proporcionado y, por último, cargar los datos transformados en AWS Object Storage en un bucket de S3. Este balde también se le entregará en el laboratorio. El proceso de extraer, transformar y, a continuación, cargar los datos se conoce como ETL, sobre el que obtendrá más información en la semana 3 de este curso y en el siguiente curso. Ahora, el Glue Crawler que ve aquí es una herramienta de administración de datos que puede rastrear automáticamente su almacenamiento de S3 para inferir la estructura de los datos y, a continuación, escribir los metadatos en una tabla del catálogo de datos de AWS Glue, que es solo una base de datos que contiene información sobre los datos utilizados como fuente y destino en el trabajo de ETL. Esto permite al analista de datos descubrir los datos almacenados en S3 mediante un servicio de consultas, como Amazon Athena, para recuperar los datos de S3 mediante consultas similares a las de SQL. En el laboratorio, creará la ETL de AWS Glue y el AWS Glue Crawler. Puede crear estos recursos manualmente con la consola de AWS o crearlos mediante programación mediante código. En este laboratorio, explorará la forma programática de usar Terraform, que es una herramienta de código infraestructuralista que le permite configurar recursos mediante código. El código de Terraform se le proporcionará en el laboratorio, pero aprenderá más sobre Terraform en el curso 2 de la especialización. Por último, el analista de datos utilizará Amazon Athena para consultar los datos de S3. En este laboratorio, para ofrecerle una experiencia integral con la canalización de datos, se pondrá en la piel del analista de datos para realizar consultas sobre los datos transformados. Por lo tanto, después de crear y ejecutar los recursos de su canalización de datos, ejecutará un cuaderno de Jupyter para realizar algunos análisis de los datos transformados. Ahora, para acceder a las instrucciones del laboratorio y al código proporcionado, utilizará VS Code como entorno de desarrollo integrado, o IDE, que estará listo para usted después de abrir el laboratorio. Este es el mismo entorno de laboratorio que utilizará para completar todos los laboratorios de este programa. Por lo tanto, en el siguiente vídeo, le proporcionaré una descripción general rápida de las características de este entorno de laboratorio, para que sepa qué esperar cuando abra el laboratorio por primera vez.

En este vídeo, veremos el entorno del laboratorio y cómo acceder a las instrucciones y al Notebook de Jupyter necesarios para tu primer laboratorio en este curso. Siéntete libre de abrir el laboratorio tú mismo en otra pestaña del navegador mientras hago esto, o simplemente míralo por ahora y dirígete al laboratorio más tarde. Así que aquí estoy en Coursera, y he hecho clic en el laboratorio de la semana, que es este llamado Asignación de Programación. Si te desplazas hacia abajo, verás algunas instrucciones genéricas sobre cómo abrir, reiniciar y enviar el laboratorio, que vamos a repasar en este vídeo. También verás estas instrucciones genéricas al principio de cada laboratorio, así que asegúrate de leerlas detenidamente antes de empezar cualquier laboratorio. Para abrir el laboratorio, haré clic en este recuadro azul y esperaré a que se cargue el entorno del laboratorio. Ten en cuenta que la primera vez que inicies el laboratorio, el entorno debería cargarse en unos segundos. Sin embargo, los siguientes intentos pueden tardar unos 10 minutos, porque la cuenta de AWS que fue creada necesita pasar por un proceso de limpieza, y porque los recursos para el laboratorio necesitan ser creados de nuevo. Bien, ahora el laboratorio está listo. Aquí está la interfaz de VS Code. A la izquierda, verás este archivo Markdown, C1W2Assignment, que contiene los pasos de laboratorio que seguirás para crear y ejecutar tu primera canalización de datos. Después de eso, abrirás el archivo de cuaderno C1W2Dashboard para realizar algunos análisis sobre los datos transformados. Ahora abriré el archivo Markdown. Haré clic con el botón derecho en el archivo y haré clic en Open Preview. Aquí están las instrucciones detalladas del laboratorio. La primera sección representa la introducción, que ya cubrimos en el vídeo anterior. Vamos a repasar ahora esta parte de la segunda sección para mostraros cómo podéis abrir la consola de AWS desde aquí, y en el próximo vídeo, os guiaré a través de las secciones restantes. Para abrir la consola de AWS, copiaré este comando y lo pegaré en el terminal para obtener un enlace en la consola. A continuación, copiaré este enlace y lo pegaré en otra pestaña. Aquí tienes la consola de AWS. Te sugiero que mantengas esta pestaña abierta mientras realizas el laboratorio, porque se te pedirá que compruebes en la consola de AWS los recursos que se te proporcionen o que vayas a crear. Si cierras esta pestaña, entonces después de 15 minutos, el enlace en la consola de AWS expira y necesitas ejecutar este comando de nuevo para obtener un nuevo enlace. Pero si mantienes esta pestaña abierta, el enlace no expirará. Ahora de vuelta a la terminal aquí. En muchos de los laboratorios de este programa, utilizarás el terminal para ejecutar comandos para crear algunos recursos de AWS, verificar su estado o información, o conectarte a una base de datos. Y eso es lo que practicarás en este primer laboratorio. Así que mantendré el terminal abierto. Pero en cualquier momento en que la terminal se llene, siempre puedes borrarla usando el comando clear. Siempre puedes volver atrás en el historial para encontrar un comando anterior usando la tecla de flecha hacia arriba. Si cierras esta terminal por error, puedes abrirla de nuevo haciendo clic en ver y luego terminal, o puedes crear otra terminal. Ahora, si desea ocultar esta lista de archivos de la izquierda, puede hacer clic en este icono. Y si desea mantenerlos visibles, puede hacer clic de nuevo en el mismo icono. Como puede ver, VS Code utiliza un tema oscuro por defecto. Si prefieres usar otro tema, puedes hacer clic en archivo, preferencias, y luego color tema. Aquí puedes elegir un tema de tu elección. Para mí, me quedaré con el que viene por defecto. Ahora, cuando termines todos los pasos del laboratorio, volverás aquí y enviarás el laboratorio haciendo clic en este botón, que te mostrará un informe de las secciones calificadas del laboratorio y la puntuación que has obtenido en cada una. Por último, mientras realizas este laboratorio y todos los laboratorios de este programa, ten en cuenta que todos los recursos de AWS estarán disponibles durante dos horas. Así que después de dos horas, te aparecerá este mensaje informándote de que la sesión del laboratorio ha finalizado. Y para reiniciar el laboratorio, tendrás que hacer clic en este signo de interrogación y luego en reiniciar. Esto creará de nuevo los recursos de AWS en tu cuenta. Por otro lado, todos los archivos del laboratorio seguirán disponibles con los cambios guardados. Así que ese es el entorno del laboratorio. En el siguiente vídeo, te guiaré a través de los siguientes pasos para crear tu canalización de datos. Nos vemos allí.

En este vídeo, te daré un avance de lo que harás en el laboratorio. Como recordatorio, en el vídeo anterior, pasamos por el entorno de laboratorio que se parecía a esto, donde tienes todos los archivos necesarios para el laboratorio en esta estructura de carpetas aquí a la izquierda, y has abierto el archivo Markdown, C1, W2, Asignación, que contiene las instrucciones detalladas para el resto del ejercicio de laboratorio. En estas instrucciones, encontrarás una introducción aquí en la parte superior, y luego tu trabajo comienza en la Sección 2, Explorando el Sistema Fuente. Como he mencionado antes, la base de datos Source System ya está instanciada para ti. Puedes ver los detalles de esta base de datos en la consola de AWS. Para ello, puedes ir a la consola y escribir RDS, y luego seleccionar ese servicio. Aquí a la izquierda, haré clic en Databases. Aquí está la instancia de la base de datos de origen. Si hago clic en el identificador, obtengo todos los detalles de esta base de datos, como el endpoint o la URL que permite acceder a la base de datos, el número de puerto y otra información de red, como la VPC y la subred donde está configurada la instancia de RDS. Para explorar la base de datos y la tabla almacenada en esta instancia, necesitarás este endpoint. También puedes obtener este endpoint a través del terminal ejecutando este comando. Tendrás que sustituir esta parte por un identificador de base de datos y, a continuación, ejecutarlo. Ahora aparece el mismo endpoint que viste en la consola. Necesitas establecer una conexión con esta base de datos antes de poder ver su contenido. Dado que la base de datos es una base de datos MySQL, puedes conectarte a ella escribiendo MySQL, luego especificando el host o el endpoint de la base de datos, el nombre de usuario, que en este caso es admin, la contraseña, y finalmente el puerto, que es 3306. Ahora que tienes la conexión establecida, puedes elegir la base de datos que quieres explorar con el comando Usar, y luego especificar el nombre de la base de datos, que es Modelos clásicos. Puedes ver las tablas dentro de esta base de datos escribiendo Mostrar tablas. Si tienes curiosidad, también puedes ver el script que se utilizó para rellenar esta base de datos subiendo aquí a la carpeta Data y abriendo este archivo llamado MySQLSampleDatabase.sql. Aquí puedes ver algo de información sobre la procedencia de los datos y un poco del historial de versiones. Si te desplazas hacia abajo, encontrarás un montón de código SQL. Primero hay una línea para crear la base de datos en sí, luego hay toda una serie de comandos create table que van poblando cada una de las tablas de la base de datos. Por ahora, no tendrás que preocuparte de todo este código SQL. En el próximo curso, tendrás la oportunidad de leer, escribir e interpretar comandos SQL. Por último, para salir de la conexión a la base de datos y continuar con las instrucciones del laboratorio, deberás escribir Exit. A continuación, vamos a crear los recursos para la canalización de datos, es decir, la instancia de AWS Glue y el rastreador de Glue. Para ello, se te proporcionan archivos Terraform que contienen código para crear y configurar esos recursos. Si haces clic en la carpeta Terraform aquí a la izquierda, todos estos archivos aquí que terminan con la extensión .tf son archivos Terraform. Así, por ejemplo, el archivo glue.tf contiene las configuraciones para la instancia de AWS Glue. Los otros archivos Terraform contienen configuraciones de red y permisos sobre los que aprenderás más en el curso dos. Por ahora, voy a hojear rápidamente el archivo glue.tf para mostrarte los recursos declarados en este archivo. Así que aquí está el catálogo de datos de AWS Glue, y vamos a definir una conexión entre AWS Glue y la base de datos de origen RDS. Aquí tenemos otro recurso que define un Glue crawler, que se utilizará para rastrear el S3 bucket. Por último, aquí tenemos el trabajo de Glue, donde especificamos la conexión con el sistema de origen, la ubicación del script que contiene el código de transformación, y la ubicación de destino para los datos transformados. Podemos encontrar el script bajo la carpeta asset, que consiste en el código que extrae los datos, los transforma y, finalmente, los carga en S3. En el cuarto curso profundizaremos en los detalles de AWS Glue. Si tienes curiosidad, también puedes echar un vistazo a las transformaciones que se aplicarán a los datos abriendo este script de Python llamado gluejob.py en la carpeta taraform-assets. Pero no necesitas preocuparte por los detalles aquí. Lo que estás haciendo con el script es tomar los datos en una forma llamada normalizada y modelarlos como un esquema de estrella de tablas de hechos y dimensiones para facilitar el análisis. Y esta es una práctica muy común en la ingeniería de datos de datos tabulares para análisis y casos de uso. Para crear realmente los recursos para este pipeline, necesitarás ejecutar los archivos taraform . En la terminal aquí, primero ejecutaré este comando para definir las variables de entorno que serán utilizadas por los archivos taraform. Después, navegaremos hasta el directorio taraform y comenzaremos con el comando taraform init. A continuación, puedes crear el comando taraform plan, que te muestra la lista de recursos que taraform está planeando crear. Por último, puedes ejecutar el comando taraform apply para crear los recursos. En este punto, volverás a ver el plan y tendrás que confirmar que quieres que taraform realice estas acciones. Escribiré yes y esperaré a que se creen los recursos. Una vez creados los recursos, podrás verlos a través de la consola de administración de AWS. Así que vamos a la consola y escribimos AWS Glue en la barra de búsqueda, pinchamos en ese servicio, y a la izquierda, pinchamos en ETL jobs. Aquí veremos la instancia que se acaba de crear. También podemos buscar el bucket de S3. Aquí tenemos el bucket que contendrá los datos transformados. Ahora que se han creado los recursos, vamos a ejecutar el trabajo de Glue. Copiaré este comando de las instrucciones del laboratorio y lo pegaré aquí en el terminal para iniciar el trabajo de Glue. Para monitorizar el estado del trabajo, vamos a la consola y buscamos de nuevo AWS Glue. Navegaremos hasta la sección de trabajos ETL y luego haremos clic en el nombre del trabajo. En la pestaña de ejecuciones, veremos el estado del trabajo, que ahora se está ejecutando. Es posible que tengamos que esperar dos o tres minutos para que el trabajo finalice. Puedes refrescar tu navegador para ver el último estado. Una vez que el trabajo de Glue haya finalizado y el estado muestre éxito, deberías tener los datos transformados en el bucket de S3. Verifiquémoslo accediendo al contenido de este bucket de S3. Aquí puedes encontrar las carpetas que contienen las tablas de los datos transformados. En la última parte de este laboratorio, explorarás cómo los conjuntos de datos pueden ser consultados por el analista de datos. Puedes hacer clic en este Notebook de Jupyter, que contiene celdas que realizan consultas analíticas sobre los datos transformados. Si quieres mostrar el archivo markdown en el cuaderno de Jupyter uno al lado del otro, puedes hacer clic en este icono. Pero en la última parte del laboratorio, ya no necesitarás las instrucciones detalladas, así que cerraré esta pestaña. En esta primera celda, el paquete AWS Wrangler importado aquí te permite utilizar Amazon Athena para extraer datos del bucket de S3. Aquí tienes la primera sentencia de consulta que extrae todos los productos de la tabla Dim Products. La siguiente celda contiene una consulta que encuentra el total de ventas por país. La siguiente celda contiene una consulta que agrupa aún más el número total de ventas por país, líneas de producto, fecha de pedido, línea de producto y nombre de producto. Si el código de estas sentencias de consulta SQL no le resulta familiar, no se preocupe. Obtendrá experiencia práctica con la escritura e interpretación de consultas en los cursos 2 y 3. Puede ejecutar este panel interactivo para explorar aún más los datos. Una vez que haya terminado con los pasos de este laboratorio, asegúrese de hacer clic en Enviar tarea aquí. Tenga en cuenta que el entorno de laboratorio caducará después de dos horas, así que asegúrese de enviar antes de que se acabe el tiempo. Ahora que tienes una mejor idea de lo que harás en el laboratorio de esta semana, es tu turno para probarlo. Por favor, sigue las instrucciones del laboratorio cuidadosamente, y siempre puedes volver a visitar estos vídeos si te quedas atascado en algún punto. Después de que hayas completado este laboratorio, te veré de nuevo aquí para un rápido resumen de la semana.

