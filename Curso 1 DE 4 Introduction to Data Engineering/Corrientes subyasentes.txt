Hemos estado analizando el ciclo de vida de la ingeniería de datos y cómo se incorporarán los datos de los sistemas de origen, los transformarán, los almacenarán y se los entregarán a los usuarios finales. Como campo, la ingeniería de datos está madurando rápidamente, mientras que hace solo una década, el papel de un ingeniero de datos se centraba principalmente en la capa tecnológica. La continua abstracción y simplificación de las herramientas ha ampliado el alcance de la función. La ingeniería de datos ahora abarca mucho más que solo herramientas y tecnologías. En otras palabras, el campo está ascendiendo en la cadena de valor, lo cual es una gran noticia para usted como ingeniero de datos. La ingeniería de datos moderna incorpora prácticas empresariales tradicionales, como la administración de datos y la optimización de costos, así como prácticas más nuevas, como DataOps. Cuando se trata de trabajar como ingeniero de datos, hay un conjunto de prácticas de este tipo que se aplicarán a su trabajo durante todo el ciclo de vida. En el libro sobre los fundamentos de la ingeniería de datos, describimos estas prácticas como las corrientes subyacentes del ciclo de vida de la ingeniería de datos. Estas corrientes subyacentes incluyen la seguridad, la administración de datos, DataOps, la arquitectura de datos, la orquestación y la ingeniería de software. En los próximos vídeos, analizaremos más de cerca cada una de estas corrientes subterráneas. Después de eso, comenzará a explorar cómo los datos que entran en el ciclo de vida y las corrientes subyacentes toman forma en la vida real en la nube de AWS. Vamos a empezar.

Antes de analizar cómo se aplica la seguridad a su función como ingeniero de datos, me gustaría que reflexionara un momento sobre cómo se aplican los problemas de seguridad a sus propios datos personales todos los días. Por ejemplo, es probable que no proporciones la información de tu cuenta bancaria a cualquiera y no publiques las contraseñas de todas tus cuentas en línea para que otros puedan verlas. Del mismo modo, no le darías las llaves de tu casa a alguien que no conozcas en fideicomiso. En su función de ingeniero de datos, se le confían datos confidenciales, ya sea la información personal y privada de sus clientes o la información empresarial patentada. Los propietarios de esos datos confían en que eso mantendrá su información segura. Es importante reconocer su papel a la hora de reunir el conjunto correcto de principios , protocolos y prácticas culturales para garantizar la seguridad de los sistemas que crea. En este vídeo, abordaré algunos de esos principios fundamentales y mejores prácticas para la seguridad de los datos. Cuando administre una canalización de datos y entregue datos a los usuarios finales, tendrá que dar acceso a los datos y los recursos del sistema a otros usuarios o aplicaciones. El principio de seguridad clave a tener en cuenta al hacerlo es el principio del privilegio de arrendamiento. Esto significa que usted otorga a los usuarios o las aplicaciones acceso solo a los datos y recursos esenciales que necesitan para realizar su trabajo y solo durante el tiempo que sea necesario. Debe aplicar el principio del privilegio de arrendamiento, no solo a los demás miembros de su organización, sino también a usted mismo. Esto significa, por ejemplo, que al igual que no concedes a todo el mundo acceso de administrador de equipo, cuando se trata de tu propio trabajo, no operes desde la consola raíz cuando no sea necesario y no utilices permisos de administrador o superusuario a menos que sea absolutamente esencial. Cuando piense en la mejor manera de proteger los datos en su trabajo, debe pensar no solo en el acceso a los datos, sino también en la confidencialidad de los datos. Adherirse al principio del privilegio de arrendamiento significa hacer que la información confidencial sea visible para los usuarios solo cuando sea absolutamente necesario. Más allá de eso, la mejor manera de proteger los datos confidenciales es no introducirlos en su sistema desde el principio. Si no tienes un propósito claro para ingerir y almacenar datos confidenciales, simplemente no lo hagas y habrás eliminado por completo el riesgo de que esos datos se divulguen accidentalmente. En el mundo actual centrado en la nube, la seguridad adquiere otra dimensión, que requiere que comprenda aspectos como la administración de identidades y accesos, o los Rals de mensajería instantánea, los métodos de cifrado y los protocolos de red. Encontrará más información sobre estos temas a lo largo de la especialización a medida que profundizaremos en cómo garantizar la seguridad de sus canalizaciones de datos. Más allá de eso, la seguridad no solo tiene que ver con principios y protocolos, sino también con las personas. La seguridad comienza y termina contigo, la persona, así como con otras personas de tu organización. Cuando se trata de seguridad, debes adoptar una mentalidad defensiva. Sea siempre cauteloso cuando se le pidan credenciales o datos sensibles y confidenciales. Imagine posibles escenarios de ataque y filtración y diseñe sus canalizaciones de datos y sistemas de almacenamiento teniendo en cuenta estos escenarios. Cuando se trata de filtraciones de datos del mundo real, las personas individuales han sido la fuente de muchas de las mayores brechas de seguridad. Las personas ignoran las precauciones básicas, como compartir sus contraseñas de forma insegura con otras personas, las personas que son víctimas de ataques de suplantación de identidad, en los que un atacante intenta robar información confidencial haciéndose pasar por una figura de autoridad en la que muchos confían, o personas que actúan de manera irresponsable mientras trabajan en los sistemas y datos de la empresa. En lo que respecta a la seguridad y la ingeniería de datos, no deja de sorprenderme la frecuencia con la que veo a los ingenieros de datos saltar a salvo en AWS, tres cubos, un servidor o una base de datos expuestos a la Internet pública cuando esa no era la intención. Hay algunas soluciones sencillas para evitar que esto suceda. Sin embargo, con frecuencia veo que esto ocurre porque el ingeniero de datos simplemente no conoce las prácticas recomendadas de seguridad o, accidentalmente, se olvidó de aplicarlas. En lo que respecta a la seguridad a nivel organizacional, he visto que, con demasiada frecuencia, las organizaciones se configuran con una fachada de seguridad. Tal vez un conjunto de listas de verificación para demostrar que cumplen con las regulaciones o los estándares de cumplimiento, pero la cultura de la organización carece de un espíritu de seguridad más profundo. Este enfoque de apegarse a la letra de la seguridad sin una cultura y un espíritu de seguridad es lo que yo llamo teatro de seguridad. La seguridad surge de una cultura en la que cada miembro del equipo reconoce el papel que desempeña en la protección de los datos y todos, de arriba a abajo, adoptan la seguridad como una prioridad y un hábito. En su trayectoria como ingeniero de datos, recuerde que, si bien los principios y protocolos, así como las herramientas y las tecnologías, son sus aliados a la hora de proteger los datos. Una verdadera cultura de seguridad emana de una comprensión compartida de las responsabilidades y vulnerabilidades en toda la organización. A medida que avancemos en la especialización, adquirirá experiencia práctica con las consideraciones de seguridad que se aplican a todos los aspectos de su arquitectura de datos. En el próximo vídeo, analizaremos la siguiente subcorriente del ciclo de vida de la ingeniería de datos: la gestión de datos.

A lo largo de estos cursos, haré hincapié en que, independientemente de en lo que esté trabajando como ingeniero de datos, debe pensar en la forma en que su trabajo agrega valor a las partes interesadas de su organización. Los datos pueden ser un activo empresarial increíblemente valioso, pero solo cuando se gestionan correctamente. De hecho, la administración de datos es tan importante que existe una organización internacional, la Asociación Internacional de Administración de Datos o DAMA, que se dedica exclusivamente a proporcionar recursos para que las empresas y las personas gestionen correctamente los datos. DAMA ofrece esta pequeña y elegante publicación, el Libro de conocimiento sobre la gestión de datos o DMBOK para abreviar. Como puede ver, hay mucho que saber en lo que respecta a la administración de datos. Pero antes de que empieces a asustarte, déjame decirte que, como ingeniero de datos, no necesitas memorizar todo lo que hay en este libro. De hecho, este es un gran volumen de referencia. En su trabajo como ingeniero de datos, se centrará en un subconjunto de tareas de administración de datos y compartirá toda la responsabilidad de la administración de datos con los equipos, incluidos los de ingeniería de software, TI y otros. En este vídeo, destacaré rápidamente los aspectos clave de la administración de datos de los que debe preocuparse como ingeniero de datos. En primer lugar, el DMBOK define la administración de datos como el desarrollo, la ejecución y la supervisión de planes, programas y prácticas que entregan, controlan, protegen y mejoran el valor de los datos y los activos de información a lo largo de sus ciclos de vida. Ahora, eso es un bocado, y tal vez incluso suene un poco vago. Pero vamos a desglosarlo un poco. Como campo, la gestión de datos consta de muchas facetas y disciplinas, cada una con su propio conjunto de responsabilidades. Esto puede hacer que el entorno de administración de datos sea confuso. El DMBOK desglosa las diferentes facetas de la administración de datos en 11 áreas de conocimiento de datos. Estos incluyen la gobernanza de datos, el modelado de datos, la integración e interoperabilidad de datos , los metadatos, la seguridad y más. Organizan todo eso en un diagrama como este. Encontrará más información sobre todos estos temas a lo largo de estos cursos. Sin embargo, como puede ver en este diagrama, la gobernanza de datos abarca todas las demás áreas de la administración de datos. Resulta que muchas de las demás áreas de conocimiento interactúan entre sí a través de prácticas de gobierno de datos. En este vídeo, me centraré únicamente en la gobernanza de datos en relación con muchas de las áreas que serán importantes en su función como ingeniero de datos. Según otro libro llamado Gobierno de datos, la guía definitiva. La gobernanza de datos es, ante todo, una función de gestión de datos para garantizar la calidad, la integridad, la seguridad y la usabilidad de los datos recopilados de una organización. A partir de esta definición, puede empezar a ver que la gobernanza de datos abarca muchos aspectos, desde la seguridad y la privacidad de los datos hasta la calidad y la usabilidad de los datos. Hablamos un poco sobre la seguridad y la privacidad en el vídeo anterior. Para los fines de este vídeo, me gustaría centrarme en la calidad de los datos, que está estrechamente relacionada con algunos de los otros términos clave que ha visto aquí, como integridad, usabilidad y confiabilidad. La calidad de los datos es un tema profundo y lleno de matices, pero en esencia, este concepto es relativamente sencillo. Los datos de alta calidad son precisos, completos, fáciles de detectar y están disponibles de manera oportuna. Más allá de eso, los datos de alta calidad representan exactamente lo que las partes interesadas esperan que representen en términos de definiciones de datos y esquemas bien definidos. Los datos que cumplen con los estándares de calidad son una herramienta poderosa para la toma de decisiones y agregan un gran valor a nuestra organización. Por el contrario, los datos de baja calidad pueden ser inexactos, incompletos o inutilizables por algún otro motivo. Los datos de baja calidad pueden hacer que las partes interesadas pierdan el tiempo , tomen malas decisiones o incluso despidan a todo su equipo de datos. En el próximo curso, obtendrá más información sobre cómo supervisar y garantizar la calidad de los datos en sus canalizaciones de datos. Pero por ahora, seguiremos adelante y echaremos un vistazo al siguiente trasfondo. Acompáñeme en el siguiente vídeo para explorar la arquitectura de datos en lo que respecta al ciclo de vida que garantiza los datos.

Puede pensar en la arquitectura de datos como una hoja de ruta o un plan para sus sistemas de datos. En la primera semana de este curso, hablamos sobre la recopilación de requisitos y sobre cómo pensar en tomar las necesidades de las partes interesadas y convertirlas en requisitos específicos que se pueden utilizar para tomar decisiones de diseño y tecnología. Para asignar los requisitos a un diseño exitoso de su sistema, debe pensar como un arquitecto. Ahora, para que quede claro, en su función de ingeniero de datos, según dónde trabaje, es posible que no sea directamente responsable de tomar decisiones de arquitectura y diseño. Es posible que su organización tenga a alguien en la función de arquitecto de datos. Quién es responsable de establecer el diseño y transmitirlo para que usted lo implemente. Sin embargo, según mi experiencia, poder pensar como un arquitecto hará que tengas más éxito en tu papel como ingeniero de datos. En algunas circunstancias, como si trabajas en una pequeña empresa emergente, es posible que seas tanto el arquitecto como el ingeniero. En cualquier caso, ojalá alguien me hubiera enseñado a pensar como un arquitecto cuando empecé con la ingeniería de datos. En este vídeo, presentaré brevemente algunos de los principios clave que forman parte del pensamiento de un arquitecto. A lo largo de estos cursos, revisaremos estos principios para que pueda confiar en sus habilidades para diseñar y construir sistemas de datos sólidos. En nuestro libro Fundamentos de la ingeniería de datos, Matt Housley y yo definimos la arquitectura de datos como el diseño de sistemas para respaldar las cambiantes necesidades de datos de una empresa, que se logra mediante decisiones flexibles y reversibles tomadas a través de una evaluación cuidadosa de las ventajas y desventajas. Dediquemos un minuto a analizar esa definición. En primer lugar, observará que la arquitectura de datos debe respaldar las cambiantes necesidades de datos de la organización. Esto significa que un buen diseño no solo es compatible con las necesidades de datos de hoy, sino también de mañana. En la práctica, esto significa que la arquitectura de datos es un esfuerzo continuo, en lugar de algo que solo se hace una vez y se termina. La siguiente parte de esta definición dice que un buen diseño se logra mediante decisiones flexibles y reversibles. Esto pone de manifiesto el hecho de que las necesidades de datos de su empresa pueden evolucionar de maneras que no había previsto y que, con el tiempo, tendrá que actualizar su arquitectura. Si sus opciones de diseño iniciales fueron flexibles y reversibles desde el principio, le resultará mucho más fácil hacer evolucionar su arquitectura para satisfacer las necesidades de la organización. Por último, lo verás en esta última parte de la definición. Todo esto se logra mediante una evaluación cuidadosa de las compensaciones, que pueden incluir las compensaciones en cuanto al rendimiento o el costo o la escalabilidad u otros parámetros. Ahora bien, creo que vale la pena mencionar en este punto que, cuando prácticamente todas las arquitecturas de datos se construían como sistemas locales, tomar decisiones flexibles y reversibles era mucho más difícil, en algunos casos imposible. Por ejemplo, si decide comprar e instalar hardware de servidor por valor de millones de dólares. Es probable que estés comprometido con ese sistema durante varios años, te guste o no. Hoy en día, la mayoría de las arquitecturas de datos se construyen en la nube. En cierto sentido, puede cambiar de opinión con la frecuencia que desee acerca de las elecciones tecnológicas que ha tomado para su arquitectura. Siempre que hayas tomado decisiones flexibles y reversibles desde el principio. Para ampliar un poco más estas ideas, analicemos un conjunto de principios de una buena arquitectura de datos que revisaremos a lo largo de estos cursos. Antes de entrar en esto, solo diré que no tienes que preocuparte por memorizar ninguna de las cosas ahora mismo. Solo quiero darte una vista previa de lo que está por venir en estos cursos y que comiences a pensar como un arquitecto. Principio número uno: elige sabiamente los componentes comunes. Los componentes comunes son las partes de la arquitectura que utilizarán varias personas y equipos de la organización. Una buena elección de componentes comunes es aquella que proporciona el conjunto correcto de funciones para proyectos individuales y, al mismo tiempo, facilita la colaboración entre equipos. Principio número dos: planifica para el fracaso. Esto significa exactamente lo que dice. Una buena arquitectura está diseñada no solo para el caso en el que todo funciona como se espera, sino también para cuando las cosas se estropean. Principio número tres: diseñar para la escalabilidad. Los sistemas escalables pueden ampliarse para satisfacer la demanda según sea necesario y reducirlos para minimizar los costos cuando la demanda disminuye. Al incorporar la escalabilidad a su arquitectura, puede responder a una demanda cambiante y, al mismo tiempo, optimizar los costos. Principio número cuatro. La arquitectura es liderazgo. Ahora bien, si bien es posible que el principio de la arquitectura como liderazgo no se aplique directamente a su función de ingeniero de datos, si se esfuerza por pensar como un arquitecto y busca la tutoría de arquitectos de datos, estará en mejores condiciones de liderar y asesorar a otros miembros del equipo a medida que sus habilidades se desarrollen y adquiera más experiencia. Con el tiempo, es posible que usted mismo ocupe el puesto de arquitecto de datos. Principio número cinco: ser siempre arquitecto. Como dije antes, el diseño arquitectónico no es algo que ocurre solo una vez. En su lugar, evaluará constantemente sus sistemas en función de las necesidades cambiantes de su organización y rediseñará la arquitectura de forma continua. Principios seis y siete: construir sistemas poco acoplados y tomar decisiones reversibles. Un sistema de acoplamiento débil es aquel que se construye a partir de componentes individuales que se pueden cambiar fácilmente por otros sin tener que volver a diseñar todo el sistema. Eligiendo construir con componentes fácilmente intercambiables como este. Está tomando una serie de decisiones reversibles, es decir, si cambia de opinión o las necesidades de su organización evolucionan, puede revertir fácilmente su conjunto de decisiones anterior e intercambiar los componentes de su arquitectura para cumplir con las nuevas especificaciones de diseño. Principio número ocho: priorizar la seguridad. Ya hemos analizado algunos principios de seguridad, como el principio del privilegio de arrendamiento, y más adelante, en nuestro análisis de la arquitectura, abordaremos otros, como el principio de confianza cero. La principal conclusión de todos estos principios es que la seguridad es fundamental para su función como ingeniero de datos. Principio número nueve: adoptar FinOps. La estructura de costos de los datos ha evolucionado drásticamente en la era de la nube. FinOps es un movimiento para unir las prioridades empresariales de finanzas y DevOps, o en este caso, DataOps. En la nube, la mayoría de los sistemas de datos se pagan por uso y son fácilmente escalables. Al adoptar FinOps, puede diseñar sus sistemas para que se optimicen simultáneamente en función de los costos y la posible generación de ingresos. Este es un vistazo rápido a los principios clave de una buena arquitectura de datos. La semana que viene, en este curso, analizaremos más de cerca estos principios y la buena arquitectura de datos en general. Pero por ahora, pasemos al siguiente trasfondo del ciclo de vida de la ingeniería de datos. Acompáñeme en el siguiente vídeo para ver DataOps.

Alrededor de 2007, surgió un marco denominado DevOps en el desarrollo de software para romper los silos entre los equipos de desarrollo de software que escriben y prueban el código y los equipos de implementación de software que implementan y mantienen el código. DevOps se basa en otras metodologías conocidas, como Lean y Agile, para lograr cosas como la eliminación de los cuellos de botella, la reducción de los residuos y la identificación rápida de los problemas, así como la rápida iteración. El movimiento DevOps ha dado como resultado un aumento de los ciclos de lanzamiento y una mejor calidad de los productos de software. A medida que el campo de los datos ha ido madurando, hemos adoptado un enfoque similar, conocido como DataOps, para el desarrollo de productos de datos. Al igual que DevOps mejora el proceso de desarrollo y la calidad de los productos de software, DataOps tiene como objetivo mejorar el proceso de desarrollo y la calidad de los productos de datos. DataOps es, ante todo, un conjunto de hábitos y prácticas culturales que puede adoptar. Estas incluyen cosas como priorizar la comunicación y la colaboración con otras partes interesadas de la empresa, aprender continuamente de sus éxitos y fracasos y adoptar un enfoque de iteración rápida para mejorar sus sistemas y procesos. Estos también son los hábitos y prácticas culturales de DevOps, y están tomados directamente de la metodología ágil, que es un marco de gestión de proyectos. Concéntrese en entregar el trabajo en pasos incrementales e iterativos. En cuanto a los elementos técnicos de DataOps, hay tres pilares clave. El primer pilar que ves aquí a la izquierda es la automatización. Luego, el segundo pilar es la observabilidad y el monitoreo. Y, por último, el último pilar es la respuesta a los incidentes. Son similares a los componentes principales de DevOps, donde el objetivo final es proporcionar funcionalidades y características específicas en un producto de software. Sin embargo, en Data Ops, el objetivo es ofrecer productos de datos de alta calidad, en los que pueda pensar en un producto de datos como cualquier dato o sistema de datos que proporcione a los usuarios finales. Por eso, analicemos más de cerca cada uno de estos tres pilares de las operaciones de datos. En términos de automatización, una de las prácticas de DevOps que aceleró el ciclo de vida de creación de software es lo que se conoce como integración continua y entrega continua, o CI/CD para abreviar. Con CI/CD, los desarrolladores pueden automatizar muchos de los procesos manuales necesarios para crear, probar e implementar el código. Esta automatización no solo se traduce en ciclos de revisión e implementación más rápidos, sino también en menos errores, lo que, en última instancia, hace que los equipos de software sean más eficientes y efectivos a la hora de crear productos de software de alta calidad. Además, DataOps emplea un marco de automatización similar al procesamiento de datos, al igual que DevOps se aplica al desarrollo de software. Dentro de dataops, el objetivo de alto nivel de la administración automatizada de cambios sigue siendo el mismo, por ejemplo, cuando se trata de administrar los cambios en el código, la configuración o el entorno. Además, DataOps se centra en la gestión del cambio en lo que respecta a las canalizaciones de procesamiento de datos y a los datos en sí. Para tener una idea de cómo se aplica la automatización al procesamiento de datos, imaginemos que acabas de empezar en una organización pequeña. Y se le ha encomendado la tarea de crear una canalización de datos que comience con la ingesta de datos de varios sistemas de origen. Entonces, tal vez esté ingiriendo datos de una base de datos, así como de algunos archivos y una API o una plataforma de intercambio de datos. Entonces, quizás esté realizando algunas transformaciones durante el proceso de ingesta y, a continuación, almacenando los datos ingeridos en un sistema de almacenamiento, tal vez en una base de datos. Y luego supongamos que tiene dos casos de uso final que está atendiendo, uno para el análisis y otro para el aprendizaje automático. A continuación, supongamos que está realizando algunas transformaciones adicionales, tal vez modelando y agregando los datos antes de enviarlos a otro sistema de almacenamiento y ponerlos a disposición de los usuarios finales. Por lo tanto, aquí tenemos básicamente dos canalizaciones para las etapas de transformación y servicio. Si es el primer ingeniero de datos de esta organización y se encuentra en las primeras etapas del desarrollo de sus sistemas de datos, puede optar por ejecutar manualmente las distintas tareas de esta canalización de datos, como iniciar manualmente cada uno de estos procesos de ingesta. Luego, una vez que se hayan completado, ejecute manualmente cada uno de los pasos siguientes en las etapas de transformación, almacenamiento y servicio. Este podría ser un enfoque razonable para empezar rápidamente y crear prototipos de algunos aspectos de su canalización de datos a largo plazo. Sin embargo, este tipo de ejecución manual en varias etapas será propensa a errores e ineficaz porque requiere que ejecutes manualmente cada tarea con un nivel mínimo de automatización. Puedes optar por adoptar el denominado enfoque de programación pura, lo que significa que configurarías cada tarea de tu proceso para que comience a una hora determinada del día. Así que tal vez comiences todas estas tareas de ingestión a medianoche todas las noches. A continuación, calculará cuánto tiempo tardan todos los datos en ingerirse y cargarse en su sistema de almacenamiento. Luego, puede programar las tareas de transformación posteriores para que comiencen después de eso, y así sucesivamente a lo largo de todas las tareas de su proceso. Esto se denomina programación porque se crea una programación para iniciar automáticamente cada una de las tareas de la canalización de datos. Para llevar las cosas al siguiente nivel de automatización de DataOps, puede adoptar un marco de orquestación como Airflow. Los marcos de orquestación comprueban las dependencias entre las tareas de su canalización de datos antes de ejecutar cada tarea. De este modo, puedes decidir la hora y la frecuencia con la que quieres que comience la primera tarea de tu pipeline. Luego, el marco de orquestación iniciará automáticamente las tareas posteriores una vez que las anteriores se hayan completado correctamente. El marco de orquestación también puede notificarle cuando haya un error en alguna de las tareas. De este modo, las tareas posteriores que dependen de las anteriores no se inicien cuando no deberían hacerlo. Muchos marcos de orquestación no solo automatizan la ejecución de tareas en sus canalizaciones de datos, sino que también mejoran el desarrollo de estas canalizaciones al permitir la verificación e implementación automáticas de nuevos aspectos de su canalización de datos. Similar al proceso de CI/CD para la implementación de software. Cuando se trata del siguiente pilar de la observabilidad y el monitoreo, lo principal que debes tener en cuenta es que cualquier canalización de datos que configures está destinada a fallar eventualmente. Para citar a Werner Vogels, el CTO de Amazon Web Services, todo falla todo el tiempo. Esto significa que si no observa y monitorea de cerca sus sistemas de datos, no lo sabrá cuando fallen. En el peor de los casos, es posible que solo se dé cuenta de estas fallas del sistema cuando las partes interesadas intermedias descubran estos problemas por sí mismas, por ejemplo, en sus informes o paneles de análisis. En mi propio trabajo con clientes, he visto innumerables casos de datos incorrectos que permanecen en los informes durante meses o incluso años debido a fallas no descubiertas en los sistemas de procesamiento de datos. Este tipo de fracasos pueden ser una pérdida de tiempo y dinero, llevar a decisiones mal informadas y, en última instancia, podrían costarte el trabajo si las partes interesadas pierden la confianza en tu trabajo. Por lo tanto, la observabilidad y el monitoreo son aspectos cruciales de los sistemas de datos que construya. El tercer pilar de DataOps es la respuesta a los incidentes, que consiste en utilizar las capacidades de observación y supervisión que configuras para identificar rápidamente las causas fundamentales de un incidente y resolverlo de la forma más rápida y fiable posible. Como dije antes, las cosas se romperán y es solo cuestión de tiempo que se rompan. Con la respuesta a incidentes, no se trata solo de la tecnología y las herramientas que se utilizan para identificar un problema y responder a él. También se trata de una comunicación abierta e impecable y de la coordinación de los esfuerzos de los miembros del equipo de datos que responden al incidente, así como de otros miembros de la organización. Como ingeniero de datos, debe buscar problemas de forma proactiva antes de que otras partes interesadas de su organización le informen de ellos. DataOps es un conjunto de ideas relativamente nuevo que aún está en progreso, y no todas las organizaciones han adoptado las mejores prácticas de DataOps. En su trabajo como ingeniero de datos, es posible que se encuentre en una organización en la que DataOps esté bastante maduro o en un lugar en el que aún no haya adoptado DataOps. A continuación, analizaremos más de cerca la orquestación, que es un componente clave de DataOps. Y es un componente tan importante de las arquitecturas y canalizaciones de datos modernas que lo consideramos una corriente subyacente independiente del ciclo de vida de la ingeniería de datos.

Cuando piensas en la palabra orquestación, ¿qué te viene a la mente? Tal vez un director, guiando una orquesta o un coro, la señalización cuando se deben presentar varios instrumentos o voces y los cambios y cosas como el tempo y la intensidad, todo ello en un esfuerzo por hacer buena música. Al igual que una orquesta, una canalización de datos tiene muchas partes móviles que deben coordinarse para obtener un buen resultado. Como ingeniero de datos, usted es el conductor a cargo de coordinar y administrar las tareas de sus canalizaciones de datos. En el vídeo anterior, abordamos brevemente la orquestación, ya que es un componente central de las operaciones de datos. A continuación, me limitaré a decir unas palabras más sobre cómo la orquestación desempeña un papel clave como trasfondo del ciclo de vida de la ingeniería de datos. Como mencioné en el último vídeo, si acabas de empezar, tal vez como el primer ingeniero de datos en una pequeña empresa emergente o en las etapas de creación de prototipos de un nuevo proyecto en una organización de cualquier tamaño, puedes configurar inicialmente una canalización de datos en la que ejecutes manualmente cada una de las tareas de cada etapa. Analizamos un proceso como este, en el que se ingieren datos de varias fuentes en los sistemas de almacenamiento y, al mismo tiempo, se aplican algunas transformaciones en vuelo. Luego, en sentido descendente, tiene más procesos para transformar, almacenar y entregar los datos a los usuarios intermedios para casos de uso de análisis y aprendizaje automático. La ejecución manual de todos estos pasos en sus canalizaciones de datos, es decir, activar manualmente cada paso para que se ejecute cuando lo necesite, puede ser algo que pueda ayudar a crear prototipos de varios aspectos de su sistema. Sin embargo, a la larga, este no es un método sostenible para el procesamiento de datos. Una vez que sepa qué tareas necesita ejecutar en su canalización de datos, puede adoptar un enfoque de programación pura para que esas tareas se ejecuten automáticamente a determinadas horas del día o con frecuencias determinadas. Por ejemplo, puedes programar la prueba de ingestión y almacenamiento inicial para que comiencen a la misma hora todos los días. Luego, puede programar los pasos de transformación para que comiencen una hora después de esperar que todos los datos ingeridos estén presentes en su sistema de almacenamiento. La programación pura es un enfoque que históricamente se ha utilizado ampliamente para diversas tareas de procesamiento de datos. Sin embargo, puede tener problemas con este enfoque. Por ejemplo, si la tarea de ingestión programada no se pudo ejecutar, esto podría provocar que también fallaran las tareas de transformación posteriores. O si esta tarea de transformación simplemente tarda más de lo esperado y la siguiente tarea de transformación comienza antes de que finalice la anterior, puede terminar con datos incompletos, obsoletos o problemáticos que se propaguen a través de su proceso. No hace mucho tiempo, los marcos de orquestación que eran más sofisticados que los de los programadores puros solo estaban disponibles para las grandes empresas que contaban con los recursos para crear sus propias soluciones personalizadas. Pero hoy en día, hay una serie de marcos de orquestación de código abierto que permiten crear una orquestación sofisticada en sus propios canales de datos, sin importar el tamaño del equipo o la empresa con la que trabaje. El marco más popular en este momento es Apache airflow, pero varios otros marcos de código abierto más nuevos, como Dagster, Prefect y Mage, también están ganando terreno. Lo que estos marcos le permiten hacer es automatizar sus canalizaciones de datos e incorporar dependencias y capacidades de supervisión complejas. Puede realizar una programación basada en el tiempo, si lo desea, pero puede, por ejemplo, crear una dependencia que verifique que esta primera tarea de transformación se haya completado antes de comenzar la siguiente tarea de transformación. O bien, en lugar de predefinir una hora del día o una frecuencia en particular, si desea iniciar una tarea, puede tener tareas que se activen por eventos. Por ejemplo, puede activar este paso de sugerencia para que comience cuando haya una cantidad determinada de datos nuevos disponibles en la base de datos de origen. Puede configurar la supervisión dentro de su marco de orquestación y activar alertas para informarle si, por ejemplo, esta tarea de transformación no se ejecuta o no se ha completado en un momento determinado. Muchos marcos de orquestación requieren que configure su canalización de datos, lo que se conoce como gráfico acíclico dirigido, que en realidad es un término demasiado complicado para describir cómo fluyen los datos a través de su canalización de datos. Veamos cómo sería un gráfico acíclico dirigido o DAG, para abreviar, para este oleoducto del que hemos estado hablando. En cierto sentido, se podría decir que ya se trata de un DAG en el que cada uno de estos íconos representa una tarea diferente en su proceso y las flechas muestran cómo se mueven los datos de una tarea a la siguiente. Pero ahora voy a modificar la imagen aquí solo para que funcione explícitamente como un DAG, ya que los verá representados en otros lugares. Se denominan sistemas de origen, fuente 1, fuente 2, fuente 3 y fuente 4, y estamos ingiriendo o extrayendo datos de todas estas fuentes, y aquí se está produciendo un paso de transformación a partir de la fuente 4. A continuación, almacena todos los datos extraídos en el almacenamiento. Después de eso, la canalización se divide y, a lo largo de esta rama superior, hay dos pasos de transformación más, seguidos del almacenamiento que servirá para el caso de uso final del aprendizaje automático. En la rama inferior, tiene un paso de transformación y, a continuación, el almacenamiento para atender el caso de uso final de la analítica. La palabra gráfico acíclico dirigido de extremo a extremo indica que el flujo de datos va solo en una dirección. Acíclico indica que no hay bucles. Los datos no vuelven a un paso anterior y se pueden describir como un gráfico porque están compuestos de nodos y bordes. Puede pensar en un DAG como un diagrama de flujo que muestra cómo se mueven los datos a través de sus canalizaciones. Construirá e implementará el DAG dentro del marco de orquestación que elija. Como mencioné, podrás especificar criterios y dependencias sobre cómo se debe activar cada tarea y qué monitoreo y alertas deseas configurar. Más adelante en estos cursos, aprenderás a configurar y ejecutar DAG para algunas canalizaciones de datos y algunos marcos populares. Sin embargo, por ahora, la conclusión principal es que la orquestación es una corriente subyacente que abarca todo el ciclo de vida de la ingeniería de datos, así como los aspectos clave de las operaciones de datos. Acompáñeme en el siguiente vídeo y observe cómo la ingeniería de software, el último trasfondo, se relaciona con su función como ingeniero de datos.

Hasta ahora, en esta lección, hemos estado hablando de temas bastante complejos en lo que respecta a las corrientes subyacentes del ciclo de vida de la ingeniería de datos, como la seguridad, la arquitectura de datos, las operaciones y la administración, así como la orquestación de los canales de datos. De todas las corrientes subyacentes, quizás la más sencilla de entender sea la última, la ingeniería de software. Lo que quiero decir con eso es que, como ingeniero de datos, necesitas saber leer y escribir código. Es tan sencillo como eso. Así que no me refiero solo a hackear un código que haga lo que sea necesario en este momento, sino a escribir código de nivel de producción que sea limpio, legible, comprobable e desplegable. Por lo tanto, la ingeniería de software es el diseño, el desarrollo, la implementación y el mantenimiento de las aplicaciones de software. Y no están muy lejos. En el pasado no existía la ingeniería de datos como profesión oficial. Solo había ingenieros de software que de vez en cuando se ocupaban de los datos en su propio trabajo. Con el tiempo, a medida que las empresas reconocieron el valor de los datos, los ingenieros de software comenzaron a incorporar varios aspectos de la ingeniería de datos como parte de su trabajo. Con el aumento de la diversidad y el volumen de datos en las últimas décadas, el componente de la ingeniería de software orientado a los datos se hizo mucho más intensivo y, finalmente, surgió como un campo propio. A lo largo de los años, los ingenieros de software que se dedicaban a la ingeniería de datos crearon una variedad de soluciones fantásticas, de modo que hoy en día, como ingeniero de datos, tiene acceso a una amplia gama de servicios y aplicaciones gestionados que le permiten realizar el trabajo de manera más eficiente. Y esto es algo bueno. Le permite dedicar más tiempo a centrarse en los aspectos más importantes para añadir valor real a su organización. En cierto sentido, estos servicios y aplicaciones existentes le permiten ascender en la cadena de valor, por así decirlo. Esto también significa que, como ingeniero de datos actual, a menudo se le exige escribir mucho menos código que sus antepasados orientados a la ingeniería de software de hace una o dos décadas. Sin embargo, esto no significa que la codificación no sea importante en su trabajo como ingeniero de datos. De hecho, es más importante que nunca que puedas escribir un código excelente y que el código que escribas sea de la mejor calidad. Por ejemplo, se le pedirá que escriba el código de procesamiento de datos principal en todas las etapas del ciclo de vida de la ingeniería de datos. Desde la ingestión hasta la transformación y el servicio, tendrás que dominar marcos y lenguajes como SQL, Spark o Kafka. También es probable que encuentres lenguajes de máquinas virtuales Python o Java, como Java o Scala, así como Bash, para operar en la línea de comandos. Es posible que también tengas que trabajar en otros idiomas, como rust o go. Pero si te enfocas en desarrollar habilidades fundamentales de ingeniería de software, no tendrás muchos problemas para moverte de un idioma a otro. En esta especialización. Nos centraremos principalmente en SQL, Python y bash en los ejercicios de laboratorio, ya que estas son las formas más comunes en las que interactuarás con los datos como ingeniero de datos y, como ingeniero de datos, es probable que te involucres en el desarrollo de marcos de código abierto. La forma en que esto suele suceder es que adoptas un marco de código abierto para resolver un problema en particular y terminas desarrollando aún más el marco para tu caso de uso específico. Siempre que escribas un buen código, puedes hacer una solicitud de cambios y añadir tus contribuciones al proyecto de código abierto para ayudar a otros a resolver problemas similares. Otras iniciativas en las que participarás o participarás en el desarrollo de las llamadas soluciones de infraestructura como código o canalización como código, de las que hablaremos más adelante en estos cursos. Además de estos casos específicos en los que escribirás código en tu rol de ingeniero de datos, también necesitarás escribir código para la resolución diaria de problemas de uso general en todas las etapas del ciclo de vida de la ingeniería de datos. Así que, como dije al principio de este vídeo, como ingeniero de datos, necesitarás saber leer y escribir código. La codificación formará parte de su trabajo diario, y su capacidad para escribir código limpio, legible, comprobable e desplegable se traducirá en valor para su organización. Vale la pena dedicar tiempo a entablar amistad con los ingenieros de software de su organización y aprender de ellos a escribir código excelente. Y con eso, terminamos esta lección sobre las corrientes subyacentes del ciclo de vida de la ingeniería de datos. En este punto, estoy seguro de que ya estás harto de todo esto de la teoría y estás listo para arremangarte y sumergirte en la aplicación de algunos de estos conceptos en algunos ejercicios prácticos. Únase a mí en la siguiente lección para ver cómo el ciclo de vida de la ingeniería de datos y las corrientes subyacentes cobran vida en la nube de AWS.

