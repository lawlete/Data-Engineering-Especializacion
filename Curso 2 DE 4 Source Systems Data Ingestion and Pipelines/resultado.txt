
C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "C2W1-1.txt"  
 echo.
) 
Bienvenido al segundo curso de la especializaci칩n en ingenier칤a de datos. En el primer curso, obtendr치 una visi칩n general de alto nivel del campo de la ingenier칤a de datos, los principios de una buena arquitectura de datos y c칩mo traducir las necesidades de las partes interesadas en requisitos y herramientas para sus sistemas de datos. En este curso, aprender치 m치s sobre la ingesti칩n de datos de los sistemas de origen, as칤 como sobre DataOps y la orquestaci칩n de la canalizaci칩n de datos de extremo a extremo. Estoy aqu칤 nuevamente con su instructor, Joe Reis, quien tambi칠n es coautor del superventas Fundamentals of Data Engineering. Joe, 쯣odemos ver un poco sobre lo que ver치n los alumnos en este curso? Claro que s칤, Andrew. Este curso, como ha dicho, incluye un enfoque en las dos primeras etapas del ciclo de vida de la ingenier칤a de datos, que son la generaci칩n y los sistemas fuente de datos y la ingesti칩n de datos de esos sistemas fuente. Empezaremos por analizar los diferentes tipos de sistemas fuente, tal vez cosas como bases de datos o sistemas de almacenamiento y transmisi칩n de objetos. Analizaremos en detalle c칩mo interactuar치 con los sistemas fuente en su trabajo como ingeniero de datos. Despu칠s de eso, analizaremos la ingesta de datos de los sistemas de origen, as칤 como los aspectos de DataOps y orquestaci칩n de la creaci칩n de canalizaciones de datos. Eso suena genial. Para muchos sistemas de IA, la ingenier칤a de datos o la ingesta de datos representan aproximadamente el 80% del trabajo, y luego el modelado del aprendizaje autom치tico representa quiz치s el 20% del trabajo. Sin embargo, la atenci칩n de las personas sobre estos dos temas suele cambiar, ya que el 80% de la atenci칩n se centra en el modelado de la IA y no se presta suficiente atenci칩n , ni en las mejores pr치cticas, y, francamente, tambi칠n en la ingesti칩n de datos. De hecho, en algunos de mis trabajos anteriores, cuando trabajaba en una gran empresa de tecnolog칤a, era responsable del almac칠n de datos de los usuarios de la empresa, por lo que cada dato que afectaba a un usuario individual deb칤a entrar en mi almac칠n de datos, lo que creaba mucho valor para la empresa. Pero ese trabajo intelectual para dise침ar la base de datos, el esquema de la base de datos, el sistema de ingesta, los datos para mantenerla, result칩 ser una tarea bastante ingente. Es una empresa enorme. Siempre he descubierto que cuando hablamos de ingerir datos de los sistemas fuente, esto lo es todo. Si no puede obtener los datos, no hay mucho m치s que pueda hacer con ellos. Esto deber칤a parecer bastante simple, pero como usted se침ala, parece que, muchas veces, se ignora la ingesti칩n o se centra en otras cosas. As칤 que esto es algo fundamental que debes corregir. Si no puede obtener los datos, realmente no puede hacer nada m치s. En esta parte del curso, sin duda, hablaremos sobre la comprensi칩n de los sistemas de origen de los que va a obtener sus datos y las diferentes formas de ingerirlos. Adem치s de las diferentes formas de organizar estos flujos de trabajo de canalizaci칩n de datos y supervisarlos para garantizar que se preserva la calidad de los datos, as칤 como la ingesti칩n, el rendimiento y otras caracter칤sticas, etc., son muy importantes para su trabajo como ingeniero de datos. De hecho, creo que esto es v치lido para muchos flujos de trabajo de datos diferentes, desde los datos de las tablas de estructura hasta los datos estructurados, como textos e im치genes, a medida que la palabra procesa m치s datos estructurados. Esto parece haber permanecido igual. De hecho, incluso cuando hablo con mis amigos que entrenan a los grandes modelos ling칲칤sticos y cuando se trata de liderar equipos de IA, gran parte del tiempo, no todo, pero s칤 mucho tiempo, lo dedico a pensar en los datos. Tambi칠n hay algo relacionado con el entrenamiento de modelos, pero la ingesta de datos ocupa mucho tiempo para todas estas cargas de trabajo de IA. Es muy interesante. Supongo que lo que ven en t칠rminos de las complejidades de la ingesta de datos, porque me imagino que eso funciona a una escala enorme. De hecho, hay muchos datos de Internet. Cosas como Common Crawl contienen muchos datos. Pero teniendo en cuenta los datos, 쯖칩mo se ingieren, se procesan y se filtran para que sean de alta calidad? Adem치s, si tiene lagunas en los datos, si observa que su modelo no tiene 칠xito en estos temas, 쯖칩mo puede averiguar en qu칠 lugares no le va tan bien y en qu칠 parte de la Tierra va a obtener datos, si es que existen para llenar esos vac칤os? Por lo tanto, estas son muchas de las cosas intelectualmente desafiantes por las que la gente que se entrena sobre el terreno, incluso algunos de los principales OM, MM, grandes modelos multimodales y otros grandes modelos b치sicos, dedican tiempo a preocuparse. Eso es fascinante. Una de las cosas de las que hablamos en este curso y que aprender치s es la ingesti칩n de datos de varios tipos de sistemas de origen. Obviamente, vamos a hablar de la ingesta de datos tabulares, pero hoy en d칤a ese es un subconjunto muy peque침o del universo total de datos. Cuando hablamos de conjuntos de datos no estructurados, desde texto hasta im치genes y v칤deos, este universo se est치 convirtiendo cada vez m치s en un universo mucho m치s grande. Yo dir칤a que tradicionalmente hemos utilizado o pensado en el mundo de los datos. Una de las cosas que trataremos en este curso, una vez m치s, no es solo los conjuntos de datos estructurados de las bases de datos, sino que tambi칠n comenzaremos a trabajar con texto , datos de im치genes, etc. Esto lo ayudar치 a prepararse como ingeniero de datos, no solo para las cargas de trabajo actuales, tal vez si trabaja en un almac칠n de datos, sino tambi칠n para las cargas de trabajo del futuro. La mayor parte del valor de los datos probablemente haya sido informaci칩n estructurada hasta este momento. Sin embargo, a medida que crezca nuestra capacidad para procesar datos no estructurados, veremos si eso cambia. Quiz치 ya est칠 cambiando. El volumen de datos no estructurados en el mundo es mucho mayor que el volumen de datos estructurados en el mundo. Creo que esto supondr칤a a칰n m치s desaf칤os e incluso m치s puestos de trabajo para los ingenieros de datos. Exactamente. Esperamos que esta sea una introducci칩n muy interesante al ciclo de vida de la ingenier칤a de datos. Una vez m치s, la ingesti칩n de los sistemas de origen , la organizaci칩n de estas cargas de trabajo y la supervisi칩n de estas cargas de trabajo son fundamentales para su trabajo como ingeniero de datos. Una vez m치s, como se침al칩 Andrew, solo se volver치 m치s interesante, m치s emocionante y mucho m치s grande. Muchas cosas interesantes. Pasemos al siguiente v칤deo para profundizar en todos estos temas.

Programa
Estado: Traducido autom치ticamente del Ingl칠s
Traducido autom치ticamente del Ingl칠s
Informaci칩n:
Este elemento incluye contenido que a칰n no se tradujo a tu idioma preferido.
쮻e qu칠 trata este programa?

Este programa fue dise침ado por Joe Reis en colaboraci칩n con DeepLearning.IA y AWS para cubrir los fundamentos de la ingenier칤a de datos, tanto en t칠rminos de la teor칤a subyacente y marcos para pensar como un ingeniero de datos, as칤 como habilidades pr치cticas para la construcci칩n de soluciones de ingenier칤a de datos en la nube.
쮸 qui칠n va dirigido este programa?

Este programa est치 dise침ado para cualquier persona interesada en seguir una carrera en o adyacente a la ingenier칤a de datos. Puede que seas estudiante o que ya trabajes profesionalmente en un campo relacionado con los datos. En cualquiera de los casos, le interesa adquirir habilidades y conocimientos de ingenier칤a de datos para respaldar sus objetivos profesionales. Incluso si ya est치 trabajando como ingeniero de datos, encontrar치 valor en la combinaci칩n de conocimientos te칩ricos y aplicaciones t칠cnicas que se presenta aqu칤. 
쯈u칠 conocimientos previos necesito para realizar este curso?

    Se requierenconocimientos intermedios de programaci칩n en Python, incluida la familiaridad con la sintaxis, las estructuras de datos, las funciones y las clases de Python. Tambi칠n puede ser 칰til estar familiarizado con los dataframes de Pandas, aunque no es obligatorio. Para aprender los conceptos b치sicos de Pandas, recomendamos los tutoriales de Pandas de la Escuela W3

 o los tutoriales de Pandas de Kaggle

. 

Tambi칠n puede ser 칰tilestar familiarizado con SQL , pero no es necesario. No dudes en consultar el curso SQLBolt Tutorials

 si quieres aprender los fundamentos de SQL.

Lafamiliaridad b치sica con los fundamentos t칠cnicos de la nube de AWS ser치 칰til pero no necesaria. Para aprender los fundamentos de AWS recomendamos los cursos AWS Cloud Practitioner Essentials
 y AWS Cloud Technical Essentials

    .

쯈u칠 tiene de especial este programa?

    Este programa leense침a a pensar como un ingeniero de datos
    Este programa le ense침ar치 a pensar como un ingeniero de datos a la hora de dise침ar, crear y mantener sistemas que toman datos sin procesar, los convierten en algo 칰til y los ponen a disposici칩n de las partes interesadas. No se trata s칩lo de herramientas y tecnolog칤as En primer lugar, aprender치 a recopilar las necesidades de las partes interesadas y a comprender los problemas empresariales que intentan resolver con los datos. A continuaci칩n, traducir치 esas necesidades en requisitos del sistema y elegir치 las herramientas y tecnolog칤as adecuadas para las soluciones que pretende crear. Al final de este programa, dispondr치 de un s칩lido marco mental que podr치 aplicar a cualquier proyecto de ingenier칤a de datos.

    Pr치ctica
    Tendr치s muchas oportunidades de practicar la aplicaci칩n del marco mental para pensar como un ingeniero de datos a trav칠s de actividades pr치cticas. Participar치 en conversaciones simuladas con las partes interesadas y se le pedir치 que re칰na requisitos para sus sistemas de datos. Dise침ar치 e implementar치 canalizaciones de datos de streaming y por lotes de extremo a extremo en la nube de AWS, solucionar치 problemas comunes a los que se enfrentan muchos nuevos ingenieros de datos, utilizar치 herramientas populares de c칩digo abierto para orquestar y monitorizar sus canalizaciones de datos, crear치 arquitecturas de almacenamiento de lago de datos y data lakehouse, consultar치, modelar치 y transformar치 sus datos utilizando varios marcos de procesamiento, y servir치 datos a las partes interesadas aguas abajo para casos de uso de an치lisis empresarial y aprendizaje autom치tico. Este programa adopta un enfoque justo a tiempo para presentarle las herramientas y tecnolog칤as que necesitar치 para cada ejercicio, y se le guiar치 a trav칠s de cada paso de los laboratorios con instrucciones detalladas y gu칤as en v칤deo.

Libro de texto y lecturas

    Fundamentos de la ingenier칤a de datos

    A lo largo del curso se proporcionar치 material de lectura complementario

Esquema del programa

Este programa est치 estructurado en 4 cursos.

Curso 1 - Introducci칩n a la ingenier칤a de datos

Este curso consta de 4 semanas de contenido y cubre estos objetivos principales de aprendizaje:

    Identificar los principales colaboradores y partes interesadas para los ingenieros de datos

    Articular un marco mental para construir soluciones de ingenier칤a de datos

    Identificar algunas de las consideraciones necesarias para la recopilaci칩n de requisitos al inicio de un nuevo proyecto

    Describir la estructura del ciclo de vida de la ingenier칤a de datos y sus corrientes subyacentes, y c칩mo pensar en los problemas de ingenier칤a de datos a trav칠s de esta lente

    Identificar algunas de las tecnolog칤as clave que pueden emplearse en las distintas fases del ciclo de vida de la ingenier칤a de datos

    Evaluar tecnolog칤as y herramientas en el contexto de los requisitos y una buena arquitectura de datos

    Dise침ar una arquitectura de datos en AWS basada en los requisitos de las partes interesadas

    Implementar una canalizaci칩n por lotes y de streaming en AWS para respaldar un sistema de recomendaci칩n de productos

Curso 2 - Sistemas de origen, ingesti칩n de datos y canalizaciones

Este curso consta de 4 semanas de contenido y cubre estos objetivos de aprendizaje principales:

    Identificar diferentes formatos de datos y determinar los sistemas de origen apropiados para generar cada tipo de datos

    Explicar a alto nivel c칩mo se generan, almacenan y recuperan los datos en varios sistemas fuente, incluyendo bases de datos relacionales, bases de datos NoSQL, almacenamiento de objetos y sistemas de streaming

    Explicar los fundamentos de las redes en la nube

    Solucionar errores de conexi칩n a bases de datos

    Explicar la diferencia entre las ingestas por lotes y por flujo e identificar casos de uso para cada patr칩n

    Diferenciar entre los dos patrones de ingesta por lotes: Extracci칩n-Transformaci칩n-Carga (ETL) y Extracci칩n-Carga-Transformaci칩n (ELT)

    Crear un script para ingestar datos desde una API REST

    Describir los componentes b치sicos de una plataforma de flujo de eventos

    Interactuar con una plataforma de transmisi칩n de eventos como sistema de origen y como herramienta de ingesti칩n

    Utilizar Terraform para aprovisionar recursos de AWS para su canalizaci칩n de datos

    Identificar herramientas para monitorizar sus sistemas de datos y la calidad de los datos

    Identificar y monitorizar m칠tricas de calidad de datos relevantes

    Explicar c칩mo se puede aplicar la orquestaci칩n a un canal de datos y enumerar sus beneficios

    Construir canalizaciones de datos con DAGs en Airflow utilizando funciones como Taskflow API, operadores, variables XCom, etc.

Curso 3 - Almacenamiento de datos y consultas

Este curso consta de 3 semanas de contenido y cubre estos objetivos de aprendizaje principales:

    Explicar c칩mo se almacenan f칤sicamente los datos en el disco y en la memoria

    Comparar c칩mo se almacenan y consultan los datos en sistemas de almacenamiento de objetos, bloques y archivos

    Explicar c칩mo se almacenan los datos en bases de datos orientadas a filas frente a bases de datos orientadas a columnas

    Explicar c칩mo almacenan y recuperan datos las bases de datos gr치ficas y vectoriales

    Explicar las caracter칤sticas arquitect칩nicas clave de los almacenes de datos, los lagos de datos y los data lakehouses

    Implementar un lago de datos con AWS Glue

    Implementar un lago de datos con una arquitectura tipo medall칩n utilizando Lake Formation e Iceberg

    Explicar las etapas de la vida de una consulta

    Implementar consultas SQL avanzadas

    Explicar el papel de un 칤ndice y su impacto en el rendimiento de la consulta

    Resumir los enfoques para procesar consultas agregadas y join

    Comparar los tiempos de ejecuci칩n de consultas agregadas entre almacenamiento en filas y en columnas

    Enumerar algunas estrategias para mejorar el rendimiento de las consultas

    Agregaci칩n y uni칩n de flujos de datos

Curso 4 - Modelamiento de datos, Transformaci칩n y Servicio de Datos

Este curso consta de 4 semanas de contenido y cubre estos objetivos principales de aprendizaje:

    Definir el Modelamiento de datos y su rol en reflejar la l칩gica del negocio

    Aplicar las etapas de normalizaci칩n a una tabla desnormalizada

    Describir las tablas de hechos y dimensiones de un esquema en estrella y transformar datos en tercera forma normal a un esquema en estrella

    Describir los enfoques de modelado de almacenes de datos como Inmon, Kimball, Data Vault y One Big Table

    Utilizar la Ingenier칤a de caracter칤sticas para convertir un conjunto de datos en una forma tabular esperada por los algoritmos cl치sicos de Aprendizaje autom치tico

    Preprocesar y vectorizar datos textuales

    Enumerar t칠cnicas para procesar y aumentar los datos de im치genes

    Comparar un marco de procesamiento en memoria como Spark y un marco de procesamiento basado en disco como Hadoop

    Describir las consideraciones t칠cnicas para elegir un marco de procesamiento distribuido, como Spark, frente a un marco no distribuido como Pandas dataframes

    Describir las consideraciones t칠cnicas para utilizar Spark SQL frente a Spark DataFrames al transformar datos utilizando PySpark

    Describir c칩mo funciona la transformaci칩n de flujo con un motor de procesamiento casi en tiempo real como Spark Structured Streaming

    Identificar diferentes formas de servir datos para casos de uso de anal칤tica y aprendizaje autom치tico

    Describir el prop칩sito de una capa sem치ntica que se asienta sobre el modelo de datos

    Creaci칩n de vistas y vistas materializadas

    Describir los beneficios e inconvenientes de servir datos utilizando vistas y vistas materializadas

Actividades de aprendizaje utilizadas en este programa

    V칤deos de conferencias: Una colecci칩n de videos cortos que cubren la teor칤a subyacente, as칤 como demostraciones de herramientas y tecnolog칤as importantes que necesita para cada semana. Tambi칠n hay v칤deos "Lab Walkthrough" que le dan una visi칩n general de alto nivel de los laboratorios antes de sumergirse en ellos. Algunos de los v칤deos est치n etiquetados como "[Opcional]", y est치n dise침ados para complementar su experiencia de aprendizaje, pero no ser치 evaluado en este contenido. Algunos de estos v칤deos opcionales est치n protagonizados por expertos del sector y su objetivo es proporcionarle informaci칩n pr치ctica de veteranos en el campo de opciones.

    Laboratorios: Ejercicios pr치cticos que le permitir치n aplicar lo aprendido en los v칤deos de las clases. Est치n dise침ados para ayudarle a desarrollar habilidades para determinadas tecnolog칤as de c칩digo abierto o AWS que se utilizan habitualmente al crear soluciones de ingenier칤a de datos. Hay dos tipos de laboratorios:

        Tareas de programaci칩n calificadas: Estas tareas de laboratorio cubren conceptos cr칤ticos para esa semana, y por lo general representan un mayor porcentaje de su calificaci칩n.

        Laboratorios de pr치ctica: Estos laboratorios no se califican. Los conceptos cubiertos en estos laboratorios siguen siendo importantes. Sin embargo, para reducir la presi칩n de tener que sobresalir en todos los laboratorios, hemos decidido designar algunos de ellos como "pr치cticas". De esta manera, usted puede centrarse en el aprendizaje de los laboratorios en lugar de simplemente completarlos para obtener una calificaci칩n. Te animamos a que pruebes todos los laboratorios, tanto si se califican como si son pr치cticos

    Cuestionarios: Una colecci칩n de preguntas para ayudarle a reforzar su aprendizaje sobre los conceptos cubiertos en cada semana. Encontrar치 un cuestionario calificado al final de cada semana, y la calificaci칩n que obtenga en esos cuestionarios contribuir치 a su calificaci칩n general para cada curso. Ocasionalmente encontrar치 cuestionarios de pr치ctica que contienen preguntas de reflexi칩n o breves cuestionarios calificados que contienen preguntas para comprobar su comprensi칩n a lo largo de la semana. Despu칠s de completar cada cuestionario, aseg칰rese de leer atentamente los comentarios.

    Lectura: Contenido presentado en formato textual para que pueda consultar la informaci칩n m치s adelante con mayor facilidad. Algunos de estos elementos de lectura incluir치n enlaces adicionales para que pueda obtener m치s informaci칩n sobre el tema. A menos que se especifique lo contrario, no es necesario que revise los materiales de estos enlaces externos para tener 칠xito en este programa. Algunas de estas lecturas est치n etiquetadas como "[Opcional]" y cubren material secundario que no es cr칤tico para el programa. Usted puede revisar estas lecturas si est치 interesado, pero no ser치 evaluado en este contenido.

쮺칩mo se califican las evaluaciones?

Se requiere una calificaci칩n de 80% o m치s para aprobar todos los elementos de evaluaci칩n (cuestionarios, laboratorios de pr치ctica y tareas de programaci칩n calificadas). Por favor, consulte la pesta침a"Calificaciones" de la p치gina principal del curso para ver sus calificaciones para cada evaluaci칩n.
Obtener y dar ayuda

Preguntas relacionadas con el contenido del curso: Se le anima a unirse a la comunidad DeepLearning.IA donde se puede llegar a los mentores del curso y compa침eros de clase para obtener ayuda con cualquier problema relacionado con el contenido del curso. 춰Puedes hacer clic 游댕 este enlace

 para crear tu cuenta gratuita y conectarte con la comunidad global de IA!

Preguntas relacionadas con la plataforma Coursera: Puedes consultar el Centro de ayuda al alumno
 para encontrar informaci칩n relacionada con problemas t칠cnicos espec칤ficos. Por ejemplo, los problemas t칠cnicos incluir칤an mensajes de error, dificultad para enviar tareas o problemas con la reproducci칩n de v칤deos.

Bienvenido a la primera semana de este curso sobre sistemas de origen, ingesti칩n y canalizaciones de datos. Esta semana, vamos a empezar por analizar los diferentes tipos de sistemas fuente y c칩mo puedes interactuar con estos sistemas. Como vio en el primer curso de la especializaci칩n, la generaci칩n de datos en los sistemas fuente es la primera etapa del ciclo de vida de la ingenier칤a de datos. Y como ingeniero de datos, normalmente no es responsable de generar estos datos usted mismo ni de mantener estos sistemas de origen. Sin embargo, la ingesti칩n desde los sistemas fuente es donde comenzar치n todas sus canalizaciones de datos. Por lo tanto, es importante que comprenda c칩mo se generan estos datos, d칩nde y c칩mo se almacenan. Y algunas de sus caracter칤sticas permiten crear canalizaciones de datos s칩lidas con estos sistemas ascendentes como fuente de datos. Por eso, en esta primera semana de este curso, profundizaremos en algunos de los detalles de algunos sistemas fuente comunes, incluidos los diferentes tipos de bases de datos, almacenamiento de objetos y fuentes de streaming. En los laboratorios, podr치 trabajar con estos sistemas fuente en AWS. Luego, en la segunda semana, nos centraremos en configurar diferentes tipos de ingesti칩n desde los sistemas fuente. Despu칠s de eso, en la semana 3 de este curso, analizaremos las DataOps actuales. Utilizar치 la infraestructura como c칩digo para automatizar algunas de las tareas de canalizaci칩n y utilizar치 varias herramientas para supervisar la calidad de los datos. Y, por 칰ltimo, en la cuarta y 칰ltima semana del curso, nos ocuparemos de la organizaci칩n para coordinar las tareas de sus canalizaciones de datos. Configurar치 gr치ficos ac칤clicos dirigidos, o DAG, mediante el flujo de aire, trabajar치 con la infraestructura como marcos de c칩digo e implementar치 soluciones de monitoreo para sus canalizaciones de datos. As칤 que vamos a cubrir mucho terreno en este curso. Acomp치침eme en el siguiente v칤deo para empezar a analizar m치s de cerca los diferentes tipos de sistemas fuente.

Los sistemas fuente espec칤ficos con los que trabajar치 como ingeniero de datos suelen variar seg칰n el tipo de datos que ingiera de esos sistemas. El tipo de datos m치s com칰n con el que trabajar치 son los datos estructurados, es decir, los datos organizados como tablas de filas y columnas. Lo m치s probable es que hayas trabajado con datos estructurados en el pasado, ya sea en una hoja de c치lculo o en una base de datos relacional, o tal vez incluso hayas usado Python para leer un archivo CSV. Los otros tipos de datos que encontrar치 como ingeniero de datos son los datos semiestructurados y no estructurados. Los datos semiestructurados son datos que no est치n en forma tabular, por lo que no se componen de filas y columnas, pero a칰n tienen cierta estructura. Un formato de datos semiestructurados com칰n con el que te encontrar치s es lo que se conoce como notaci칩n de objetos de JavaScript o JSON. Un archivo JSON contiene una serie de pares clave-valor. En este ejemplo, la primera clave es FirstName con el valor correspondiente Joe. La siguiente clave es LastName con el valor correspondiente Reis. Y con el formato JSON, todos estos pares clave-valor se enumeran entre llaves como este. Y puedo tener m치s pares clave-valor, incluida la informaci칩n que quiera registrar en este formato. Cada valor puede adoptar un tipo de datos diferente, por ejemplo, un n칰mero, una cadena o una matriz. De hecho, el valor de un par clave-valor puede ser incluso otra serie de pares clave-valor. Y como puedes ver en este ejemplo, el valor de direcci칩n tiene una clave: ciudad , c칩digo postal y pa칤s con los valores correspondientes. Esto crea lo que se denomina un formato JSON anidado. Por lo tanto, como puede ver, aunque los datos no se presenten como una tabla, estos datos todav칤a tienen cierta estructura. Los datos no estructurados, por otro lado, no tienen una estructura predefinida. Por ejemplo, el texto, el v칤deo, el audio y las im치genes son ejemplos de datos no estructurados. Sin embargo, observar치 que cosas como el v칤deo, el audio y las im치genes tienen una estructura inherente detr치s de escena, en el sentido de que hay dimensiones de p칤xeles y colores como el rojo, el azul y el verde. A lo largo de este curso, profundizaremos en los datos no estructurados. Cuando se trata de ingerir estos diferentes tipos de datos, me gustar칤a clasificar los sistemas fuente relevantes que puede encontrar en tres tipos generales: bases de datos, archivos y sistemas de transmisi칩n. Si bien estos tres tipos de sistemas fuente no se corresponden necesariamente uno a uno con los tres tipos de datos que acabo de mencionar, se podr칤a decir que de las bases de datos la mayor칤a de las veces se ingieren datos estructurados y semiestructurados. Desde los sistemas de streaming, con frecuencia ingerir치s mensajes semiestructurados en formato de datos. Y los archivos, bueno, los archivos pueden ser cualquier cosa, desde texto hasta im치genes , audio, v칤deo o incluso filas y columnas antiguas normales de datos tabulares. Empecemos por echar un vistazo m치s de cerca a las bases de datos. Las bases de datos almacenan la informaci칩n de forma organizada que permite buscar, recuperar, actualizar y eliminar datos. Esto funciona mediante un patr칩n transaccional conocido como CRUD, que significa crear, leer, actualizar y eliminar. La C es lo primero porque, por supuesto, los datos deben crearse antes de poder leerlos, actualizarlos o eliminarlos. Tiene sentido, 쯨erdad? Por lo general, hay una interfaz de software llamada sistema de administraci칩n de bases de datos, o DBMS, que se encuentra entre el almacenamiento de la base de datos f칤sica y la persona o la aplicaci칩n que interact칰a con la base de datos. El DBMS es lo que le permite acceder y manipular los datos almacenados en la base de datos. Hay dos tipos de bases de datos que analizaremos esta semana. Bases de datos relacionales, que almacenan informaci칩n en tablas con filas y columnas, y bases de datos no relacionales, tambi칠n conocidas como NoSQL o no solo SQL, que almacenan datos no tabulares. Analizaremos m치s de cerca estos dos tipos de bases de datos a finales de esta semana. Adem치s de las bases de datos, el siguiente tipo de sistema fuente m치s com칰n con el que interactuar치 son los archivos. Sin duda, ya tiene mucha experiencia trabajando con archivos de varios tipos. Pueden ser documentos que almacenas en tu computadora, im치genes o videos que grabas con tu tel칠fono, o incluso un archivo CSV que recibes en un correo electr칩nico de un compa침ero de trabajo. Puede parecer extra침o pensar en los archivos antiguos normales como un sistema fuente para la ingenier칤a de datos, pero en esencia, un archivo es solo una secuencia de bytes que representan informaci칩n. Las aplicaciones de todo tipo escriben datos en los archivos, por lo que los archivos son un medio universal de intercambio de datos. Y aunque no lo crea, son uno de los sistemas fuente m치s comunes con los que trabajar치 como ingeniero de datos. Los archivos, al igual que los datos, pueden estar estructurados como una hoja de c치lculo, semiestructurados como un archivo JSON o XML, o desestructurados como un archivo de texto, imagen, v칤deo o audio. Es posible que recibas estos archivos o accedas a ellos desde un sistema de archivos como Google Drive o un sistema de almacenamiento de objetos como Amazon S3, o simplemente como un archivo adjunto a un correo electr칩nico. El tercer tipo de sistema fuente del que es probable que ingieras datos son los sistemas de streaming. Y puede pensar que los sistemas de streaming proporcionan un flujo continuo de datos, grabados como mensajes que contienen informaci칩n sobre eventos. Y esos eventos incluyen algo que ocurri칩 en el mundo o un cambio en el estado de un sistema. En la pr치ctica, es posible que interact칰es con una secuencia de eventos a trav칠s de colas de mensajes u otras plataformas de transmisi칩n. Por ejemplo, un dispositivo de IoT, como un termostato inteligente, puede registrar un evento que contenga la lectura de temperatura m치s reciente y publicar ese evento como un mensaje en una plataforma de transmisi칩n como Kinesis o Kafka. Luego, como ingeniero de datos, puede configurar otro servicio para incorporar este mensaje y enviar una actualizaci칩n a un panel de an치lisis integrado. En este caso, puedes pensar en la plataforma de streaming como un sistema fuente del que est치s extrayendo datos sin procesar. En las 칰ltimas semanas de este curso, ver치 c칩mo estos sistemas de transmisi칩n tambi칠n pueden abarcar todo el ciclo de vida de la ingenier칤a de datos y usarse en las etapas de ingesti칩n y transformaci칩n para procesar datos para varios casos de uso posteriores. De hecho, puede ver lo mismo en todos los tipos de sistemas fuente, ya sea que se trate de bases de datos, archivos o sistemas de transmisi칩n. Pueden ser sistemas de los que est치 ingiriendo datos sin procesar o pueden ser sistemas que incorpore a sus canalizaciones de datos en otra etapa del ciclo de vida. En resumen, como ingeniero de datos, extraer치 datos sin procesar de diferentes sistemas de origen. Estos datos sin procesar pueden estar estructurados, semiestructurados o no estructurados, y los sistemas fuente pueden ser bases de datos, archivos o sistemas de transmisi칩n. En la pr칩xima serie de v칤deos, profundizar칠 un poco m치s en las caracter칤sticas de cada uno de estos diferentes tipos de sistemas fuente. Empezaremos con las bases de datos, despu칠s veremos c칩mo funciona el almacenamiento de objetos como fuente de datos para los archivos y, a continuaci칩n, profundizaremos en las colas de mensajes, los registros y las plataformas de streaming con m치s detalle. Acomp치침eme en el siguiente v칤deo para empezar a echar un vistazo a las bases de datos relacionales.

Como ingeniero de datos, el tipo de sistema fuente m치s com칰n con el que interactuar치 es una base de datos relacional, y esto se debe a que las bases de datos relacionales est치n en todas partes. Muchas aplicaciones web y m칩viles utilizan bases de datos relacionales en el backend, y usted tambi칠n las encontrar치. En muchos sistemas corporativos, como los sistemas de gesti칩n de relaciones con los clientes , recursos humanos y planificaci칩n de recursos empresariales, tambi칠n se suelen utilizar para lo que se denomina procesamiento de transacciones en l칤nea o sistemas OLTP, en los que es necesario ejecutar un gran volumen de transacciones de forma simult치nea, como en el caso de las reservas bancarias o en l칤nea. El nombre base de datos relacional proviene del hecho de que este tipo de base de datos se usa con mayor frecuencia para almacenar datos en diferentes tablas que est치n relacionadas entre s칤 a trav칠s de un conjunto de claves o atributos comunes. Estas tablas suelen organizarse en funci칩n de c칩mo se estructura la informaci칩n en la empresa. Por ejemplo, como ingeniero de datos que trabaja en una empresa de comercio electr칩nico, es posible que est칠 trabajando con una base de datos relacional en la que una tabla captura la informaci칩n de los clientes, otra tabla captura la informaci칩n de los productos y una tercera tabla captura la informaci칩n de los pedidos. Estructurar una base de datos de esta manera reduce la redundancia y facilita la administraci칩n de los datos al no tener la misma informaci칩n duplicada en varias filas o tablas de la base de datos. Para tener una idea de lo que quiero decir con eso, imagine por un momento que, en lugar de varias tablas, cre칩 una tabla grande para almacenar los datos de cada pedido individual de un cliente. En este caso, la tabla puede contener un gran n칰mero de columnas, incluyendo todo lo relacionado con el cliente, como su nombre, direcci칩n, n칰mero de tel칠fono, etc. Y luego tendr칤as toda la informaci칩n sobre el producto que compraron, como la marca, el n칰mero de SKU, la descripci칩n del producto y otros detalles, as칤 como los detalles del pedido, como la fecha y la hora, el importe de la compra y cu치nto pagaron. Por lo tanto, en este escenario, si un cliente hiciera un pedido de tres productos diferentes, esto se registrar칤a como tres filas independientes en tu base de datos y tendr칤as tres filas en la tabla que contienen todos los mismos datos del cliente. O si tres clientes diferentes compraron el mismo producto, entonces tendr칤as tres filas en la base de datos que contienen informaci칩n id칠ntica para ese producto. En resumen, la informaci칩n se duplicar칤a en varias filas de la tabla y, adem치s, podr칤a haber incoherencias con los datos de las distintas filas. Por ejemplo, si un cliente cambiara su direcci칩n, habr칤a una incoherencia entre las filas, a menos que volvieras y actualizaras todas las filas que contienen su direcci칩n anterior, o si cambiaran los detalles de un producto en particular, tendr칤as que volver y actualizar todas las filas que contienen la informaci칩n anterior. Si, en cambio, separas la informaci칩n de los clientes , los productos y los pedidos en varias tablas, una fila de la tabla de clientes representa a un solo cliente y una fila de la tabla de productos contiene informaci칩n sobre un solo producto. Si un cliente cambia su direcci칩n o cambian los detalles de un producto, solo tienes que actualizar la 칰nica fila que contiene la informaci칩n sobre ese cliente o producto. La forma en que una base de datos se organiza en tablas relacionadas como esta se denomina esquema de base de datos. Las bases de datos relacionales representan estas relaciones entre tablas mediante el uso de claves. Una clave principal es una columna especial o un conjunto de columnas que identifican de forma exclusiva cada fila de una tabla. Para la tabla del cliente, la clave principal podr칤a ser esta columna denominada Id. La relaci칩n entre el cliente y el pedido se puede establecer haciendo que la columna Customer_id y la tabla de pedidos hagan referencia a la columna id de la tabla de clientes. En este caso, la columna de identificador de cliente de la tabla de pedidos se denomina clave externa y hace referencia a la columna de clave principal o identificador de la tabla de clientes. M치s all치 de la estructura de filas de una base de datos relacional, cada columna tiene un nombre 칰nico y un tipo de datos espec칤fico. Por ejemplo, en la tabla de clientes puede tener columnas como ID, nombre y apellidos que contienen cadenas, y otra columna llamada edad que contiene un entero. A continuaci칩n, cada nueva fila de una tabla debe seguir la misma estructura de columnas, es decir, la misma secuencia de columnas y tipos de datos. Esto tambi칠n forma parte del esquema de la base de datos. Ahora, en una base de datos con un esquema como este, para registrar informaci칩n sobre un nuevo pedido de un cliente existente, puedes crear un nuevo registro en la tabla de pedidos e indicar el identificador del cliente de la tabla del cliente y el identificador del producto de la tabla de productos y los detalles del pedido, como la fecha, la hora y el pago, etc. Adem치s, si ese cliente cambia su direcci칩n o cambia el n칰mero de SKU del producto que ha pedido, esos cambios solo afectar치n a una fila de la tabla de clientes o productos, y la informaci칩n se mantendr치 coherente. Como puede imaginar, hay muchas maneras diferentes de establecer relaciones entre tablas, y aqu칤 es donde entra en juego el concepto de normalizaci칩n de datos. La normalizaci칩n de datos es un enfoque que se desarroll칩 en la d칠cada de 1970 para minimizar la redundancia y garantizar la integridad de los datos mediante el almacenamiento de datos en tablas de forma l칩gica. Pero ahora creo que vale la pena hacer una pausa para preguntarnos: 쯣or qu칠 preocuparse tanto por la redundancia o la informaci칩n duplicada en primer lugar? Parece l칩gico y ordenado estructurar los datos como lo he estado describiendo. Pero, 쯛ay alg칰n inconveniente? Bueno, resulta que, si bien una estructura de base de datos relacional normalizada proporciona un alto grado de integridad y minimiza la redundancia, en realidad puede resultar lenta a la hora de consultar los datos. Hoy en d칤a, el almacenamiento es relativamente barato y la velocidad suele ser fundamental. La integridad de los datos es fundamental, por supuesto, pero la respuesta a la forma exacta en que se almacenan los datos tabulares podr칤a depender del objetivo para el que se est칠 intentando optimizar. Como ingeniero de datos, es posible que est칠 ingiriendo datos normalizados de un sistema de base de datos relacional, pero seg칰n el caso de uso final que est칠 atendiendo, puede decidir organizar los datos seg칰n un modelo diferente en sus propios sistemas de almacenamiento. Hoy en d칤a, incluso hay algunos casos de uso en los que los ingenieros de datos eligen adoptar el denominado enfoque de una gran tabla u OBT, en el que todos los datos se registran en una sola tabla para un procesamiento m치s r치pido de lo que ser칤a posible si se unieran varias tablas en una base de datos relacional tradicional. Profundizaremos en los detalles del modelado de datos en el Curso 4, especializaci칩n. Cuando se trata de interactuar con la base de datos, utilizar치 un sistema de administraci칩n de bases de datos relacionales (RDBMS). Es una capa de software que se encuentra en la parte superior de una base de datos relacional. Existen muchos RDBM populares, incluidos MySQL, PostgreSQL, Oracle y SQL Server. La mayor칤a de los RDBMS admiten el lenguaje de consulta estructurado, tambi칠n conocido como secuela o SQL para abreviar. Algunas personas dicen SQL, otras dicen SQL. Puedes elegir lo que prefieras. A veces digo ambas cosas. Lo importante que debe saber es que SQL proporciona un conjunto de comandos para realizar diversas operaciones en las bases de datos relacionales. Y como ingeniero de datos, SQL formar치 parte de su trabajo diario. En el siguiente v칤deo, te explicar칠 algunos de los comandos SQL que necesitar치s para el laboratorio. Luego, en el laboratorio, tendr치 la oportunidad de practicar la consulta de datos en una base de datos relacional mediante consultas SQL. Despu칠s de eso, 칰nase a m칤 en el siguiente v칤deo, eche un vistazo a las bases de datos NoSQL.

En el laboratorio, trabajar치 con una base de datos transaccional para una empresa ficticia de alquiler de DVD llamada Rentio. Esta base de datos incluye tablas que contienen informaci칩n sobre las tiendas, el personal, los clientes, el inventario de DVD y las transacciones de alquiler de Rentio. Escribir치s sentencias o consultas SQL en un Jupyter Notebook para recuperar informaci칩n de esta base de datos con el fin de responder a preguntas empresariales. Para consultar los datos, tendr치 que entender el esquema de la base de datos. En otras palabras, tendr치 que saber los nombres de las tablas, las columnas que contienen y c칩mo se relacionan las tablas entre s칤 a trav칠s de claves principales y externas. Esta base de datos est치 normalizada, lo que significa que los datos, como las direcciones de las tiendas, el personal y los clientes, se almacenan en tablas separadas para reducir la redundancia y facilitar la actualizaci칩n de los datos cuando cambian. Del mismo modo, los datos sobre las pel칤culas y las propias transacciones de alquiler tambi칠n se almacenan en tablas separadas. Puede consultar este modelo de relaciones entre entidades que muestra las relaciones y los atributos de las tablas de la base de datos de Rentio. Para explicarte los conceptos b치sicos de SQL, solo me centrar칠 en tres tablas. La tabla de pel칤culas que contiene informaci칩n como el t칤tulo y la duraci칩n de una lista de pel칤culas, la tabla de categor칤as que contiene una lista de categor칤as de pel칤culas y la tabla film_category que muestra los films_id, junto con sus correspondientes category_id de pel칤cula. La sentencia SQL m치s b치sica comienza con una cl치usula SELECT, en la que se especifican los datos que se desean, seguida de una cl치usula FROM, en la que se especifica de qu칠 tabla se desean recuperar estos datos. Por ejemplo, supongamos que quiero explorar los t칤tulos y los a침os de estreno sin tener en cuenta la mesa cinematogr치fica. Puedo escribir SELECT title, release_year FROM film. Una vez que ejecute esta consulta, obtendr칠 una lista de todos los t칤tulos y a침os de lanzamiento. Resulta que hay 1000 pel칤culas en esta mesa de filmaci칩n. Si no quiero ver la lista completa, puedo limitar el n칰mero de resultados devueltos mediante una cl치usula LIMIT. Por ejemplo, si a침ado LIMIT 10 al final de esta consulta, solo obtendr칠 los 10 primeros t칤tulos y los a침os de estreno de la tabla de pel칤culas. Pero, 쯤u칠 sucede si desea recuperar datos de todas las columnas de una tabla? Bueno, puedes enumerar todos los nombres de las columnas en la cl치usula SELECT de esta manera, o puedes usar un atajo y escribir SELECT * FROM film. Esto recuperar치 los datos de todas las filas y columnas de la tabla de pel칤culas. En el pr칩ximo curso, aprender치 c칩mo se ejecutan las canteras entre bastidores. Pero por ahora, ten en cuenta que recuperar todos los datos de todas las columnas puede requerir muchos recursos de procesamiento, especialmente si tu conjunto de datos es muy grande. Recomiendo usar SELECT * 칰nicamente para recuperar todos los datos de una tabla, donde puede filtrar los resultados con alguna condici칩n booleana. Por ejemplo, supongamos que solo te interesa explorar pel칤culas de menos de 60 minutos de duraci칩n. Puede a침adir una cl치usula WHERE despu칠s de la cl치usula FROM para filtrar los resultados en funci칩n de la longitud de la columna. Escribir칠 SELECT * FROM pel칤cula cuya duraci칩n sea inferior a 60. Esto devolver치 una lista de las 96 pel칤culas que tienen menos de una hora de duraci칩n. Tambi칠n puedes ordenar los resultados por cualquier columna que desees. Por ejemplo, puedo a침adir ORDER BY length despu칠s de la cl치usula WHERE para almacenar los resultados en orden ascendente seg칰n la duraci칩n de la pel칤cula. Si quiero ver los resultados en orden descendente por duraci칩n de la pel칤cula, puedo a침adir la palabra clave D-E-S-C o DESC al final de la cl치usula ORDER BY. Si quisiera limitar los resultados a, digamos, 10 registros, puedo agregar un L칈MITE 10 al final de esta consulta. Acabas de ver c칩mo puedes recuperar datos de una sola tabla usando las cl치usulas SELECT, FROM, WHERE, ORDER BY y LIMIT. 쯈u칠 sucede si desea explorar los datos de m치s de una tabla? Puede usar la cl치usula JOIN para combinar registros de dos o m치s tablas en funci칩n de las columnas compartidas entre esas tablas. Por ejemplo, supongamos que quiero obtener una lista de t칤tulos de pel칤culas y su correspondiente categor칤a_pel칤cula para todas las pel칤culas de menos de 60 minutos de duraci칩n. Modifiquemos la consulta anterior, donde seleccion칠 una estrella de una pel칤cula cuya longitud es inferior a 60. Quiero combinar las filas de la tabla de pel칤culas con las filas de la tabla film_category en funci칩n de los identificadores de pel칤culas coincidentes. Al final de la cl치usula FROM, escribir칠 JOIN film_category ON film.film_id = film_category.film_id. De esta forma, los resultados devueltos incluir치n todas las columnas de la tabla de pel칤culas, junto con las columnas de la tabla film_category para cada par de film_id coincidentes en ambas tablas. Tenga en cuenta que la tabla film_category solo incluye el category_id, pero no el nombre de la categor칤a. Por lo tanto, necesitamos hacer otra combinaci칩n para combinar estos registros con las filas de la tabla de categor칤as en funci칩n de los category_id. A침adir칠 la categor칤a JOIN ON film_category.category _id = category.category_id. Ahora los resultados incluir치n todas las columnas de la tabla de pel칤culas, todas las columnas de la tabla film_category y todas las columnas de la tabla de categor칤as. Como solo quiero el t칤tulo de la pel칤cula y la categor칤a de pel칤cula correspondiente, puedo modificar la instrucci칩n SELECT para especificar que solo quiero la columna film.title en la columna category.name. Tenga en cuenta que, de forma predeterminada, la cl치usula JOIN combina solo los registros de ambas tablas que tienen un valor de columna coincidente especificado en la instrucci칩n ON. No incluir치 ning칰n registro de ninguna de las tablas que no tenga valores coincidentes. Por ejemplo, si la tabla de pel칤culas tiene una fila con film_id que no aparece en la tabla film_category, esa fila no se incluir치 en los resultados. Este tipo de uni칩n tambi칠n se conoce como INNER JOIN, y puede pensar en los resultados de la uni칩n como la parte central superpuesta de un diagrama de Venn. Los otros tipos de combinaciones incluyen LEFT JOIN, que devuelve todos los registros de la primera tabla, junto con los registros coincidentes de la segunda tabla, RIGHT JOIN, que devuelve todos los registros de la segunda tabla, junto con los registros coincidentes de la primera tabla, y FULL JOIN, que devuelve todos los registros de ambas tablas y combina los que tienen valores coincidentes. Volviendo a los resultados de la 칰ltima consulta, puedo ver que bastantes de los cortometrajes pertenecen a la categor칤a infantil o documental. Digamos que quiero saber con certeza cu치l es la categor칤a m치s popular de cortometrajes. Puedo usar el comando GROUP BY para agrupar las filas seg칰n la categor칤a_pel칤cula. A continuaci칩n, utilice el comando COUNT para contar el n칰mero de registros de cada una de las categor칤as de pel칤culas. El comando GROUP BY se escribe despu칠s de la cl치usula WHERE. Aqu칤 a침adir칠 GRUPO POR CATEGOR칈A.nombre. Luego modificar칠 la instrucci칩n SELECT para seleccionar category.name y COUNT (*), que cuenta todas las filas de cada categor칤a. Tambi칠n usar칠 el comando AS para dar a la salida de este recuento el nombre de alias film_count. Por 칰ltimo, ordenar칠 los resultados seg칰n el recuento de pel칤culas en orden descendente. Como puede ver, la categor칤a m치s popular para cortometrajes de menos de una hora es la de documental, seguida de acci칩n y luego de ni침os. Estos son algunos de los comandos SQL m치s comunes. Ahora est치 listo para empezar con el laboratorio. El laboratorio tambi칠n cubre algunas operaciones de manipulaci칩n de datos, como CREATE, INSERT INTO, UPDATE y DELETE. Aseg칰rese de leer las instrucciones detenidamente cuando intente cada uno de los ejercicios. Cuando termine el laboratorio, 칰nase a m칤 para echar un vistazo a las bases de datos NoSQL.

Pautas antes de empezar los laboratorios de este curso
Estado: Traducido autom치ticamente del Ingl칠s
Traducido autom치ticamente del Ingl칠s
Informaci칩n:
Este elemento incluye contenido que a칰n no se tradujo a tu idioma preferido.

Por favor, consulte las siguientes indicaciones importantes al iniciar cualquier laboratorio en este curso.

    No utilice una cuenta personal de AWS. Los laboratorios ser치n aprovisionados para usted sin costo alguno en este curso. Si tiene una cuenta personal de AWS, aseg칰rese de cerrar la sesi칩n de esa cuenta antes de comenzar los laboratorios para evitar que se le cobren los servicios de AWS utilizados durante los laboratorios.

    En la medida de lo posible, evite utilizar ordenadores port치tiles proporcionados por la empresa o conectarse a Internet en su lugar de trabajo para trabajar en los laboratorios. Estos pueden tener configuraciones de seguridad que inadvertidamente bloqueen el tr치fico a los recursos que usted utilizar치. Varios alumnos han informado de problemas que no hemos podido reproducir, y la soluci칩n ha sido simplemente cambiar de un port치til de trabajo a un dispositivo personal.

    Si encuentras alg칰n problema con los laboratorios, puedes buscar o crear un tema en esta categor칤a

 en nuestro Foro de la Comunidad. Nuestro personal y nuestros mentores estar치n encantados de ayudarte. 쮸칰n no eres miembro? Puedes hacer clic en este enlace

     para crear tu cuenta gratuita. Si no puedes acceder al laboratorio o a sus recursos, consulta el p치rrafo siguiente.

    Por favor, evita utilizar la opci칩n "Informar de un problema" de Coursera en la parte inferior de la p치gina cuando tengas problemas con los laboratorios. Esto no se controla con tanta frecuencia como el Foro, por lo que la respuesta puede ser m치s lenta.

Problemas para acceder a los laboratorios

Si te encuentras con alguno de los siguientes problemas (enumerados a continuaci칩n) y no puedes acceder al laboratorio, rellena este formulario de google

. Tu cuenta de AWS se actualizar치 en un plazo de 2 d칤as laborables.

Si su problema no es ninguno de los que se indican a continuaci칩n, cree o busque un tema en la plataforma de la Comunidad en lugar de utilizar el formulario (puede hacer clic en este enlace

 para crear su cuenta). Enviar el formulario para otros problemas puede no ayudar a resolverlos.

    El laboratorio se bloquea en la p치gina de carga durante m치s de 20 minutos: intente primero actualizar la p치gina web. Si el problema persiste, rellena el formulario

    .

    El enlace a la consola de AWS est치 vac칤o: En algunos de los laboratorios, se le pedir치 que ejecute este comando para obtener el enlace a la consola de AWS. Si recibes un enlace vac칤o, rellena el formulario

    .

    Si ejecutas un comando en el terminal para obtener el endpoint/direcci칩n de una base de datos u otro recurso y no obtienes ning칰n recurso, por favor rellena el formulario

.

Problemas de formaci칩n de lagos insuficientes (este es un problema temporal en el que estamos trabajando para solucionarlo, pero por ahora rellena el formulario

    ).

Cuando rellenes el formulario, se te guiar치 para que introduzcas la siguiente informaci칩n:

    La direcci칩n de correo electr칩nico de tu cuenta de Coursera y tu nombre de usuario en la comunidad DeepLeaning.IA (Puedes hacer clic en este enlace

     para crear tu cuenta).

    ID del laboratorio: puedes obtener el ID del laboratorio haciendo clic en el signo de interrogaci칩n (ayuda del laboratorio) y luego copiando el ID del laboratorio.

 3.  Si puede acceder al terminal de VS Code, deber치 ejecutar este comando en el terminal:

cat /home/coder/vocapi_logger > vocapi_logger

Este comando crear치 el archivo vocapi_logger que contiene la informaci칩n de registro. Tendr치s que copiar el texto de este archivo y pegarlo en el lugar correspondiente del formulario.



C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "C2W1-2.txt"  
 echo.
) 
A principios de la d칠cada de 2000, gigantes tecnol칩gicos como Google y Amazon comenzaron a superar sus bases de datos relacionales. Necesitaban procesar grandes vol칰menes de datos de fuentes dispares que no encajaban perfectamente en el modelo de base de datos relacional. La aplicaci칩n de estructuras tabulares generar칤a problemas de redundancia de datos y rendimiento a escala, por lo que estas empresas lideraron el desarrollo de nuevas bases de datos no relacionales distribuidas para escalar sus plataformas web. De esta manera, las bases de datos NoSQL se desarrollaron para superar las limitaciones de las bases de datos relacionales, intercambiando ciertas caracter칤sticas del RDBMS, como una s칩lida consistencia, uniones y un esquema fijo, para obtener m치s flexibilidad, escalabilidad y un rendimiento mejorado. Antes de continuar, aclaremos una cosa: NoSQL no significa No SQ, significa no solo SQL. Es una categor칤a de bases de datos que rompe con el marco relacional que vimos en el v칤deo anterior. Sin embargo, algunas bases de datos no relacionales a칰n admiten SQL o SQL, como los lenguajes de consulta. Repasemos los conceptos b치sicos de las bases de datos NoSQL. Las bases de datos NoSQL tienen estructuras no tabulares. Pueden admitir varios formatos de datos, incluidos valores clave, documentos de columna ancha, gr치ficos y otros. Hablar칠 sobre la clave, el valor y la fuente del documento m치s adelante en este v칤deo y ver치s algunos de estos otros tipos en los pr칩ximos cursos. A diferencia de las bases de datos relacionales, las bases de datos NoSQL no requieren esquemas predefinidos, lo que significa que tiene m치s flexibilidad a la hora de decidir c칩mo desea almacenar los datos. Las bases de datos NoSQL destacan por su escalado horizontal, lo que significa distribuir autom치ticamente los datos y las cargas de trabajo en varios servidores para satisfacer las crecientes demandas de tr치fico. Cuando un usuario escribe datos en una base de datos NoSQL que est치 distribuida en varios servidores o nodos, la operaci칩n de escritura se realiza primero en un solo nodo de este sistema distribuido, que es una ubicaci칩n en la que se ejecuta una versi칩n de la base de datos. En ese caso, es posible que se produzca un ligero retraso antes de que esos cambios se propaguen a todos los dem치s nodos del sistema. A diferencia de las bases de datos relacionales, las bases de datos NoSQL funcionan seg칰n el principio de una coherencia eventual en lugar de una s칩lida, lo que significa que la base de datos le permitir치 leer desde un nodo que no haya recibido la 칰ltima actualizaci칩n de escritura y es posible que no obtenga los datos m치s actualizados. Sin embargo, con el tiempo suficiente, la base de datos ser치 coherente y la lectura de los datos de cualquier nodo le proporcionar치 los mismos datos. Con una base de datos relacional que proporciona una coherencia s칩lida, no podr치 leer los datos hasta que se hayan actualizado todos los nodos del sistema. De esta manera, la coherencia final no permite que ninguna base de datos individual priorice la velocidad. Esto es perfecto para aplicaciones en las que la disponibilidad y la escalabilidad del sistema son m치s importantes que la coherencia en tiempo real, como las plataformas de redes sociales o las redes de distribuci칩n de contenido. En cuanto a la integridad de los datos, no todas las bases de datos NoSQL garantizan los principios de atomicidad, coherencia, aislamiento y durabilidad, tambi칠n conocidos como conformidad con ACID, que veremos en el siguiente v칤deo, pero algunas s칤, por ejemplo, MongoDB. Esto significa que si obtiene datos de una base de datos NoSQL, es posible que deba tomar medidas adicionales para garantizar la integridad de los datos. Por 칰ltimo, las bases de datos NoSQL utilizan lenguajes de consulta especializados adaptados a su modelo de datos, que a menudo, pero no siempre, son diferentes de los de SQL. Analicemos m치s de cerca dos tipos comunes de bases de datos NoSQL, las bases de datos de valores clave y las bases de datos de documentos. Una base de datos clave-valor almacena los datos como una colecci칩n de pares clave-valor, similar a lo que puede encontrar en un archivo JSON o en una estructura de diccionario de Python. La clave sirve como identificador 칰nico para recuperar el valor correspondiente. Tanto las claves como los valores pueden ser objetos simples o complejos. Este tipo de base de datos NoSQL es perfecto para escenarios en los que se necesita una b칰squeda r치pida de datos, como el almacenamiento en cach칠 de los datos de las sesiones de usuario en una aplicaci칩n web o m칩vil. Por ejemplo, cuando un usuario inicia sesi칩n en una aplicaci칩n de comercio electr칩nico, acciones como ver diferentes productos, a침adir art칤culos al carrito de la compra y finalizar la compra se pueden almacenar en una base de datos de valores clave con el identificador de sesi칩n del usuario como identificador 칰nico. Los almacenes de documentos son un tipo especial de base de datos de valores clave que almacenan datos en JSON como documentos. Cada documento tiene una clave 칰nica que identifica un documento y le permite recuperar los datos de ese documento. Los documentos se organizan en colecciones, por lo que puede pensar en una colecci칩n como una tabla en una base de datos relacional y en un documento como una fila. En este ejemplo, los datos se almacenan en una colecci칩n denominada usuarios. Cada documento representa a un 칰nico usuario y el identificador es la clave que identifica de forma exclusiva a cada usuario. Esta localidad facilita la recuperaci칩n de toda la informaci칩n sobre un usuario en particular en comparaci칩n con una base de datos relacional, donde la informaci칩n del usuario puede estar distribuida en varias tablas. Sin embargo, los almacenes de documentos no admiten combinaciones, por lo que es m치s dif칤cil y menos eficiente combinar informaci칩n de varios documentos en comparaci칩n con combinar informaci칩n de varias tablas en una base de datos relacional. La ventaja, sin embargo, es la noci칩n de un esquema flexible. Como ha visto con las bases de datos relacionales, todos los registros deben ajustarse a un esquema fijo, pero con las bases de datos de valores clave y los almacenes de documentos, los registros de datos no tienen una estructura fija o predefinida. Los almacenes de documentos se utilizan habitualmente para aplicaciones que incluyen cat치logos de administraci칩n de contenido y lecturas de sensores. Cada interacci칩n, producto o lectura de sensor de un dispositivo de IoT, por ejemplo, se puede almacenar como un 칰nico documento con un esquema flexible. Pero ten cuidado, esta flexibilidad puede tener un inconveniente. He visto c칩mo las bases de datos de documentos se convierten en pesadillas absolutas para gestionar las consultas. Y si est치 ingiriendo datos de un almac칠n de documentos NoSQL como sistema de origen, la flexibilidad del esquema hace que sea a칰n m치s f치cil para los propietarios del sistema de origen cambiar algo que interrumpa sus canalizaciones de datos. Tanto las bases de datos relacionales como las bases de datos NoSQL se pueden utilizar como una especie de amplia gama de aplicaciones. Cuando se trata de aplicaciones que procesan transacciones en l칤nea en 치reas como la banca, las finanzas y el comercio electr칩nico, entre otras, las cosas suceden r치pidamente, el dinero cambia de manos y los productos est치n en movimiento. Y en este tipo de aplicaciones de procesamiento de transacciones en l칤nea o OLTP, cualquier error o inconsistencia en los datos puede causar problemas importantes. En el siguiente v칤deo, analizaremos los principios de atomicidad, coherencia, aislamiento y durabilidad, tambi칠n conocidos como principios ACID, que son de vital importancia para sus fuentes de datos y canalizaciones de datos cuando trabaje con sistemas OLTP. Nos vemos all칤.

Tanto las bases de datos relacionales como las no relacionales pueden soportar tasas de transacci칩n muy altas. Se usan com칰nmente en el procesamiento de transacciones en l칤nea o en los sistemas OLTP. Por lo general, estos sistemas necesitan almacenar los estados de las aplicaciones que cambian r치pidamente, como los detalles de los saldos de las cuentas bancarias o los pedidos en l칤nea. La mayor칤a de los sistemas de bases de datos relacionales cumplen con lo que se conoce como compatibles con ACID, lo que significa que cumplen con los principios de atomicidad, coherencia, aislamiento y durabilidad, lo que ayuda a garantizar que las transacciones se procesen de manera confiable y precisa en un sistema OLTP. Por el contrario. Muchas bases de datos NoSQL no cumplen con los activos de forma predeterminada, pero muchas le ofrecen la posibilidad de configurarlas para que cumplan con los activos. En este v칤deo, voy a hablar sobre qu칠 es cada principio de activo para que pueda tener una mejor idea de cu치ndo se aplicar치n estos principios al trabajo que realiza como ingeniero de datos. Pero primero, pensemos por un momento en lo que ocurre si las transacciones, por ejemplo, se realizan en un sistema bancario o no se procesan de manera confiable o precisa. Por ejemplo, 쯤u칠 pasa si intentas transferir dinero en l칤nea de una cuenta a otra y el sitio web del banco simplemente deja de funcionar mientras intentas transferir dinero? Cuando finalmente regreses a tu cuenta, esperar치s descubrir que la transacci칩n se realiz칩 o no, pero no que el dinero se haya deducido de una cuenta, pero no que el dinero se haya deducido de una cuenta, pero no se haya agregado a la otra. Lo mismo se aplica a todos los sistemas, desde la banca hasta las compras en l칤nea y muchos m치s. El primer principio de los activos es la atomicidad, que garantiza que las transacciones sean at칩micas o, en otras palabras, que se traten como una sola unidad indivisible. Una transacci칩n puede constar de varias operaciones. Sin embargo, el principio de atomicidad garantiza que todas las operaciones de una transacci칩n se ejecuten correctamente o que ninguna de ellas lo haga. Por ejemplo, imagine la transacci칩n de un cliente que hace un pedido de un art칤culo. Supongamos que esta transacci칩n implica dos operaciones: deducir el costo total de la cuenta del cliente y actualizar el inventario para reflejar el art칤culo comprado. Supongamos que el cliente experimenta un error de conexi칩n de red justo despu칠s de deducir el coste total de su cuenta. Pero antes de que se completara la actualizaci칩n del inventario. El principio de atomicidad garantiza que ambas operaciones se realicen como una sola transacci칩n. Si el error de conexi칩n de red impide que se complete la segunda operaci칩n, la primera se anular치, por lo que no se cobrar치 al cliente y se producir치 un error en toda la transacci칩n. El segundo principio es la coherencia, lo que significa que cualquier cambio en los datos realizado dentro de una transacci칩n debe seguir el conjunto de reglas o restricciones definidas por el esquema de la base de datos. Esto garantiza que la base de datos pasar치 de un estado v치lido a otro. Por ejemplo, supongamos que el esquema de la base de datos de inventario evita que cualquier nivel de existencias descienda por debajo de cero. Supongamos que el nivel de existencias de un art칤culo en particular es actualmente uno. Si un cliente intenta hacer un pedido de dos de esos art칤culos, la operaci칩n fallar치 y se descartar치 toda la transacci칩n para garantizar que la base de datos siga siendo coherente con el esquema predefinido. Como ya he mencionado, este es el valor predeterminado en las bases de datos relacionales, pero tendr칤a que configurarse en la mayor칤a de los sistemas NoSQL. Solo una nota para mayor claridad, la palabra coherencia termina sobrecarg치ndose un poco aqu칤. En el v칤deo anterior, describ칤 la s칩lida propiedad de consistencia de las bases de datos relacionales, que hace referencia a la idea de que todos los nodos de un sistema distribuido proporcionar치n los mismos datos actualizados. Resulta que la consistencia s칩lida de un sistema de base de datos es el resultado del cumplimiento de los principios de ACID, pero es un concepto ligeramente diferente al de la consistencia representada por la C en ACID. El siguiente principio es el aislamiento, que garantiza que cuando varios clientes intentan ejecutar transacciones simult치neamente, cada transacci칩n se ejecute de forma independiente en orden secuencial. Por ejemplo, supongamos que el inventario muestra que quedan 10 unidades de un art칤culo. Supongamos que dos clientes hacen cada uno un pedido de cinco de estos art칤culos al mismo tiempo. El principio de aislamiento garantiza que, aunque la marca de tiempo de estas dos transacciones sea la misma, ambas transacciones se realizar치n de forma independiente en secuencia, de modo que cuando se completen las dos transacciones, el nivel de inventario de ese art칤culo ser치 cero y no cinco. Del mismo modo, si un cliente pide cinco y otro pide 10 del art칤culo al mismo tiempo, el pedido que se procese primero, lo procesaremos y el segundo fallar치, lo que dar치 como resultado un nivel de inventario de cinco o cero. El principio final es la durabilidad, que garantiza que, una vez que se complete una transacci칩n, sus efectos sean permanentes y sobrevivan a cualquier fallo posterior del sistema, como una p칠rdida de energ칤a. Esto es esencial para mantener la confiabilidad de la base de datos, incluso cuando se enfrenta a un evento inesperado, como un desastre natural. En resumen, los principios ACID garantizan que una base de datos mantendr치 una imagen coherente del mundo. Eso puede parecer l칩gico y relativamente sencillo. Sin embargo, en el mundo real, una base de datos puede particionarse en varios servidores debido a su tama침o o replicarse en varios centros de datos para lograr redundancia y velocidad. En estos casos, es especialmente importante saber que los datos que est치 leyendo y escribiendo permanecen consistentes en toda la red de servidores. Este es el principio de lo que se denomina consistencia s칩lida, que es una caracter칤stica clave del cumplimiento de ACID que se mantiene incluso en un sistema de base de datos distribuido. Ahora bien, es importante tener en cuenta que, si bien las bases de datos relacionales suelen cumplir con ACID, no todas las bases de datos deben cumplir con todos los principios de ACID para poder soportar los backends de las aplicaciones. Algunas bases de datos NoSQL solo poseen cierto grado de conformidad con ACID. Sin embargo, al relajar una o m치s de estas restricciones, puede mejorar ciertos aspectos del rendimiento de las bases de datos y hacerlas m치s escalables. Como ingeniero de datos, comprender cu치ndo su base de datos debe cumplir con ACID puede ayudarlo a prevenir desastres. En el siguiente laboratorio, trabajar치 con DynamoDB, una base de datos de valores clave de NoSQL. Acomp치침eme en el siguiente v칤deo para echar un vistazo r치pido al laboratorio antes de entrar.

En el siguiente laboratorio, trabajar치 con Amazon DynamoDB como base de datos de valores clave y aplicar치 algunas operaciones de creaci칩n, lectura, actualizaci칩n y eliminaci칩n o CRUD a los datos de esta base de datos NoSQL. En este v칤deo, le proporcionar칠 una descripci칩n general de algunas de las funciones de DynamoDB, los datos con los que trabajar치 y algunos de los m칠todos de DynamoDB que utilizar치 para aplicar operaciones CRUD a los datos. DynamoDB es una base de datos de valores clave que almacena un conjunto de elementos de valores clave en una tabla. Cada fila contiene los atributos de un elemento y se identifica de forma exclusiva mediante la clave del elemento. Por ejemplo, aqu칤 hay dos elementos de valor clave en los que cada uno corresponde a una persona. La clave representa el identificador de la persona y el valor consiste en un conjunto de atributos que describen a la persona. DynamoDB almacena estos datos en una tabla, como se muestra aqu칤. Cada fila contiene el identificador de la persona y los atributos correspondientes. Dado que una columna de ID de persona identifica de forma exclusiva cada fila, representa la clave principal de esta tabla. Cuando trabaja con DynamoDB, tambi칠n puede hacer referencia a la clave principal como clave de partici칩n. Esto se debe a que DynamoDB usa la clave principal para determinar la partici칩n o, dicho de otro modo, el almacenamiento f칤sico en el que se almacenar치 el elemento. Tambi칠n puede definir una clave principal compuesta para una tabla de DynamoDB. Por ejemplo, esta es una tabla en la que cada fila representa el art칤culo de un pedido y se identifica de forma 칰nica mediante una clave principal compuesta. Esta clave compuesta se compone de dos claves. La primera representa el identificador del pedido y se denomina clave de partici칩n. La segunda representa el n칰mero de l칤nea del art칤culo con un pedido y se denomina clave de clasificaci칩n. Con esta clave compuesta, puede tener dos elementos con la misma clave de partici칩n, pero deben tener diferentes claves de clasificaci칩n, de modo que pueda identificar cada elemento de forma 칰nica. DynamoDB usa la clave de partici칩n para determinar en qu칠 partici칩n se almacenar치 el elemento y la clave de clasificaci칩n para ordenar los elementos de la misma partici칩n. En estas dos tablas, puedes ver que cada elemento puede tener sus propios atributos distintos. Esto se debe a que las tablas de DynamoDB no tienen esquemas, lo que significa que no es necesario definir los atributos de antemano. En este laboratorio, crear치 cuatro tablas de DynamoDB e interactuar치 con ellas mediante c칩digo Python en un cuaderno Jupyter. Para ello, utilizar치 Boto3, que es el kit de desarrollo de software de AWS que le permite crear y configurar los servicios de AWS mediante Python. Esta es la documentaci칩n de Boto3, donde encontrar치 una lista de m칠todos que puede utilizar para interactuar con varios recursos de AWS. Si hace clic en DynamoDB aqu칤, encontrar치 la lista de todos los m칠todos disponibles que puede usar al trabajar con una tabla de DynamoDB. En el laboratorio, se centrar치 en los m칠todos que puede usar para aplicar las operaciones CRUD, como CreateTable para crear las tablas, putItem, WriteBatchItems, updateItem para agregar o actualizar elementos en las tablas creadas, scan, getItem, query para leer los elementos de las tablas y DeleteItem para eliminar elementos de las tablas. Para llamar a estos m칠todos, primero debe crear un objeto de cliente, como se muestra aqu칤, que le proporciona una interfaz que representa una tabla de Amazon DynamoDB. Con este objeto de cliente, llamar치 a cualquiera de estos m칠todos de DynamoDB. Para crear las tablas, se proporcionan estos cuatro archivos JSON que se descargan de la Gu칤a para desarrolladores de Amazon DynamoDB. Leer치 y cargar치 cada uno de los contenidos en una tabla de DynamoDB. El archivo del cat치logo de productos contiene informaci칩n sobre algunos productos que se venden en Amazon. Cada producto se define por su identificador, que utilizar치s como clave principal simple para la tabla correspondiente. El archivo del foro contiene informaci칩n sobre algunos foros de AWS en los que los usuarios publican preguntas o inician hilos sobre los servicios de AWS. Para cada foro, puedes encontrar el n칰mero total de hilos, mensajes y vistas. Cada foro se define por su nombre, que utilizar치s como clave principal simple para la tabla correspondiente. El archivo de hilo contiene informaci칩n sobre cada hilo del foro, como el asunto del hilo, el mensaje del hilo, el n칰mero total de visitas y respuestas al hilo determinado. Cada hilo se define por el nombre del foro y el asunto del hilo, que utilizar치s como clave principal compuesta para la tabla correspondiente. El archivo de respuesta contiene informaci칩n sobre las respuestas de cada hilo, como la hora de la respuesta, el mensaje de respuesta, el usuario que public칩 la respuesta y el ID de la respuesta, que es una concatenaci칩n del foro y el asunto del hilo. Cada respuesta se define por el identificador y la hora de la respuesta, que utilizar치s como clave principal compuesta para la tabla correspondiente. Echemos un vistazo al archivo del foro. Contiene una lista de elementos de solicitud de venta, cada uno de los cuales contiene un 칰nico elemento. Extraer치 los atributos de cada elemento y, a continuaci칩n, los cargar치 en la tabla de DynamoDB. Tenga en cuenta que cada atributo est치 definido por otro par clave-valor para definir el tipo del atributo y su valor. Por lo tanto, la letra N, que significa n칰mero, y la S, que significa cadena, son ejemplos de descriptores de tipos de datos de DynamoDB que indican a DynamoDB c칩mo interpretar el tipo de cada atributo. Todos los archivos JSON restantes siguen el mismo patr칩n, por lo que te animo a que hojees r치pidamente los dem치s archivos JSON para comprender mejor cada uno de ellos. Repasemos ahora el primer ejercicio del laboratorio. Ya empec칠 el laboratorio y abr칤 este cuaderno de Jupyter. Ejecutar칠 esta celda para importar los paquetes necesarios y luego esta celda para definir esta variable que usar치 en el laboratorio con fines de etiquetado. Tras estas dos celdas, encontrar치 una explicaci칩n sobre los datos y la tabla de DynamoDB, que ya he explicado en este v칤deo. El primer ejercicio consiste en crear las cuatro tablas en las que utilizar치 el m칠todo CreateTable de Boto3. Si consulta la documentaci칩n, ver치 que este m칠todo espera un nombre para la tabla y una definici칩n para la clave principal. Puede definir los atributos que componen la clave principal mediante el argumento de definiciones de atributos. Con el argumento del esquema de claves, puede especificar para cada atributo si es una clave de partici칩n o una clave de clasificaci칩n. Hash aqu칤 significa clave de partici칩n y rango significa clave de clasificaci칩n. En el laboratorio, no necesitar치 codificar estos argumentos para cada tabla. En su lugar, se le proporcionan estos diccionarios que representan los argumentos de cada tabla. Para crear las tablas, se le proporciona esta funci칩n CreateTableDB dentro de la cual llamar치 al m칠todo CreateTable de Boto3. La funci칩n CreateTableDB toma el nombre de la tabla y quargs, que es la abreviatura de argumentos de palabras clave, es un diccionario que contiene los argumentos adicionales del m칠todo Boto3. Las dos estrellas situadas junto a los cuarteles significan que los elementos del diccionario est치n desempaquetados en una secuencia de argumentos. Aqu칤 tendr치s que completar esta l칤nea de c칩digo en la que se llama al m칠todo Boto3 mediante el objeto cliente. Para el nombre de la tabla, usar칠 la entrada TableName y luego pasar칠 los argumentos restantes usando quargs. Ahora ejecutar칠 la celda y las dos celdas siguientes para crear las tablas. Y desde aqu칤, puedes seguir trabajando en los ejercicios restantes. Ten en cuenta que algunos de los ejercicios est치n marcados como opcionales, as칤 que no dudes en saltarte las partes opcionales. Despu칠s del laboratorio, acomp치침eme en el siguiente v칤deo para ver el segundo tipo principal de sistema de almacenamiento con el que trabajar치, los archivos y, m치s espec칤ficamente, los archivos en el almacenamiento de objetos en la nube.


C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "C2W1-3.txt"  
 echo.
) 
>> Como dije a principios de esta semana, los archivos son uno de los sistemas fuente m치s comunes con los que se enfrentar치 todos los d칤as como ingeniero de datos. Y es posible que recibas estos archivos o accedas a ellos desde un sistema de archivos como Google Drive o un sistema de almacenamiento de objetos como sthree, o simplemente como un archivo adjunto a un correo electr칩nico. Si bien los archivos pueden venir de muchos lugares diferentes, el almacenamiento de objetos es sin duda el mecanismo m치s importante para el almacenamiento y la recuperaci칩n de archivos en su trabajo como ingeniero de datos. Object Storage trata los datos, en este caso los archivos, como objetos individuales y los almacena en una estructura plana que no se ajusta a la jerarqu칤a tradicional del sistema de archivos. Esto significa que, aunque est칠 acostumbrado a almacenar archivos en una jerarqu칤a de carpetas y subcarpetas en su equipo local, Object Storage no tiene ninguna jerarqu칤a. Ahora, solo como nota al margen, esto de la estructura plana puede resultar confuso. Si vas a Amazon S3, por ejemplo, hay un bot칩n Crear carpeta y puedes crear carpetas y subcarpetas a tu antojo y almacenar tus archivos sin problemas en lo que se parece mucho a un sistema de archivos jer치rquico. Sin embargo, resulta que esto es solo una caracter칤stica de la interfaz de usuario para mantener las cosas organizadas de una manera que resulte familiar. El mecanismo de almacenamiento actual es plano, lo que significa que, aunque parezca que tienes carpetas y subcarpetas en la interfaz de usuario, todos los archivos se almacenan en el nivel superior. Y esto es por dise침o, ya que permite un acceso r치pido y sencillo a todos los objetos sin preocuparse por la sobrecarga de la estructura de carpetas. De todos modos, estoy divagando. Por lo tanto, los objetos pueden ser cualquier cosa, desde archivos CSV, JSON, texto, v칤deo, imagen o audio hasta datos binarios legibles por m치quina. Esta versatilidad convierte a Object Storage en el repositorio perfecto para datos semiestructurados y no estructurados, lo que puede resultar 칰til cuando se admiten aplicaciones como la entrega de datos para entrenar modelos de aprendizaje autom치tico. El almacenamiento de objetos desempe침a un papel crucial como fuente de datos. En semanas y cursos posteriores, ver치 c칩mo el almacenamiento de objetos tambi칠n se integra a lo largo de todo el ciclo de vida de la ingenier칤a de datos. Pero por ahora, echemos un vistazo a algunos de los componentes clave del almacenamiento de objetos. En Object Storage, a cada objeto se le asigna un identificador 칰nico universal, o UUID, que es una especie de clave. Esta clave es necesaria para acceder al objeto correspondiente y administrarlo. Cada objeto tambi칠n tiene metadatos asociados, que son informaci칩n adicional sobre el objeto, como la fecha de creaci칩n, el tipo de archivo o el propietario. Vale la pena se침alar que despu칠s de la escritura inicial, los objetos t칠cnicamente se vuelven inmutables y no admiten operaciones aleatorias de escritura o adici칩n. En este sentido, un archivo de Object Storage no es como una tabla de una base de datos relacional o un documento de una base de datos no relacional que se puede actualizar o anexar. Para cambiar los datos almacenados en un objeto, debe volver a escribir el objeto completo y hacer que el uuid apunte a este nuevo objeto. Con el almacenamiento de objetos, puede habilitar el control de versiones de objetos, lo que le permite agregar metadatos a un objeto para especificar su versi칩n. Por lo tanto, cuando actualizas un objeto, en lugar de sobrescribir el objeto anterior con el mismo uuid, puedes conservar varias versiones de ese objeto. Entonces, 쯣or qu칠 usar el almacenamiento de objetos? Bueno, Object Storage le permite almacenar archivos de varios formatos de datos sin una estructura de sistema de archivos espec칤fica. Esto elimina la complejidad asociada con las bases de datos y los sistemas de carpetas jer치rquicos. En un entorno de nube, el almacenamiento de objetos puede ampliarse f치cilmente para proporcionar un espacio de almacenamiento pr치cticamente ilimitado para cantidades masivas de datos. En t칠rminos de disponibilidad, los datos del almacenamiento de objetos en la nube suelen replicarse en varias zonas de disponibilidad, lo que significa que los datos se replican en varios centros de datos f칤sicos que est치n aislados unos de otros. Esto hace que los datos sean muy duraderos y est칠n disponibles incluso en el caso de desastres naturales. Por ejemplo, como mencion칠 en el curso anterior, Amazon S3 ofrece una durabilidad de datos de 11 nueves, lo que significa que el almacenamiento de objetos en S3 puede soportar fallos simult치neos de dispositivos o centros de datos. Adem치s, el almacenamiento de objetos suele ser m치s econ칩mico que otras opciones de almacenamiento, especialmente si almacenas datos a los que no necesitas acceder de forma regular. Cloud Object Storage se usa en muchas aplicaciones y es el almacenamiento subyacente para los dise침os de arquitectura m치s nuevos, como lagos de datos y casas de lagos de datos, debido a su flexibilidad, alta escalabilidad, rentabilidad y durabilidad. A continuaci칩n, tendr치 la oportunidad de trabajar con el almacenamiento de tres objetos de Amazon. Crear치 una consulta de datos de tres cubos a partir del dep칩sito y trabajar치 con el control de versiones de objetos. Despu칠s del laboratorio, acomp치침eme en el siguiente v칤deo para ver los registros de las aplicaciones como fuentes de datos del sistema de streaming.

El tipo de sistema de transmisi칩n m치s simple que se me ocurre es un registro. De hecho, el registro ni siquiera es un sistema. Es solo un registro de informaci칩n sobre eventos que puede servir para rastrear la actividad de un sistema o una aplicaci칩n. En el curso anterior, mencion칠 que sol칤a ser com칰n que los desarrolladores consideraran los datos provenientes de las aplicaciones de software como un escape o un subproducto, que no necesariamente ten칤an ning칰n valor intr칤nseco por s칤 mismos, pero eran 칰tiles para monitorear o depurar un sistema. Los datos espec칤ficos que se consideran m치s com칰nmente como datos de escape son los datos contenidos en los registros producidos por las aplicaciones de software. Cuando un desarrollador implementa un producto o una plataforma, como un sitio web o una aplicaci칩n m칩vil, lo configurar치 de manera que toda la actividad que se produzca dentro de la aplicaci칩n se registre en un registro. El registro puede incluir la actividad de un usuario, como un usuario que inicia sesi칩n o navega a una p치gina en particular. Tambi칠n puede incluir un registro de eventos en el back-end, como una actualizaci칩n de una base de datos o un error que se gener칩 al intentar ejecutar un procedimiento en particular. Los registros se utilizan con mayor frecuencia en la pr치ctica como medio para supervisar el estado de los sistemas. Los ingenieros usar치n los registros para activar alertas o para depurar lo que sali칩 mal cuando se produce un error. En este sentido, los troncos pueden parecer aburridos y la caracterizaci칩n de los troncos como gases de escape para aplicaciones puede parecer apropiada. Sin embargo, los registros son una fuente rica de datos que pueden ser 칰tiles para mucho m치s que solo monitorear el estado de una aplicaci칩n. Como tales, pueden ser un sistema fuente importante. Absorber치s datos de tu trabajo como ingeniero de datos. En esencia, un registro es una secuencia de registros que solo se pueden anexar ordenados por tiempo que captura informaci칩n sobre los eventos que ocurren en los sistemas. Por ejemplo, si eres el ingeniero de datos de una empresa de comercio electr칩nico, los registros de tu servidor web pueden capturar datos detallados de la actividad de los usuarios que podr칤an usarse para respaldar el an치lisis posterior de los patrones de comportamiento de los usuarios. Muchos sistemas de bases de datos tendr치n registros que puede usar para rastrear los cambios en el proceso de la base de datos, conocido como captura de datos de cambios o CDC, por sus siglas en ingl칠s. Puede usar esos cambios para activar los procesos de ingesti칩n de modo que se ejecuten en funci칩n de la llegada de nuevos datos a la base de datos. O puede ingerir datos de registro para usarlos en ciertas aplicaciones de aprendizaje autom치tico, como la detecci칩n de anomal칤as, si, por ejemplo, est치 ingiriendo datos de registro de sistemas de seguridad. Los registros desempe침an un papel crucial en el seguimiento de lo que ocurri칩 en muchos de los sistemas de software anteriores con los que trabajar치. Esto los convierte en una fuente de datos rica que puede respaldar casos de uso posteriores, como el an치lisis de datos, la resoluci칩n de problemas, la supervisi칩n del rendimiento , las aplicaciones de aprendizaje autom치tico y la automatizaci칩n. Como mencion칠 anteriormente, el registro es un registro de informaci칩n sobre eventos. En general, los datos que se registran primero para cada evento de un registro son la cuenta personal, de sistema o de servicio asociada al evento, como un identificador de usuario y su direcci칩n IP. A continuaci칩n, encontrar치 un registro del evento que ocurri칩 junto con sus metadatos. Por ejemplo, un usuario agreg칩 un producto espec칤fico a su carrito, junto con el estado de esa acci칩n. Por 칰ltimo, encontrar치s un registro de la marca de tiempo del evento. Los datos de registro se pueden registrar como texto simple no estructurado o en formato JSON o CSV, o incluso como datos codificados binarios. Adem치s de los datos que describen la hora y el contenido de un evento, los registros tambi칠n suelen incluir una etiqueta para clasificar el evento asignando lo que se conoce como nivel de registro a cada registro. Los niveles de registro pueden incluir etiquetas como debug, info, warn , error o fatal que permiten saber qu칠 tipo de informaci칩n contiene un registro determinado. Por ejemplo, a un registro que contenga informaci칩n de actividad b치sica se le asignar칤a el nivel de registro de informaci칩n. Bueno, es posible que a un registro que contenga un mensaje de error se le asigne el nivel de registro de errores o si ocurre algo m치s grave, como que los sistemas principales fallan y necesitan atenci칩n urgente. Esto podr칤a incluir el nivel de registro fatal como etiqueta. Hablaremos m치s sobre los niveles de registro m치s adelante, cuando comience a crear registros en sus propias aplicaciones de canalizaci칩n de datos, en lugar de supervisar sus propios sistemas. Como ingeniero de datos, es importante que comprenda c칩mo trabajar con los registros, sus tipos, formatos y aplicaciones. Los registros ser치n una fuente importante de datos para el trabajo que realice y pueden ayudarlo a solucionar problemas, supervisar el rendimiento y atender muchos casos de uso posteriores. Acomp치침ame en el siguiente v칤deo para ver algunos de los sistemas de streaming.



C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "C2W1-4.txt"  
 echo.
) 
En la lecci칩n anterior, analizamos algunos detalles pr치cticos, algunos sistemas fuente comunes con los que trabajar치 como ingeniero de datos. En los laboratorios, tienes algo de pr치ctica en la manipulaci칩n de datos en bases de datos relacionales y no SQL, as칤 como en el almacenamiento de objetos. Cuando se trata de conectarse realmente a los sistemas de origen y a su trabajo como ingeniero de datos, es relativamente com칰n encontrarse con problemas imprevistos que le impiden acceder a los datos que le interesan. Estos problemas pueden deberse a problemas como tener una administraci칩n de identidad y acceso o definiciones de mensajer칤a instant치nea incorrectas , configuraciones de red incorrectas o incluso tener un conjunto incorrecto de credenciales de acceso. Al principio, estos problemas pueden parecer relativamente triviales, pero seg칰n mi propia experiencia, estos problemas ocurren todo el tiempo en la ingenier칤a de datos y pueden ser un obst치culo importante si no sabes c칩mo depurarlos y resolverlos correctamente. De hecho, creo que resolver problemas como estos es una habilidad fundamental para los ingenieros de datos, por lo que cuando entrevisto a nuevos ingenieros de datos, me gusta utilizar una canalizaci칩n de datos funcional que pueda conectarse a los sistemas de origen previstos y, tal vez, romper algunas de las configuraciones de mensajer칤a instant치nea o de red, y luego pedirle al candidato que resuelva los problemas. Esto me demuestra lo bien preparados que est치n para solucionar estos problemas en el trabajo. En esta lecci칩n, empezar칠 por repasar algunos de los detalles de c칩mo conectarse a los diferentes sistemas fuente. Lo demostrar칠 en AWS. Sin embargo, los principios que analizaremos tambi칠n se aplican a otras plataformas en la nube. En el contexto de las funciones y los permisos de IAM, analizaremos la importancia de la seguridad en la nube, donde la IAM es la clave para controlar y administrar el acceso a las fuentes de datos basadas en la nube y a otros componentes de sus canalizaciones de datos. Por 칰ltimo, nos adentraremos en la creaci칩n de redes. Empezar칠 con una descripci칩n general de alto nivel y, a continuaci칩n, Morgan le explicar치 los detalles de las redes en AWS, incluidas las VPC y las subredes, las puertas de enlace, el enrutamiento, los grupos de seguridad y m치s. Por lo tanto, en esta lecci칩n, se basar치 en su conocimiento de los conceptos b치sicos de redes que analizamos en el curso anterior. Despu칠s de esta lecci칩n, pondr치s a prueba tus habilidades. Al igual que hago con los candidatos de ingenier칤a de datos en el proceso de entrevista, he establecido un desaf칤o para ustedes en el pr칩ximo ejercicio de laboratorio. All칤 encontrar치s una canalizaci칩n de datos que deber칤a resultarte familiar en uno de los laboratorios anteriores, pero ahora la he desglosado. En ese ejercicio de laboratorio, tendr치 una idea de lo que es trabajar como ingeniero de datos al que se le asigna la tarea de conectarse a un sistema fuente en la nube. Te prometo que este no ser치 como el ejercicio est치ndar de Hello World, en el que todo funciona como se espera. En el juego, te ver치s envuelto en un escenario en el que, al igual que en el mundo real, las cosas no funcionan como esperabas. Su trabajo consistir치 en solucionar problemas y averiguar la causa del problema y, a continuaci칩n, resolverlo para conectarse a las fuentes de datos que necesita. Acomp치침eme en el siguiente v칤deo, en el que le proporcionar칠 una descripci칩n general de alto nivel sobre las formas de conectar los sistemas fuente.

Cuando crea una canalizaci칩n de datos en una arquitectura basada en la nube, lo que realmente est치 creando es una red de recursos conectados. Por lo tanto, la forma en que configura esa red desempe침a un papel clave para garantizar que los datos fluyan correctamente a lo largo de su canalizaci칩n de datos. En un curso anterior, analizaste los conceptos b치sicos de las redes y c칩mo las redes son en realidad solo un conjunto de dispositivos conectados que pueden compartir datos y comunicarse entre s칤 y a trav칠s de Internet. Aqu칤 me gustar칤a revisar y ampliar algunos de esos principios b치sicos de redes para prepararlo para los problemas que pueden surgir al intentar conectarse a los sistemas fuente. Cuando se trata de redes en la nube, los principios b치sicos son pr치cticamente los mismos en todos los principales proveedores de nube. Aqu칤 repasar칠 los principios en el contexto de AWS, ya que es con lo que est치 trabajando en estos cursos. Pero sepa que los conceptos subyacentes se aplican a cualquier nube en la que est칠 trabajando. El t칠rmino computaci칩n en nube puede sonar un poco abstracto, como si la computaci칩n estuviera ocurriendo de alguna manera en el 칠ter. Pero no se equivoque, la nube y la computaci칩n en nube se componen de centros de datos f칤sicos muy reales que se encuentran repartidos por todo el mundo. Como aprendi칩 en el curso anterior, la nube de AWS es una red global que se extiende por diferentes 치reas geogr치ficas conocidas como regiones. Cada regi칩n contiene cl칰steres de zonas de disponibilidad y cada zona de disponibilidad consta de uno o m치s centros de datos con alimentaci칩n, redes y conectividad redundantes. En muchas aplicaciones de computaci칩n en nube, los datos y los recursos se replican en todas las regiones y zonas de disponibilidad para garantizar que los sistemas sigan funcionando incluso si uno o m치s centros de datos dejan de funcionar. Como ingeniero de datos que est치 creando nuevos recursos en la nube, tendr치 que decidir en qu칠 regi칩n hospedar sus recursos. Al hacerlo, es posible que tengas que considerar aspectos como el cumplimiento legal. Por ejemplo, 쯔lmacenar sus datos en una regi칩n espec칤fica significa que debe cumplir con requisitos normativos o de privacidad de datos 칰nicos? Es posible que tambi칠n debas tener en cuenta otros factores, como la latencia y la disponibilidad. En general, cuanto m치s cerca est칠n los usuarios finales de la regi칩n en la que est치n alojados los recursos, menor ser치 la latencia. Y mientras m치s zonas de disponibilidad est칠n replicados sus recursos, mejor podr치 resistir un desastre o recuperarse de 칠l. Adem치s de estas consideraciones, el costo de los recursos puede variar de una regi칩n a otra, lo que puede ser un factor a la hora de tomar una decisi칩n. Por eso, cuando realmente trabajas con recursos en la nube, es f치cil perder de vista el hecho de que lo que realmente haces es interactuar con una red de dispositivos f칤sicos que est치n repartidos por todo el mundo. Sin embargo, como ingeniero de datos, es importante entender c칩mo se configura esta infraestructura global y c칩mo afecta a aspectos como la latencia, el costo, la confiabilidad y la disponibilidad de los sistemas que construyes. As칤 que volvamos a las cosas importantes que necesita saber sobre las redes en la nube. Dentro de cualquier regi칩n, puede crear nubes privadas virtuales personalizadas, o VPC, que son redes m치s peque침as que abarcan varias zonas de disponibilidad dentro de una regi칩n. La creaci칩n de VPC le permite tener un control m치s detallado sobre qui칠n puede acceder a qu칠 recursos. De este modo, puede dividir el espacio de su VPC en subredes, o subredes para abreviar, que alojan sus recursos de canalizaci칩n de datos reales. Cada subred puede entonces tener sus propias reglas de seguridad, conocidas como lista de control de acceso a la red o ACL de red para abreviar, as칤 como configuraciones de enrutamiento a trav칠s de una puerta de enlace de Internet. Esto le permite crear subredes p칰blicas para los recursos conectados a Internet, como los servidores web, y subredes privadas para los recursos internos, como las bases de datos. En el mundo real, las cosas pueden complicarse muy r치pidamente, especialmente cuando se comienzan a configurar varias VPC, subredes, puertas de enlace y configuraciones de enrutamiento entre recursos. Y es en este contexto en el que el simple hecho de conectarse a una base de datos depende de varias capas de configuraciones de red, sin mencionar los permisos de IAM. Por lo tanto, comprender los detalles de la configuraci칩n de la red es fundamental a la hora de conectarse a los sistemas de origen. Tambi칠n es necesario para la orquestaci칩n y automatizaci칩n exitosas de sus canalizaciones de datos, algo que abordaremos en la 칰ltima semana de este curso. A continuaci칩n, Morgan le explicar치 los detalles de las redes en AWS. Y despu칠s de eso, le mostrar칠 lo que puede esperar en el pr칩ximo laboratorio, donde depurar치 su conexi칩n a una base de datos.



C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "C2W1-5.txt"  
 echo.
) 
Excelente trabajo, he llegado al final de la primera semana de este curso sobre sistemas de origen, ingesti칩n y canalizaciones de datos. Espero que se sienta m치s c칩modo y con m치s conocimientos en lo que respecta a la primera etapa del ciclo de vida de la ingenier칤a de datos, en la que los datos se generan en los sistemas de origen. Entonces, est치 creando un nuevo sistema de datos desde cero o actualizando y manteniendo una canalizaci칩n de datos existente. Deber치 comprender c칩mo funcionan los sistemas fuente. Esta semana, comenzamos analizando los detalles de algunos sistemas fuente comunes, como las bases de datos relacionales y NoSQL, el almacenamiento de objetos para archivos y los sistemas de registro y transmisi칩n de eventos. Tambi칠n analizamos algunas formas de conectarse a estas fuentes de datos, espec칤ficamente en una arquitectura basada en la nube. Analizamos c칩mo las redes permiten conectarse a los sistemas de origen y la importancia de la IAM para garantizar la seguridad de los sistemas de origen y del resto de la canalizaci칩n de datos. A lo largo de la semana, destaqu칠 algunos problemas comunes que pueden surgir al trabajar con los sistemas fuente, y tuviste la oportunidad de solucionar algunos de estos problemas comunes en el laboratorio. La semana que viene, nos centraremos en las diferentes formas de ingerir datos de los sistemas de origen. Profundizaremos en los detalles entre la ingesta por lotes y la ingesta por streaming, y tambi칠n analizaremos los diferentes factores que debes tener en cuenta al dise침ar tu arquitectura de ingesta. Nos vemos all칤.



C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "C2W1-6.txt"  
 echo.
) 
En el primer curso de la especializaci칩n, recopilamos algunos requisitos bas치ndonos en las conversaciones que mantuvimos con varias partes interesadas. Incluyendo a un cient칤fico de datos, el director de datos, un gerente de marketing de productos y un ingeniero de software en una empresa de comercio electr칩nico ficticia. A trav칠s de estas conversaciones, el director de datos le dijo que la empresa tiene como objetivo expandirse a nuevos mercados y aumentar la retenci칩n de los clientes existentes. Adem치s, trabaj칩 con estas partes interesadas para configurar una canalizaci칩n de datos para un sistema de recomendaci칩n. Aqu칤 nos basaremos en ese conjunto de conversaciones y hablaremos con un analista de marketing al que se le ha encomendado la tarea de buscar informaci칩n y tendencias de venta de productos. Por eso, en este v칤deo, interpretar칠 a la ingeniera de datos y mi amiga Colleen interpretar치 a la analista de marketing. Vamos a entrar. Colleen n칰mero cuatro, me alegro de conocerte. >> Tambi칠n me alegro de conocerte. >> Hola, soy Joe, soy un nuevo ingeniero de datos y estoy deseando saber m치s sobre lo que est치is haciendo. >> S칤, por supuesto, estoy muy emocionada de trabajar contigo. As칤 que estoy trabajando en tratar de entender qu칠 tipo de factores externos podr칤an ser se침ales de que podr칤amos conectar con los h치bitos de compra de los clientes , por lo que el equipo de marketing ha estado haciendo una lluvia de ideas sobre qu칠 tipo de cosas podr칤an ser y se nos han ocurrido algunas ideas que nos gustar칤a explorar m치s a fondo. >> Eso suena genial, 쯣or qu칠 no me cuentas m치s sobre eso? >> Claro que s칤. As칤 que pensamos que, en t칠rminos generales, la forma en que alguien se siente, feliz o triste, emocionada o relajada, podr칤a afectar su comportamiento cuando se trata de comprar en l칤nea. Por supuesto, no tenemos forma de saber exactamente c칩mo se sienten nuestros clientes en un d칤a determinado, pero pensamos que podr칤amos explorar algunas ideas. En particular, nos gustar칤a ver qu칠 tipo de m칰sica escucha la gente en las distintas regiones en las que vendemos nuestros productos y, a continuaci칩n, compararlos con las ventas de productos. Ya veo. As칤 que est치s pensando que te gustar칤a obtener datos p칰blicos de algunas fuentes externas para obtener informaci칩n sobre la m칰sica que escucha la gente. >> S칤, exactamente. Lo he estado investigando y parece que Spotify tiene una API p칰blica en la que podr칤amos obtener datos sobre los artistas musicales que est치n de moda en las diferentes regiones y las tendencias de escucha de las personas a lo largo del tiempo. 쯉uena como algo con lo que podr칤as ayudarnos? >> Seguro que soy un gran fan de Nickelback, as칤 que definitivamente me gusta mucho la m칰sica. >> Bonito, debe serlo. >> S칤, s칤, as칤 que tendr칠 que echar un vistazo m치s de cerca a la API de Spotify. Y una vez que averig칲e los detalles, tal vez podamos hablar exactamente de qu칠 tipo de informaci칩n le gustar칤a obtener y c칩mo le gustar칤a que se proporcionara. >> Vale, s칤, suena fant치stico. Mientras tanto, av칤same si hay algo en lo que pueda ayudarte y espero poder hablar m치s contigo una vez que tengamos los detalles resueltos. Muy bien, eso es genial, gracias. Muy bien, ese fue un ejemplo de una conversaci칩n con un analista de marketing en la que describe sus necesidades de datos para un proyecto. Conc칠ntrese en extraer los datos de una API p칰blica que les gustar칤a analizar junto con los datos de ventas de productos. Ahora, reconozco, y tal vez t칰 tambi칠n lo est칠s pensando, que estudiar las tendencias regionales sobre el tipo de m칰sica que escucha la gente puede no parecer un enfoque de marketing particularmente brillante. Y probablemente tengas raz칩n, pero cr칠eme, he visto todo tipo de cosas locas en lo que respecta al tipo de datos que las diferentes partes interesadas quieren tener en sus manos. Por lo tanto, el punto aqu칤 no es insistir en si esto parece un esfuerzo digno, sino identificar los requisitos clave del sistema que necesitar치 construir. En este caso, no cabe duda de que se necesita m치s informaci칩n para saber exactamente cu치l ser치 el mejor enfoque a la hora de crear toda la canalizaci칩n de datos para este proyecto. Pero por el momento, nos vamos a centrar en la parte de la ingesti칩n. Lo m치s importante que aprender치s aqu칤 es que vas a necesitar ingerir datos de una API de terceros. Con el tiempo, tendr치 que considerar otros detalles, como la forma en que, en 칰ltima instancia, almacenar치 y entregar치 los datos al analista, y esto depender치 de lo que necesite el analista. En general, al ingerir datos de una API, te enfrentar치s a alg칰n tipo de proceso de ingesta por lotes, pero su aspecto exacto depender치 de lo que pretendas hacer con los datos. En el siguiente v칤deo, analizaremos m치s de cerca las ventajas y desventajas entre los populares paradigmas de procesamiento de datos por lotes de extracci칩n, transformaci칩n, carga o ETL y extracci칩n, carga, transformaci칩n o ELT en relaci칩n con la ingesta de datos. Y despu칠s de eso, analizaremos la conexi칩n y la ingesta de datos de una API REST. Nos vemos en el siguiente v칤deo.

En el v칤deo anterior, nuestros analistas de marketing compartieron sus objetivos para un proyecto en el que est치n trabajando para incorporar algunos datos externos en su an치lisis de las ventas de productos. Para este proyecto, parece que los analistas se centrar치n principalmente en las tendencias hist칩ricas de los datos y es posible que en el futuro quieran pasar a un an치lisis m치s expl칤cito de los datos actuales, pero no necesariamente en tiempo real o en un sentido urgente. Adem치s, sabe que extraer치 los datos de una API de terceros. Si bien, por lo general, tendr치 cierta flexibilidad en cuanto a la frecuencia o la cantidad de datos que extraiga, estar치 limitado a alg칰n tipo de ingesta por lotes. Esto se debe a que las llamadas a la API funcionan de manera similar a las solicitudes web, en las que env칤as una solicitud de datos y recibes una respuesta, y la cantidad de solicitudes que puedes realizar por vez suele ser limitada. En t칠rminos de ingesta de datos para este proyecto, se trata de un proceso por lotes. En el curso anterior, present칠 brevemente ETL y ELT, que son las siglas de extract transform load y extract load transform respectivamente. Se trata de dos patrones de ingesta por lotes muy comunes y, si bien t칠cnicamente incluyen componentes de las etapas de transformaci칩n y almacenamiento del ciclo de vida de la ingenier칤a de datos , en la pr치ctica, tendr치 que pensar en las ventajas y desventajas entre estos patrones y la etapa de ingesta. Eso es lo que me gustar칤a hacer ahora mismo. En primer lugar, hablar칠 un poco m치s sobre lo que distingue a estos dos procesos y , a continuaci칩n, analizaremos cu치l podr칤a ser el m치s adecuado para el caso de uso de nuestros analistas de marketing. La carga de transformaci칩n de extractos o ETL es en realidad el patr칩n original de ingesti칩n de lotes que gan칩 popularidad en las d칠cadas de 1980 y 1990. El proceso comienza con la extracci칩n de datos sin procesar de un sistema de tienda, lo que se puede hacer consultando directamente una base de datos o utilizando una API, por ejemplo. A continuaci칩n, transforma los datos en un 치rea de preparaci칩n intermedia. A continuaci칩n, carga los datos en un destino de almacenamiento de destino, como una base de datos o un almac칠n de datos. En los a침os 80 y 90, la capacidad de almacenamiento y computaci칩n era extremadamente limitada, por lo que era importante tener un plan para determinar exactamente qu칠 datos quer칤as ingerir y c칩mo quer칤as almacenar esos datos y acceder a ellos, en qu칠 formato, etc. Los almacenes de datos eran costosos de configurar y no eran adecuados para ejecutar consultas pesadas que inclu칤an combinaciones y transformaciones complejas. En aquellos d칤as, no hab칤a m치s remedio que ser muy intencional a la hora de transformar los datos sin procesar durante el proceso de ingesti칩n para garantizar que pudieran almacenarse y ponerse a disposici칩n de manera eficiente. El ETL sigue siendo muy popular hoy en d칤a como patr칩n de ingesti칩n. Pero ahora, con el costo relativamente bajo del almacenamiento en la nube y el aumento de la potencia computacional, ya no es la 칰nica opci칩n. A principios de la d칠cada de 2010, los sistemas de almacenamiento en la nube se volvieron altamente escalables y vimos la aparici칩n de lagos de datos basados en sistemas de almacenamiento de objetos como S3 y almacenes de datos en la nube, como Redshift y Snowflake. Esto hizo posible almacenar enormes cantidades de datos de forma relativamente econ칩mica y realizar todas las transformaciones de datos directamente en su almac칠n de datos. Fue entonces cuando surgi칩 el concepto de ELT o transformaci칩n de carga de extracci칩n. En el proceso ELT, se extraen los datos sin procesar de los sistemas de origen y se cargan directamente en la base de datos o el almac칠n de datos de destino o incluso en el almacenamiento 칩ptico sin realizar ninguna transformaci칩n. La interesante idea del ELT es que no es necesario que decidas por adelantado c칩mo quieres usar tus datos. Esto puede resultar atractivo porque, en cierto sentido, se podr칤a decir que al aplicar transformaciones a los datos sin procesar y almacenar solo los resultados del proceso como se hace con ETL, se pierde parte de la informaci칩n en el proceso. Sin embargo, con ELT, todas las opciones permanecen sobre la mesa, ya que solo tiene que capturar todos los datos y guardarlos para usarlos m치s adelante, y luego puede consultar y transformar los datos sin procesar de la manera que desee, sin que se pierda ninguna informaci칩n. Ahora, por muy atractivo que pueda parecer este paradigma, para ser honesto, cuando escuch칠 por primera vez sobre la idea del ELT, pens칠 que era una idea terrible. 쯇or qu칠 pens칠 que querr칤as acumular un mont칩n de datos sin procesar y almacenamiento sin pensar profundamente en c칩mo quieres usarlos? Como he estado enfatizando a lo largo de estos cursos, el primer paso en cualquier proyecto de ingenier칤a de datos debe ser establecer con firmeza cu치les son sus objetivos finales y, solo entonces, pensar en c칩mo construir un sistema para lograr esos objetivos. Sin embargo, con el tiempo, empec칠 a ver los beneficios potenciales del ELT. Por un lado, el ELT es m치s r치pido de implementar porque no requiere una planificaci칩n detallada y anticipada sobre c칩mo desea transformar exactamente sus datos. Tambi칠n es posible hacer que los datos est칠n disponibles m치s r치pidamente para los usuarios finales, aunque est칠n sin procesar, porque el ELT elimina la necesidad de un servidor provisional y de transformaciones de datos en curso. Con la potencia de procesamiento del almac칠n de datos moderno, las transformaciones a칰n se pueden realizar de manera eficiente una vez que los datos se cargan en el almacenamiento. M치s all치 de eso, como dije antes, cuando desee almacenar todos sus datos sin procesar, puede configurarlos m치s adelante para adoptar diferentes transformaciones o analizar los datos de una manera diferente que podr칤a haber sido posible si solo almacenara los datos de transformaci칩n en primer lugar. 쮺u치l es la desventaja del ELT? En resumen, si no tiene cuidado, su canalizaci칩n puede convertirse simplemente en una canalizaci칩n EL, en la que puede extraer y cargar enormes cantidades de datos sin procesar en el almacenamiento sin tener que averiguar c칩mo transformarlos en algo 칰til. Si no quiere dedicar tiempo por adelantado a planificar c칩mo va a usar sus datos, podr칤a terminar con lo que com칰nmente se conoce como un pantano de datos, que es una situaci칩n en la que los datos se vuelven desorganizados, inadministrables y pr치cticamente in칰tiles. Me gusta mostrar esta imagen cuando surge el tema de los pantanos de datos. Aqu칤 tenemos a un ingeniero de datos sentado en su pantano de datos, o se ha quedado con absolutamente todo lo que pensaba que podr칤a tener alg칰n valor alg칰n d칤a. Pero ahora, por supuesto, incluso si pudiera recordar todo lo que hab칤a all칤, probablemente ni siquiera ser칤a capaz de encontrarlo. A principios de la d칠cada de 2010, los pantanos de datos eran comunes, ya que las empresas descubrieron que era posible conservar literalmente cada fragmento de datos sin procesar por si acaso. Hoy en d칤a, gran parte de esto se ha solucionado debido en parte a las regulaciones que exigen que las empresas almacenen los datos de tal manera que puedan auditarse o eliminarse de manera ordenada; por ejemplo, un usuario solicita que sus datos se eliminen de los sistemas de la empresa. Dicho esto, el costo relativamente bajo del almacenamiento actual, combinado con la potencia de procesamiento de los almacenes de datos modernos y otras abstracciones de almacenamiento, significa que tanto ETL como ELT pueden ser enfoques razonables para el procesamiento por lotes. Sin embargo, sea cual sea el enfoque que adopte, es importante tener un conjunto y objetivos claros en mente y administrar los datos en consecuencia. Pensemos en la conversaci칩n con el analista de marketing. Para este proyecto, ingerir치s datos de una API de terceros. La mayor칤a de las veces, los datos que recibir치 a trav칠s de una conexi칩n de API ser치n datos semiestructurados, tal vez en formato JSON. En algunos casos, es posible que tambi칠n est칠s recuperando datos no estructurados, como textos e im치genes. En este caso, parece que el analista de marketing pretende realizar un an치lisis exploratorio de los datos y no podr칤a decir por adelantado exactamente qu칠 transformaciones podr칤an ser necesarias. Probablemente, un oleoducto ELT sea la elecci칩n correcta para este escenario de ingesti칩n, ya que brinda m치s flexibilidad en las etapas de transformaci칩n y navegaci칩n de este proyecto. El componente principal de este caso de uso de ingesti칩n del que a칰n no hemos hablado en detalle es una parte sobre la ingesta de datos de una API. Ah칤 es hacia donde nos dirigimos ahora. Acomp치침eme en el siguiente v칤deo para ver c칩mo trabajar치 con una API como fuente de datos.

 

	

ETL
	

ELT

Historia
	

- En los a침os 80 y 90, el coste de los almacenes de datos era muy elevado (millones de d칩lares), por lo que los ingenieros quer칤an ser muy cuidadosos con los datos que iban a cargar en el almac칠n de datos

- El volumen de datos a칰n era manejable.
	

- El almac칠n de datos en la nube redujo significativamente el coste de almacenamiento y procesamiento de datos (de millones de d칩lares a s칩lo cientos/miles de d칩lares)

- Volumen de datos disparado.

Procesamiento (transformaci칩n)
	

- Los datos se transforman en un formato predeterminado antes de cargarse en un repositorio de datos. As칤, los ingenieros de datos tienen que modelar cuidadosamente los datos y transformarlos a este formato.

- Las transformaciones dependen de la capacidad de procesamiento de la herramienta de procesamiento que se utiliza para ingerir los datos (sin relaci칩n con el destino de destino)
	

- Los datos en bruto se cargan en el destino final. A continuaci칩n, se transforman justo antes del an치lisis (pueden utilizarse con solicitudes de datos no bien definidas)

- Las transformaciones dependen de la capacidad de procesamiento del repositorio de datos, como el almac칠n de datos.

Tiempo de mantenimiento 
	

Si la transformaci칩n resulta inadecuada, es necesario volver a cargar los datos.
	

Los datos originales est치n intactos y ya cargados y pueden utilizarse cuando sea necesario para una transformaci칩n adicional: Se requiere menos tiempo para el mantenimiento de los datos.

Tiempo de carga y tiempo de transformaci칩n
	

Tiempo de carga: suele llevar m치s tiempo, ya que utiliza un 치rea y un sistema de puesta en escena.

Tiempo de transformaci칩n: depende del tama침o de los datos, de la complejidad de la transformaci칩n y de la herramienta que se utilice para realizarla.

	

Tiempo de carga: no hay transformaci칩n, los datos se cargan directamente en el sistema de destino

Tiempo de transformaci칩n: suele ser m치s r치pido porque se basa en la potencia de procesamiento y la paralelizaci칩n de los almacenes de datos modernos

(generalmente se considera m치s eficiente)

Flexibilidad (tipos de datos)
	

Los ETL suelen estar dise침ados para manejar datos estructurados.
	

Los ETL pueden manejar todo tipo de datos: estructurados, no estructurados, semiestructurados. Una vez cargados los datos en el sistema de destino, puede transformarlos.

Coste
	

Depende de la herramienta ETL/ELT que se utilice y del sistema de destino al que se carguen los datos. (Y, por supuesto, depende del volumen de datos).
	

Depende de la herramienta ETL/ELT que se utilice y del sistema de destino al que se carguen los datos. (Y, por supuesto, depende del volumen de datos).

Escalabilidad
	

Hoy en d칤a, la mayor칤a de las herramientas en la nube son escalables. Sin embargo, el reto aqu칤 es que si tiene muchas fuentes de datos y muchos objetivos, tendr치 que hacer un gran esfuerzo para gestionar el c칩digo y manejar los datos de m칰ltiples fuentes
	

ELT utiliza la potencia de procesamiento escalable del almac칠n de datos para permitir la transformaci칩n a gran escala.

Calidad/seguridad de los datos
	

Garantiza la calidad de los datos limpi치ndolos previamente. Las transformaciones tambi칠n pueden incluir el enmascaramiento de informaci칩n personal.
	

Los datos deben transferirse primero al sistema de destino antes de aplicar transformaciones que mejoren la calidad o la seguridad de los datos.

*Existe un subpatr칩n denominado EtLT, en el que t peque침o no se refiere al modelado de negocio, sino a la transformaci칩n con alcance limitado (enmascarar datos sensibles, deduplicar filas).

En el curso anterior, mencion칠 el llamado mandato de API que lleg칩 en forma de correo electr칩nico de Jeff Bezos a todos los empleados de Amazon en 2002. La esencia de este correo electr칩nico era que, de ahora en adelante, todos los equipos deber치n utilizar interfaces de servicio, tambi칠n conocidas como interfaces de programaci칩n de aplicaciones o API, para comunicarse, as칤 como para ofrecer datos y funciones. El problema que pretend칤a resolver era que, antes de esa fecha, los equipos de Amazon y de cualquier otra organizaci칩n no dispon칤an de una forma uniforme o estable de intercambiar datos y servicios, lo que generaba ineficiencias. Al configurar las API como una interfaz de servicio estable y predecible entre los diferentes equipos, cualquier equipo individual pod칤a proporcionar datos , funciones y comunicaciones a otros equipos, sin importar el tipo de l칤o complicado que pudieran tener los equipos en sus propios sistemas. La otra parte del mandato de la API consist칤a en que todas estas interfaces de servicio o API ten칤an que crearse desde cero para que, finalmente, fueran p칰blicas para los desarrolladores del mundo exterior. Esta reorientaci칩n hacia las interfaces de servicio sent칩 las bases de lo que eventualmente se convertir칤a en Amazon Web Services y marc칩 la direcci칩n de c칩mo las empresas de todo el mundo compartir칤an datos y servicios, tanto interna como externamente. B치sicamente, una API es un conjunto de reglas y especificaciones que le permite comunicarse e intercambiar datos mediante programaci칩n con una aplicaci칩n. Por comunicarse mediante programaci칩n, me refiero a comunicarse mediante la ejecuci칩n de c칩digo. Si ha desarrollado software, es posible que est칠 familiarizado con la conexi칩n a las API. Pero aunque no hayas configurado t칰 mismo las conexiones de API, no cabe duda de que est치s utilizando las API directamente a diario cuando buscas cosas en l칤nea o utilizas las aplicaciones de tu tel칠fono. Esto se debe a que, en la actualidad, las API est치n integradas en la funcionalidad de una amplia gama de aplicaciones de software. Por ejemplo, las aplicaciones de redes sociales utilizan API para obtener y mostrar datos de los servidores web a los usuarios finales. Las API tambi칠n se utilizan para facilitar las transacciones entre sitios web de comercio electr칩nico y sistemas de pago. Muchas empresas ofrecen API p칰blicas para que usted, como desarrollador, pueda acceder a sus datos y servicios e integrarlos en sus propias aplicaciones. Como ingeniero de datos, utilizar치 las API para conectarse y extraer datos de varias fuentes, como servicios web, plataformas en la nube o proveedores externos, enviando solicitudes y recibiendo respuestas en un formato estandarizado. Las API tambi칠n pueden proporcionar funciones de metadatos, documentaci칩n, autenticaci칩n y gesti칩n de errores para facilitar la extracci칩n de datos. El tipo de API m치s com칰n es lo que se conoce como API REST, o REST son las siglas de Representational State Transfer. Las API REST suelen utilizar el Protocolo de transferencia de hipertexto o lo que quiz치s conozcas m치s habitualmente como m칠todos HTTP como base para la comunicaci칩n. Puede pensar que la interacci칩n con las API REST es similar a lo que hace cuando navega por Internet. Al hacer clic en un enlace de su navegador, env칤a una solicitud HTTP a un servidor para un recurso espec칤fico, como una p치gina web, y el servidor responde proporcionando ese recurso. Con una API REST, tambi칠n env칤as una solicitud HTTP para un recurso en particular y la API est치 configurada para responder en funci칩n del contenido de tu solicitud. En la conversaci칩n, tenemos a los analistas de marketing. Nos enteramos de que les gustar칤a analizar los datos que est치n almacenados en una plataforma de terceros, Spotify en este caso, y que est치n disponibles a trav칠s de una API. Este es un escenario muy com칰n con el que se encontrar치 como ingeniero de datos, en el que se puede acceder a trav칠s de una API al sistema de origen del que necesita extraer datos, ya sea un sistema interno o un sistema externo de terceros. Pero la mejor manera de familiarizarse con el funcionamiento de esto es empezar y hacerlo usted mismo, y eso es lo que har치 en el pr칩ximo laboratorio.

En el pr칩ximo laboratorio, practicar치s c칩mo interactuar con la API de Spotify para extraer datos, explorar qu칠 significa la paginaci칩n y aprender a enviar una solicitud de API que requiere autorizaci칩n. En este v칤deo, analizar칠 primero algunos conceptos de API con los que trabajar치s en el laboratorio y, a continuaci칩n, te proporcionar칠 una descripci칩n general de las tareas del laboratorio. Para los ejercicios de laboratorio, necesitar치s tener una cuenta de Spotify para obtener las credenciales que necesitas para extraer datos de la API. Esto se debe a que cualquier solicitud que env칤es a la API de Spotify requiere autorizaci칩n y, para obtener esa autorizaci칩n, necesitas una cuenta. Ahora, que quede claro, no quiero que pienses que estoy promocionando Spotify aqu칤 o que te estoy pidiendo que abras una cuenta sin ning칰n motivo. De hecho, cuando trabajas con API de terceros como ingeniero de datos, es muy com칰n que tengas que registrar una cuenta en esa plataforma de terceros para poder usar la API. Por lo tanto, el prop칩sito de este laboratorio es que sigas ese flujo de trabajo de la misma manera que lo experimentar칤as en el trabajo. Y, por supuesto, si lo deseas, puedes cancelar tu cuenta de Spotify inmediatamente despu칠s de completar el laboratorio. Pero por ahora, si a칰n no tienes una, te animo a que crees una cuenta de Spotify y consultes la documentaci칩n de la API de Spotify para que puedas seguir lo que te voy a mostrar aqu칤. La API web de Spotify es una API RESTful a la que puedes enviar solicitudes para acceder a artistas musicales, 치lbumes y pistas directamente desde el cat치logo de datos de Spotify. Cada elemento de datos espec칤fico, como una lista de reproducci칩n, un artista o un 치lbum, se denomina recurso al que puede acceder enviando una solicitud HTTP al punto final que representa ese recurso. Hay diferentes tipos de solicitudes HTTP, pero las m치s comunes son GET, PUT, POST y DELETE. GET le permite recuperar un recurso, POST le permite crear un recurso, PUT le permite cambiar o reemplazar recursos y DELETE le permite eliminar recursos. Por ejemplo, en la documentaci칩n de la API de Spotify, si haces clic en 츼lbumes, puedes ver todas las solicitudes que puedes usar para interactuar con este recurso. Por ejemplo, puedes realizar una solicitud GET para obtener informaci칩n del cat치logo de Spotify sobre las canciones de un 치lbum usando este punto final. Tambi칠n puedes usar una solicitud GET para recuperar una lista de los lanzamientos de nuevos 치lbumes que aparecen en Spotify usando este punto final. Si la solicitud se realiza correctamente, devuelve una respuesta en formato JSON que contiene informaci칩n sobre el recurso solicitado. Por ejemplo, este es un ejemplo de respuesta a una solicitud GET de canciones de un 치lbum y otro ejemplo de respuesta a una solicitud GET de nuevos lanzamientos. Si la solicitud no se realiza correctamente, devuelve un objeto de error que contiene un c칩digo de estado que explica por qu칠 la solicitud no se ha realizado correctamente. Por ejemplo, un c칩digo de 400 significa una solicitud incorrecta que podr칤a deberse a una sintaxis mal formada, y un c칩digo de 404 significa que no se pudo encontrar el recurso solicitado. Puede encontrar en la documentaci칩n aqu칤 una lista de los c칩digos de estado y el significado de cada uno. Cuando realizas una solicitud a la API web de Spotify, debes especificar el punto final del recurso, as칤 como un token de acceso. El token de acceso es una cadena que contiene las credenciales y los permisos que se utilizan para acceder a un recurso determinado. Para obtener el token de acceso, primero tendr치s que crear una cuenta de Spotify y, desde tu cuenta, podr치s obtener un ID de cliente y un secreto de cliente que podr치s usar en el proceso de autorizaci칩n y la generaci칩n del token de acceso. El token de acceso es v치lido durante una hora. Transcurrido ese tiempo, el token caduca y tendr치s que solicitar uno nuevo. En el laboratorio, se te proporcionar치 un c칩digo que puedes usar para solicitar un token de acceso con tu ID de cliente y el secreto del cliente. Para obtener m치s informaci칩n sobre el proceso de autorizaci칩n con Spotify, puedes consultar la documentaci칩n. Ten en cuenta que es posible que trabajes con otras API que no requieran autorizaci칩n o que el proceso de autorizaci칩n sea diferente al de Spotify, por lo que te sugiero que consultes siempre la documentaci칩n de la API con la que vas a trabajar. Antes de entrar en m치s detalles sobre el laboratorio, no dudes en pausar el v칤deo para consultar la documentaci칩n de Spotify, as칤 como los dos ejemplos de solicitud de API, Get Album Tracks y Get New Releases. Antes de empezar a interactuar con la API de Spotify, debes crear una cuenta para obtener las claves que utilizar치s para generar los tokens de acceso. As칤 que primero aseg칰rate de registrarte y completar la informaci칩n requerida para crear tu cuenta. As칤 que aqu칤 ya he iniciado sesi칩n en mi cuenta. Har칠 clic en el nombre de la cuenta en la esquina superior derecha y, a continuaci칩n, en el panel de control y, a continuaci칩n, en Crear aplicaci칩n. En Nombre de la aplicaci칩n, escribir칠 este nombre. En Descripci칩n de la aplicaci칩n, escribir칠 Spotify App para probar la API. Para los URI de redireccionamiento, especificar칠 este host local. Y aqu칤 voy a elegir Web API. Por 칰ltimo, har칠 clic en Guardar. Si recibes el error de que tu cuenta no est치 lista, puedes cerrar sesi칩n y esperar unos minutos y, a continuaci칩n, volver a iniciar sesi칩n y repetir los pasos. Una vez creada la aplicaci칩n, puede hacer clic en ella para ir a la p치gina de inicio de la aplicaci칩n. Aqu칤 har칠 clic en la configuraci칩n para encontrar el ID de cliente y el secreto del cliente. Tendr치 que copiar estos valores para usarlos en el laboratorio. Una vez que inicie el laboratorio y siga las instrucciones de configuraci칩n del laboratorio, estar치 en este Jupyter Notebook. Aqu칤, en la carpeta SRC o Source, har칠 clic en el archivo env y, a continuaci칩n, pegar칠 el ID de cliente y el secreto del cliente para almacenarlos en estas variables de entorno. Volviendo al Jupyter Notebook, primero ejecutar칠 la celda para importar los paquetes necesarios, luego esta celda para cargar las variables de entorno y, a continuaci칩n, estas dos variables para representar el ID de cliente y el secreto del cliente. Junto a obtener un token de acceso, puedes encontrar en esta celda la funci칩n getToken, que espera que tu ID de cliente y tu secreto de cliente generen el token de acceso. Utilic칠 la documentaci칩n de la API para saber qu칠 detalles deben incluir esta funci칩n. No dudes en consultar este enlace para obtener m치s informaci칩n sobre estos detalles. Ahora llamar칠 a la funci칩n getToken, que devuelve esta respuesta de token. Es un diccionario que contiene tres claves: accessToken, el tipo de token y ExpiresIn. Usar치s la cadena, que es el valor de la clave accessToken, en todas las llamadas a la API de este laboratorio. As칤 que echemos un vistazo a algunos de los ejemplos de llamadas a la API. Para crear tus llamadas a la API en Python, puedes usar el paquete requests, que ya he importado a este cuaderno. Este paquete es una biblioteca popular que proporciona una forma sencilla y f치cil de usar de interactuar con las API. Por ejemplo, para realizar una solicitud GET en la API de Spotify, puedes llamar al m칠todo requests.get. Deber치s introducir el punto final del recurso al que quieres acceder y especificar el accessToken mediante los encabezados de los par치metros asign치ndolo a un diccionario en este formato. En el laboratorio, se le proporciona esta funci칩n que crea autom치ticamente el encabezado de autorizaci칩n dado el accessToken. As칤 que aqu칤, en esta solicitud GET, usar칠 la funci칩n getAuthHeader proporcionada y la asignar칠 al par치metro headers. Para esta funci칩n, usar칠 la respuesta del token para pasar el accessToken. Tenga en cuenta que el segundo GET no es la solicitud HTTP GET. Es el m칠todo que se aplica en un diccionario de Python para extraer el valor correspondiente a una clave. Obtengamos ahora el punto final de getNewReleases y veamos la respuesta devuelta. La respuesta es un objeto de respuesta de Spotify que puedes convertir en un diccionario de Python usando el m칠todo.json, tal y como se muestra aqu칤. Esta es la respuesta. Puedes ver que hay una tecla, 츼lbumes. Y para los 치lbumes, hay otro conjunto de pares de valores clave. Para verificar las claves de cada uno, llame al m칠todo .keys. As칤 que la respuesta en realidad contiene una clave, 츼lbumes. Vamos a comprobar la clave de 츼lbumes. Estas son las claves de los 치lbumes. Repasemos cada una de ellas. href contiene un enlace o el punto final al recurso. Puede ver que se agregaron dos par치metros al punto final, getNewReleases, que son offset y limit. La solicitud getNewReleases espera estos dos par치metros. Por lo tanto, de forma predeterminada, se agregaron un desplazamiento de 0 y un l칤mite de 20 al punto final, lo que significa que la respuesta contiene la informaci칩n de los primeros 20 치lbumes. Puedes encontrar los detalles de los 치lbumes usando la tecla Items. Tambi칠n puede comprobar los valores utilizados para el desfase y el l칤mite y obtener el n칰mero total de elementos, que es 100. Por 칰ltimo, next contiene el mismo punto final pero con valores diferentes para el desfase y el l칤mite. En este caso, se refiere a los 20 elementos siguientes. Por lo tanto, si desea especificar su propio desfase y l칤mite, puede hacerlo mediante este punto final. Aqu칤 eleg칤 40 para offset40 y 20 para l칤mite. En el laboratorio, se te asigna esta funci칩n que realiza los getRequests para los lanzamientos de los nuevos 치lbumes. Toma el offset y el l칤mite como par치metros y los usa para construir el punto final de la getRequest. O puede usar el punto final completo especificado en el 칰ltimo par치metro cuando no est칠 vac칤o. Su tarea consistir치 en completar esta funci칩n. A continuaci칩n, tendr치s que realizar la paginaci칩n para extraer la lista completa de lanzamientos de nuevos 치lbumes. En lugar de extraer los 100 elementos de una llamada a GetRequest, puedes realizar la paginaci칩n para extraer los elementos fragmento por fragmento. Por lo tanto, puede comenzar con la primera llamada a la API, comenzando con una compensaci칩n de 0 y eligiendo el l칤mite que desee. Y luego puedes seguir repitiendo la misma llamada, especificando una compensaci칩n diferente cada vez para seguir leyendo desde donde la dejaste en la llamada a la API anterior. O puedes usar el punto final proporcionado en el siguiente campo de la respuesta devuelta. Por lo tanto, en el laboratorio, se le proporcionan estas dos funciones. El primero realiza la paginaci칩n cambiando manualmente el desplazamiento para cada nueva llamada a la API. Y el segundo usa el siguiente campo proporcionado por la respuesta de la llamada a la API actual. As칤 que tendr치s la tarea de completar estas dos funciones. En la segunda parte del laboratorio, se le proporcionan estas funciones de Python que deber치 completar. Estas funciones te permitir치n crear un proceso de ingesta por lotes que extraiga la informaci칩n del cat치logo de Spotify para cada uno de los nuevos lanzamientos de 치lbumes. Por lo tanto, en esta parte, tendr치s que realizar dos llamadas a la API paginadas. En la primera, utilizar치s la lista de lanzamientos de nuevos 치lbumes con la misma llamada paginada que utilizaste en la primera parte del laboratorio. En la segunda, obtendr치s la informaci칩n del cat치logo de un 치lbum determinado mediante el punto final getAlbumTracks. El archivo Python de autenticaci칩n contiene el script de la funci칩n getToken que devuelve un token de acceso. El archivo de punto final contiene dos funciones. La primera corresponde a la llamada paginada al punto final, getNewAlbums. Y la segunda corresponde a la llamada paginada al punto final, getAlbumTracks. Deber치 completar estas funciones. Tambi칠n tendr치s que completar un fragmento de c칩digo que genere autom치ticamente un nuevo token cuando caduque. Y, por 칰ltimo, en la funci칩n principal, llamar치s a la primera llamada a la API paginada para obtener los ID de los lanzamientos de los nuevos 치lbumes. Luego, para cada ID de 치lbum, llamar치s a la segunda llamada a la API paginada para extraer la informaci칩n de la pista de cada ID de 치lbum. Deber치s completar parte de este c칩digo. Por lo tanto, aseg칰rese de leer las instrucciones detenidamente y de echar un vistazo a estas funciones para comprender c칩mo funcionan juntas. El laboratorio tambi칠n contiene algunas partes opcionales que puede consultar para obtener m치s informaci칩n sobre las llamadas a la API. Una vez m치s, antes de empezar el laboratorio, te animo a que consultes la documentaci칩n de la API de Spotify y crees tu cuenta. Cuando termines el laboratorio, acomp치침ame en la siguiente lecci칩n para explorar los patrones de transmisi칩n y gesti칩n.



C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "C2W2-2.txt"  
 echo.
) 
Ahora es el momento de analizar la ingesta de streaming con m치s detalle. En el curso anterior, trabajaste con un sistema de recomendaci칩n de productos en el 칰ltimo laboratorio, pero en realidad no explicamos en detalle c칩mo se configur칩 la ingesta de streaming para ese sistema. Por eso, en este v칤deo tendremos otra conversaci칩n con las partes interesadas en la que hablar칠 con un ingeniero de software sobre los detalles de ingesta de ese sistema de recomendaci칩n. Despu칠s de eso, construir치 el sistema usted mismo en el siguiente laboratorio. As칤 que pasemos a la conversaci칩n de seguimiento que tendremos con el ingeniero de software. Me alegro de volver a verte, Colleen. >> Tambi칠n me alegro de verte. >> Gracias, estoy trabajando para configurar un nuevo sistema de recomendaci칩n de productos y me gustar칤a trabajar con usted para comprender mejor c칩mo puede funcionar la ingesta de datos en t칠rminos de recibir datos de actividad de los usuarios en tiempo real desde el sitio web. >> Por supuesto, la forma en que funciona el sistema en el sitio web es que registramos continuamente los eventos en el registro del servidor web. Y esos eventos incluyen todo, desde las m칠tricas internas de rendimiento del sistema y cualquier error u otra anomal칤a que se genere, as칤 como la actividad de los usuarios. Al igual que los botones o enlaces en los que hacen clic nuestros usuarios mientras navegan por diferentes productos o realizan compras. >> De acuerdo, bueno, en un escenario ideal, probablemente me gustar칤a ingerir todos los datos de actividad de los usuarios y ninguna de las m칠tricas internas del sistema. 쮺rees que ser칤a posible separar los registros de eventos relacionados con la actividad de los usuarios y guardarlos en un registro separado para que yo pueda ajustarlos? >> S칤, sin duda podemos hacerlo. Me imagino varias formas en las que podr칤a funcionar. Pero si incluimos los mensajes sobre la actividad de los usuarios en un tema de Kafka o en una transmisi칩n de Kinesis, podr칤as incorporarlos directamente desde all칤 a tu canalizaci칩n. >> Vale, genial. S칤, parece una buena soluci칩n. Creo que si pudieras introducir una transmisi칩n de datos de Kinesis, me vendr칤a muy bien. He estado estudiando c칩mo podr칤an funcionar otros aspectos de la canalizaci칩n con la kinesis, por lo que me parece una buena opci칩n por ahora. La otra pregunta que tengo es sobre la carga 칰til de datos en s칤 y la velocidad de mensajes esperada.  Podr칤as decirme m치s sobre lo que puedo esperar en t칠rminos del formato de los mensajes individuales y la velocidad de los mensajes que llegan a la transmisi칩n? >> S칤, por lo que los mensajes se graban en formato JSON. Por lo tanto, la carga 칰til que puede esperar es un JSON que incluye un identificador de sesi칩n y toda la informaci칩n del cliente, como la ubicaci칩n, as칤 como su actividad de navegaci칩n en t칠rminos de los productos que han visto o agregado a su carrito con respecto al tama침o de los mensajes individuales, var칤an un poco, pero generalmente rondan unos cientos de bytes cada uno. La tasa de mensajes que puede esperar variar치 mucho seg칰n el n칰mero de usuarios que haya en la plataforma en un momento dado. Pero como puedes imaginar, un usuario puede generar varios eventos por minuto y, entonces, es posible que tengamos unos 10 000 usuarios en la plataforma en las horas punta. As칤 que eso podr칤a traducirse en hasta 1000 eventos por segundo. >> De acuerdo, genial, veamos, tal vez al final de una estimaci칩n general, entonces podr칤amos tener, digamos, 1000 eventos por segundo con un tama침o de unos cientos de bytes cada uno. Eso es menos de un megabyte por segundo, por lo que deber칤a estar dentro de la capacidad de una transmisi칩n de datos de Kinesis. Creo que pueden gestionar cientos de megabytes por segundo, seg칰n la configuraci칩n. >> Muy bien, la otra cosa que tendr칠 que configurar por mi parte es cu치nto tiempo se conservan los mensajes en la transmisi칩n. Como sabes, la transmisi칩n estar치 b치sicamente en un registro de solo pines, pero lo configuraremos para que los mensajes se eliminen despu칠s de un per칤odo de tiempo. >> Bien, por supuesto, la idea es que usemos los datos en tiempo real para hacer recomendaciones y tambi칠n guardemos las entradas y salidas del modelo de recomendaci칩n para analizarlas m치s adelante. Por lo tanto, si todo va bien, no necesitaremos volver a leer los mensajes de la transmisi칩n. Pero supongo que si algo sale mal, es posible que queramos tener la posibilidad de hacer una copia de seguridad y reproducir la transmisi칩n. Si algo se rompe. 쯈uiz치s podr칤amos retener los mensajes en la transmisi칩n durante un d칤a despu칠s de que se hayan escrito inicialmente? >> Claro, veamos. En un d칤a ajetreado, es posible que escribamos, como dijiste, alrededor de un megabyte por segundo en la transmisi칩n. Y hay una especie de orden de magnitud, alrededor de 100 000 segundos en un d칤a. Entonces, el tama침o total del arroyo podr칤a crecer hasta ser, 쯖u치l ser칤a? 100 gigabytes, en el peor de los casos. As칤 que parece razonable. Vale, bueno, 쯛ay algo m치s que quieras saber o deber칤amos seguir adelante y empezar a construir esto? >> Creo que eso es todo por ahora. Vamos a construirlo. >> Constr칰yelo. Bien, ese fue un ejemplo de una conversaci칩n con un ingeniero de software que ser치 una parte interesada principal y el propietario del sistema fuente del que tendr치s que ingerir datos. Como he recalcado en varios puntos a lo largo de estos cursos hasta ahora, hay otras cosas que deber칤a analizar con los propietarios de los sistemas de origen a la hora de comprender las posibles interrupciones en su canalizaci칩n de datos. Cosas como cambios de esquema o interrupciones. Pero por ahora, nos centraremos en entender los datos en s칤 y el mecanismo de ingesti칩n que utilizar치s en este caso. Acomp치침ame en el siguiente v칤deo, en el que analizaremos algunos de los detalles de esta conversaci칩n y analizaremos m치s de cerca la ingesta de streaming.

En la primera semana de este curso, analizamos los sistemas de transmisi칩n, incluidas las colas de mensajes y las plataformas de transmisi칩n de eventos, desde la perspectiva de que se trata de sistemas fuente, en los que la canalizaci칩n de datos estaba en el extremo consumidor de estas fuentes de datos. Tambi칠n mencion칠 que, seg칰n el sistema con el que trabajes, la fuente real podr칤a ser solo el productor del evento o varios elementos de un sistema de streaming, como varios productores, corredores y consumidores, podr칤an estar antes de tu sistema de ingesta. En la conversaci칩n que acabamos de mantener con el ingeniero de software, el plan que se nos ocurri칩 era que el ingeniero configurara una transmisi칩n de datos de Kinesis y usted consumir치 los mensajes de esa transmisi칩n. En este v칤deo, me gustar칤a hablar un poco m치s sobre los detalles de los flujos de mensajes y, despu칠s, ir치s al laboratorio. Como recordatorio r치pido, las plataformas de transmisi칩n de eventos y las colas de mensajes son las dos modalidades principales de ingesta de streaming. Una cola de mensajes es esencialmente un b칰fer que se utiliza para entregar mensajes de un productor de eventos a un consumidor de forma asincr칩nica. Las colas de mensajes suelen funcionar primero en entrar, primero en salir o Fifo, lo que significa que el consumidor de eventos siempre leer치 primero el mensaje m치s antiguo de la cola y, una vez consumido, se eliminar치 de la cola. Las plataformas de transmisi칩n de eventos, por otro lado, funcionan almacenando los mensajes de forma persistente en un registro que solo se puede adjuntar. El enrutador de eventos distribuye los mensajes del registro a los suscriptores y es posible reproducir o volver a procesar cualquier mensaje del registro. Utilizar치 el servicio Kinesis de Amazon como plataforma de transmisi칩n de eventos en el laboratorio, pero otra plataforma muy utilizada es Apache Kafka. En este v칤deo, utilizar칠 Kafka como plataforma de ejemplo para explicar algunos detalles y establecer paralelismos con Kinesis, donde existan. Y luego, en el siguiente v칤deo, Morgan analizar치 la kinesis con m치s detalle. Por lo tanto, al final de esta semana, puede tener un poco de contexto sobre estas dos soluciones. Por lo tanto, Apache Kafka es una plataforma de transmisi칩n de eventos de c칩digo abierto y, si bien las plataformas de transmisi칩n vienen en diferentes sabores y variedades, los principios de c칩mo se enrutan y almacenan los eventos son similares en todas las plataformas. En un nivel alto, los productores de eventos env칤an o env칤an mensajes a trav칠s de la red a un cl칰ster de Kafka, que contiene uno o m치s servidores, tambi칠n llamados corredores. Luego, los consumidores del evento leen o extraen mensajes de ese grupo de Kafka. Vamos a centrarnos un poco en el c칰mulo de Kafka que tengo aqu칤. Dentro de un cl칰ster de Kafka, los flujos de mensajes se dividen y se dirigen en lo que se denominan temas. Puede pensar en un tema como una categor칤a que contiene una colecci칩n de eventos relacionados, o tal vez en otro sentido, como un camino hacia alg칰n lugar. Por lo tanto, los mensajes se alinean en temas similares a c칩mo se alinean las filas de autom칩viles en diferentes autopistas seg칰n su destino. Un tema puede contener cualquier tipo de mensaje, por ejemplo, alertas de fraude , pedidos de clientes o lecturas de temperatura de dispositivos de IoT. Y es trabajo del productor enviar un mensaje a su tema correspondiente. Cada tema tiene una o m치s particiones, que son solo registros que contienen secuencias ordenadas e inmutables de mensajes a las que se agregan nuevos mensajes continuamente. Utilizando la analog칤a del sistema de carreteras, los tabiques son como los carriles de la carretera. M치s carriles permiten el paso de m치s coches, por lo que cada partici칩n gestiona un subconjunto de mensajes a medida que se a침aden al tema. Esto permite un tr치fico o un flujo de mensajes m치s eficientes. Una vez m치s, es tarea del productor decidir a qu칠 partici칩n enviar cada mensaje. La decisi칩n podr칤a basarse en una estrategia por turnos, por ejemplo, o calculando la partici칩n de destino en funci칩n de la clave del mensaje. Con la kinesis, todos estos conceptos son esencialmente los mismos, pero en lugar de temas, tienes transmisiones, y en lugar de particiones, tienes lo que se llama fragmentos. Ahora, por otro lado, los consumidores est치n agrupados y cada grupo de consumidores puede suscribirse a uno o m치s temas. Los consumidores de un grupo cooperan para consumir los mensajes de todas las particiones de un tema determinado. Cada partici칩n solo se puede asignar a un 칰nico consumidor del grupo y cada consumidor consume mensajes de un subconjunto diferente de particiones. Cuando un productor publica un mensaje en una partici칩n de temas, ese mensaje se entrega a un consumidor de cada grupo de consumidores suscriptor. Una vez que se publica un mensaje en un tema, el cl칰ster de Kafka conserva esa informaci칩n durante un per칤odo de tiempo configurable, independientemente de que el mensaje se haya consumido o no. Esto permite a los consumidores reproducir y volver a procesar los mensajes seg칰n sea necesario. En nuestra conversaci칩n con el ingeniero de software, nos enteramos de que las acciones de los usuarios en el sitio web se registran como mensajes en el registro del servidor web y esos mensajes se env칤an a un flujo de datos de Kinesis. Del mismo modo, esos mensajes podr칤an dirigirse a un tema de Kafka, y podr칤as consumirlos desde all칤 suscribi칠ndote a ese tema en un conjunto diferente de circunstancias. Puede imaginarse un escenario en el que tenga acceso directo para monitorear el registro del servidor web en busca de nuevos mensajes. Entonces, podr칤a tratar el registro del servidor web como el productor de eventos. A partir de ah칤, los eventos podr칤an transferirse a un tema de Kafka o a una transmisi칩n de Kinesis como primer paso del proceso de incorporaci칩n. Tambi칠n puede monitorear la actividad de la base de datos mediante un proceso conocido como captura de datos de cambio continuo o CDC continuo. Al procesar el registro de la base de datos, puede transmitir los cambios de datos a su canalizaci칩n de datos para asegurarse de que los datos de su canalizaci칩n est칠n sincronizados con las actualizaciones de datos de la base de datos de origen. A continuaci칩n, Morgan le explicar치 los detalles de las transmisiones de datos de Amazon Kinesis, que utilizaremos como herramienta de ingesta de transmisiones en el pr칩ximo laboratorio. Y despu칠s de eso, volver칠 para darte un r치pido recorrido por el 칰ltimo laboratorio antes de que te lances a crear tu propia soluci칩n de ingesta de streaming.

Acabas de aprender un poco m치s sobre c칩mo funciona Apache Kafka. Ahora quiero ayudarlo a comprender c칩mo funciona Amazon Kinesis Data Streams antes de que vaya al laboratorio. Como ya sabes, al igual que Kafka, hay productores de eventos que env칤an datos a la transmisi칩n y consumidores que leen los datos de la transmisi칩n. En el pr칩ximo laboratorio, trabajar치 con un Kinesis Data Stream como sistema fuente ascendente. No ser치s responsable de configurar la transmisi칩n en s칤. Sin embargo, tambi칠n ha visto que tambi칠n puede incorporar productores , consumidores y transmisiones en otras partes de sus sistemas de datos. Aqu칤 me gustar칤a explicarte algunos de los detalles de Kinesis que son relevantes cuando tienes el control de todo el sistema, que incluye al productor, el consumidor y la transmisi칩n en este caso. Al igual que en Kafka, los productores env칤an datos a los temas. Cuando trabaja con Kinesis Data Streams, un productor env칤a los datos a una transmisi칩n espec칤fica. Un flujo se compone de muchos fragmentos, que proporcionan las unidades de capacidad del flujo. Como necesitas escalar tu transmisi칩n para ingerir m치s datos, debes agregar m치s fragmentos a la transmisi칩n. Para saber cu치ntos fragmentos necesitar치 para su caso de uso o cu치ndo necesitar치 aumentar el n칰mero de fragmentos, necesitar치 saber el tama침o y la velocidad de las operaciones de escritura y lectura que espera realizar en su proceso. Las operaciones de escritura se producen cuando un productor de eventos escribe datos en la transmisi칩n, y las operaciones de lectura se producen cuando los consumidores intermedios leen los datos de la transmisi칩n. En cuanto a la capacidad, cada acelga puede soportar hasta cinco operaciones de lectura por segundo, y esas cinco operaciones pueden sumar una velocidad total m치xima de lectura de datos de dos megabytes/segundo. Para escribir datos, un productor puede escribir hasta 1000 registros por segundo en un disco duro con una velocidad total m치xima de escritura de datos de un megabyte/segundo. Para determinar la cantidad de fragmentos que necesitar칤a para un caso de uso espec칤fico, se necesitar칤an algunos an치lisis y algunos c치lculos matem치ticos, dado el tama침o y la velocidad de las operaciones de lectura y escritura que espera. A veces puede resultar dif칤cil estimar el n칰mero exacto de operaciones de lectura y escritura. Por ejemplo, en una aplicaci칩n nueva. O en otros casos, lo 칰nico que puede saber con certeza es que espera que el tr치fico de su aplicaci칩n var칤e dr치sticamente con el tiempo, como en una plataforma de comercio electr칩nico u otras aplicaciones p칰blicas. Para esas situaciones, puede usar Kinesis en modo bajo demanda. El modo bajo demanda gestionar치 autom치ticamente el escalado de los fragmentos hacia arriba o hacia abajo seg칰n sea necesario, y solo se le cobrar치 por lo que utilice. Esto puede resultar m치s conveniente desde una perspectiva operativa en comparaci칩n con la alternativa, que es el modo aprovisionado. Con el modo de aprovisionamiento, especific칩 la cantidad de fragmentos necesarios para la aplicaci칩n en funci칩n de la velocidad esperada de solicitudes de escritura y lectura. Luego, depende de usted agregar m치s fragmentos o volver a agregarlos cuando sea necesario. El modo de aprovisionamiento puede ser una buena opci칩n para su trabajo si tiene un tr치fico de aplicaciones predecible o si desea poder controlar los costos con m치s cuidado. Cuando se trata de los datos que se mueven a trav칠s de una transmisi칩n, cada registro de datos que un productor env칤a a la transmisi칩n incluye una clave de partici칩n, un n칰mero de secuencia y los datos en s칤 mismos en forma de lo que se denomina un objeto binario grande o blob para abreviar. Al configurar el generador de datos para su sistema, debe elegir una clave de partici칩n. La clave de partici칩n se usa luego para determinar en qu칠 fragmento se coloca el registro de datos. Luego, la propia Kinesis asigna un n칰mero de secuencia a medida que se escribe cada registro para mantener el orden de los registros dentro del fragmento. Por ejemplo, supongamos que quieres crear un flujo de transacciones desde una plataforma de comercio electr칩nico. A continuaci칩n, es posible que desee utilizar el ID de cliente como clave de partici칩n. En este caso, todas las transacciones de un solo cliente podr칤an almacenarse en el mismo fragmento. Esto facilitar칤a a los consumidores intermedios agrupar los registros relacionados con un solo cliente para su agregaci칩n y an치lisis. Un productor coloca los datos en fragmentos y un consumidor lee los datos de los fragmentos, y es com칰n que varios consumidores lean los datos de un fragmento. De forma predeterminada, los consumidores comparten la capacidad de lectura de Shards, que se denomina salida de ventilador compartida. Esto significa que los consumidores compiten por la capacidad de lectura. Esto puede ser un problema en algunos casos de uso. Para evitar este problema de capacidad, puedes configurar las cosas de manera que cada consumidor pueda leer a la capacidad total de lectura de dos megabytes/segundo del Shard, lo que se denomina ventilaci칩n mejorada. Puede usar servicios gestionados como AWS Lambda, Amazon Managed Service for Apache Flink y ADS Glue para procesar los datos almacenados en las transmisiones de datos de Kinesis, o puede crear sus propios consumidores personalizados mediante la biblioteca de clientes de Amazon Kinesis o KCL. Tambi칠n puede configurar las cosas para que la salida de una transmisi칩n se convierta en la entrada de otra, lo que puede permitirle crear flujos de trabajo de procesamiento de datos en tiempo real m치s complejos. Los consumidores tambi칠n pueden enviar datos a otros servicios de ADS, como la integraci칩n con Amazon Data Firehose para almacenar datos en Amazon S3. Tambi칠n es importante recordar que Kinesis Data Streams permite que varias aplicaciones funcionen con la misma transmisi칩n al mismo tiempo. Cada uno consume los datos de forma independiente y los env칤a de forma descendente a diferentes sistemas. A continuaci칩n, Joe le explicar치 los detalles del pr칩ximo laboratorio. Luego, usted mismo se pondr치 manos a la obra en la ingesti칩n de streaming con Kinesis. Buena suerte y divi칠rtete.

쯈u칠 es la Captura de Datos de Cambios (CDC)?
Estado: Traducido autom치ticamente del Ingl칠s
Traducido autom치ticamente del Ingl칠s
Informaci칩n:
Este elemento incluye contenido que a칰n no se tradujo a tu idioma preferido.
쯈u칠 es CDC? 

Supongamos que ha extra칤do y cargado datos de una base de datos en su sistema de almacenamiento. Al cabo de un tiempo, puede que necesite actualizar los datos almacenados en su sistema de almacenamiento para asegurarse de que est치n sincronizados con los datos del sistema de origen. Existen dos estrategias para ello:

    Instant치neas completas o carga completa: en este enfoque, cada vez que desee actualizar los datos almacenados en su sistema, ingestar치 todos los datos de su sistema de origen, sustituyendo los datos antiguos almacenados por los nuevos datos actualizados. Si sus datos son tabulares, la carga completa de los datos significa que elimina todos los datos antiguos de la tabla almacenada y extrae todas las filas de la tabla de origen cada vez que necesite actualizar sus datos almacenados. Se trata de un enfoque sencillo que garantiza la coherencia entre los datos del sistema de origen y los datos almacenados en su canalizaci칩n de datos. Sin embargo, para datos de gran volumen, puede tardar mucho tiempo en ejecutarse y requerir muchos recursos de procesamiento y memoria. Es m치s adecuado para casos en los que no es necesario actualizar los datos con frecuencia.

    Carga incremental (diferencial): en este enfoque, s칩lo se cargan las actualizaciones y los cambios desde la 칰ltima lectura de los sistemas de origen. Por ejemplo, al cargar actualizaciones de una base de datos de origen, puede utilizar una columna last_updated_at para identificar las filas de datos que se han actualizado desde la 칰ltima lectura de esta base de datos de origen y, a continuaci칩n, cargar s칩lo los datos actualizados de estas filas identificadas. Aunque este m칠todo es m치s r치pido que el de carga completa, especialmente para grandes vol칰menes de datos, su aplicaci칩n puede requerir una l칩gica m치s compleja. Cuando se trabaja con bases de datos, este proceso se conoce como Captura de datos de cambios o (CDC). Seg칰n el libro Fundamentos de la ingenier칤a de datos, "La captura de datos de cambios (CDC) es un m칠todo para extraer cada evento de cambio (inserci칩n, actualizaci칩n, eliminaci칩n) que se produce en una base de datos" y ponerlo a disposici칩n de los sistemas posteriores.

Casos de uso de CDC

    CDC le ayuda a sincronizar datos entre diferentes bases de datos, soportando la replicaci칩n continua de bases de datos. Por ejemplo, es posible que tenga un sistema PostgreSQL de origen que soporte una aplicaci칩n y desee ingerir de forma peri칩dica o continua los cambios de las tablas en un almac칠n de datos para permitir el an치lisis basado en los datos m치s recientes. O si trabaja en una empresa h칤brida, podr칤a necesitar usar CDC para capturar cambios en bases de datos locales y aplicar esos cambios a bases de datos en la nube.

    CDC le ayuda a capturar todos los cambios hist칩ricos con fines de auditor칤a y otros fines empresariales. Por ejemplo, algunas empresas est치n obligadas a mantener informaci칩n hist칩rica completa de las compras de sus clientes con fines normativos, o para extraer informaci칩n que permita a las empresas mejorar.

    CDC permite a los microservicios rastrear cualquier cambio en la base de datos de origen. Por ejemplo, considere un microservicio que gestiona pedidos de compra. Cuando se realiza un nuevo pedido, puede utilizar CDC para transmitir informaci칩n al servicio de env칤os y al servicio de atenci칩n al cliente.

Dos enfoques de CDC

    Push: Este enfoque requiere que implemente alg칰n tipo de l칩gica o proceso para capturar los cambios en la base de datos de origen. A continuaci칩n, depende de la base de datos de origen para empujar cualquier actualizaci칩n de datos al sistema de destino cuando algo cambia en el sistema de origen. Este m칠todo permite que los sistemas de destino se actualicen con los datos m치s recientes casi en tiempo real, pero si no se configura correctamente, se corre el riesgo de perder las actualizaciones de datos si los sistemas de destino est치n inaccesibles cuando los sistemas de origen intentan enviar los cambios.

    Pull: este m칠todo requiere que los sistemas de destino sondeen continuamente la base de datos de origen para comprobar si hay cambios y, a continuaci칩n, extraigan las actualizaciones de datos cuando se produzcan. Este m칠todo suele dar lugar a un retraso antes de que los sistemas de destino introduzcan nuevas actualizaciones de datos, ya que los cambios suelen producirse por lotes entre las solicitudes de extracci칩n.

Modelos de implementaci칩n de CDC

Existen varios m칠todos para que CDC extraiga los cambios de las bases de datos.

    CDC orientado a lotes o basado en consultas (pull-based): En este enfoque, se consulta la propia base de datos para identificar si se ha producido un cambio en los datos. En el caso de las bases de datos relacionales, esto requiere que la base de datos tenga una columna adicional denominada updated_at, last_updated o last_modified que le ayude a encontrar todas las filas actualizadas m치s all치 de un cierto tiempo especificado. Este proceso permite extraer los cambios y actualizar de forma incremental una tabla de destino. Sin embargo, este enfoque puede a침adir sobrecarga computacional al sistema de origen, ya que los sistemas de destino tienen que escanear cada fila de la tabla para identificar los 칰ltimos valores actualizados.

    CDC continuoo basado en registros (pull-based): En lugar de ejecutar consultas peri칩dicas para obtener los cambios de la tabla como un lote, puede tratar cada actualizaci칩n de la base de datos como un evento utilizando CDC continuo. Este tipo de CDC se basa en la comprobaci칩n del registro de la base de datos. Un registro de base de datos registra cada cambio en la base de datos de forma secuencial (por ejemplo, cada creaci칩n, actualizaci칩n, eliminaci칩n) y se utiliza en caso de fallo para restaurar el estado de la base de datos. Puede leer los eventos de este registro (escribiendo su propio c칩digo o utilizando una herramienta CDC como Debezium) y enviarlos a una plataforma de streaming, como Apache Kafka. De esta forma, puede capturar los cambios de datos en tiempo real sin incurrir en ninguna sobrecarga computacional ni requerir la necesidad de una columna adicional en las bases de datos de origen.

    CDC basado en desencadenantes (m칠todo basado en push): Un desencadenante es una funci칩n almacenada que puede configurar para que se ejecute cuando cambie una columna espec칤fica. Los disparadores informan al CDC de los cambios en las bases de datos de origen y, de este modo, se libera al CDC de la detecci칩n de cambios. Sin embargo, un n칰mero excesivo de triggers puede afectar negativamente al rendimiento de escritura de la base de datos de origen.

Herramientas para CDC

Si칠ntase libre de leer m치s sobre algunas de las herramientas comunes utilizadas para implementar CDC

    Debezium

AWS DMS

API de conexi칩n Kafka

CDC basado en registros Airbyte


Resumen: Consideraciones generales para elegir las herramientas de ingesti칩n
Estado: Traducido autom치ticamente del Ingl칠s
Traducido autom치ticamente del Ingl칠s

A la hora de elegir una herramienta de ingesti칩n para sus sistemas de datos, debe tener en cuenta las caracter칤sticas de los datos que va a ingestar, as칤 como la fiabilidad y durabilidad de la herramienta de ingesti칩n.

Caracter칤sticas de los datos

Nota: En el libro "Fundamentos de la ingenier칤a de datos", Joe y Matt se refieren a las caracter칤sticas de los datos como la carga 칰til de los datos, que incluye el tipo de datos (tipo y formato), la forma, el tama침o, el esquema y los tipos de datos, y los metadatos.

    Tipo de datos y estructura: En el curso 1 aprendimos que los datos de los sistemas fuente pueden ser estructurados, no estructurados o semiestructurados. A la hora de decidir c칩mo ingerir los datos y qu칠 herramienta elegir, es necesario conocer el tipo y la estructura de los datos (por ejemplo, una imagen en formato PNG) para poder identificar la herramienta de ingesta adecuada y las transformaciones que podr칤a ser necesario aplicar m치s adelante.

    Volumen de datos: En cuanto al Volumen de datos, hay que tener en cuenta dos cosas:

        El tama침o en bytes de los datos existentes que necesita ingestar: En el caso de la ingesta por lotes, debe tener en cuenta el tama침o de los datos hist칩ricos que necesita ingerir. 쯉e pueden ingerir todos los datos hist칩ricos en un solo lote? Dependiendo de la conexi칩n de red entre el sistema de origen y el sistema de destino, puede ser posible transferir los datos hist칩ricos a trav칠s de la red, pero si tiene un ancho de banda limitado, puede que tenga que dividir la carga masiva en trozos, lo que reduce efectivamente el tama침o de la carga en subsecciones m치s peque침as.En el caso de la ingesta de streaming, debe tener en cuenta el tama침o del mensaje. Debe asegurarse de que la herramienta de ingesta de streaming puede manejar el tama침o m치ximo esperado del mensaje. Por ejemplo, Amazon Kinesis Data Streams admite un tama침o m치ximo de mensaje de 1 MB, mientras que Kafka admite por defecto este tama침o m치ximo, pero se puede configurar para que admita un tama침o m치ximo de datos de 20 MB o m치s.

        El tama침o de los datos futuros que puede ingerir con la misma canalizaci칩n: 쯖칩mo espera que crezcan los datos? 쮺u치l es el crecimiento diario, mensual o anual de los datos? Considerar el tama침o actual y futuro le ayuda a entender c칩mo configurar su herramienta y qu칠 coste anticipar para garantizar que su sistema de ingesta satisface las demandas.

    Requisitos de latencia: A la hora de dise침ar su canalizaci칩n, uno de los requisitos de las partes interesadas que debe tener en cuenta es la latencia: 쯔 qu칠 velocidad quieren operar con los datos? 쮺u치l es el retraso aceptable? 쯅ecesitan extraer informaci칩n de los datos un d칤a despu칠s de que se ingieran, o necesitan informaci칩n casi en tiempo real? Dicho de otro modo, 쯥e trata de un escenario por lotes, en el que los datos deben incorporarse una vez al d칤a, a la semana o al mes? Para cumplir el requisito de latencia, hay que pensar en la rapidez con la que hay que procesar los datos ingeridos una vez que llegan a la canalizaci칩n y comprender tambi칠n la rapidez con la que se generan los datos de origen. La velocidad de los datos influir치 en las herramientas (batch o streaming) que elija para ingerir y procesar los datos.

    Calidad de los datos: 쮼st치n los datos de origen en buen estado para su uso posterior inmediato? 쯈u칠 tratamiento posterior es necesario para servirlos? Dependiendo de los sistemas de origen, los datos pueden estar incompletos o contener informaci칩n incoherente, duplicados o errores. Si no se espera que los datos est칠n en buen estado, es posible que tenga que comprobar la calidad de los datos ingeridos para solucionar cualquier problema. Algunas herramientas de ingesta pueden ayudarle a completar los valores que faltan o a detectar/corregir incoherencias o entradas no v치lidas. Obtendr치 m치s informaci칩n sobre las comprobaciones de calidad en el pr칩ximo curso.

    Cambiosen el esquema:los cambios en el esquema(por ejemplo, a침adir una nueva columna, cambiar el tipo de columna, crear una nueva tabla, cambiar el nombre de una columna) se producen con frecuencia en los sistemas fuente y, por lo general, est치n fuera de su control. Si espera que estos cambios se produzcan con frecuencia, puede que tenga que considerar el uso de herramientas de ingesta que detecten autom치ticamente los cambios de esquema. Sin embargo, la comunicaci칩n entre usted y las partes interesadas es tan importante como la automatizaci칩n que comprueba los cambios de esquema.

Fiabilidad y durabilidad 

La fiabilidad y la durabilidad son dos aspectos importantes en la fase de ingesta. Fiabilidad significa asegurarse de que los sistemas de ingesta cumplen correctamente su funci칩n. Durabilidad significa asegurarse de que los datos no se pierden ni se corrompen. Si dise침a un sistema de ingesti칩n fiable, garantizar치 la durabilidad de los datos ingestados. Por ejemplo, los sistemas de streaming, como los dispositivos IoT, no retienen los eventos indefinidamente, por lo que si no ingiere correctamente sus datos, estos pueden perderse. Aseg칰rate de comprender las caracter칤sticas de los sistemas de origen y las herramientas de ingesta.

Consejos: Eval칰e las compensaciones entre el coste de perder datos y la creaci칩n de un nivel adecuado de redundancia. Para m치s informaci칩n y consideraciones, consulte el cap칤tulo 7 de Fundamentos de la ingenier칤a de datos.


En el curso anterior, implement칩 una canalizaci칩n de streaming similar a la siguiente con un bucket de Kinesis Data Firehose y S3. Se le proporcion칩 un Kinesis Data Stream que transmite las actividades de los usuarios en l칤nea como eventos o registros. Procesas estos registros para calcular las recomendaciones de productos y usar la instancia de Data Firehose para enviar los registros al bucket de S3 de tu canalizaci칩n. En el pr칩ximo laboratorio, obtendr치 m치s informaci칩n sobre c칩mo puede continuar transmitiendo estos registros en proceso y explorar m치s a fondo Kinesis Data Streams como fuente. El laboratorio consta de dos partes. En la primera parte, trabajar치 con un Kinesis Data Stream que act칰a como un enrutador entre un productor simple y un consumidor simple. En la segunda parte, volver치s a trabajar con el escenario del curso 1. Se le proporcionar치 una transmisi칩n de datos de Kinesis como fuente, pero esta vez usar치 otras dos transmisiones de datos de Kinesis para continuar transmitiendo los registros en proceso. De cada uno de estos dos nuevos flujos de datos, un Data Firehose tomar치 los datos y los entregar치 al bucket S3 correspondiente. Repasemos la primera parte de este laboratorio. Para comprender mejor los componentes de una plataforma de transmisi칩n de eventos, primero crear치 una transmisi칩n de datos de Kinesis y, a continuaci칩n, interactuar치 con ella desde el punto de vista del productor y el consumidor. Para ello, se le proporcionan dos scripts de Python, consumer desde la CLI y producer desde la CLI. El script de productor representa una aplicaci칩n de productor simple que escribe un 칰nico registro de datos en Kinesis Data Stream. El registro de datos es una cadena JSON que contiene los detalles de una sesi칩n de usuario, como el identificador de sesi칩n, el n칰mero de cliente, la ciudad, el pa칤s y el historial de navegaci칩n. El script de Python del productor usa Boto3 para interactuar con Kinesis. Puede ejecutar el script desde la terminal, que espera dos argumentos: el nombre de la transmisi칩n de datos de Kinesis y la cadena JSON de un solo registro. En el script del productor, estos dos argumentos se pasan al m칠todo PutRecord de Boto3 para escribir el registro en Kinesis Data Stream. El script de consumidor tambi칠n representa una aplicaci칩n de consumidor sencilla que puede ejecutar desde el terminal especificando el nombre de la transmisi칩n de datos de Kinesis. Cuando ejecutes el script de consumidor, recorrer치 todos los fragmentos del flujo de datos, extraer치 todos los registros de cada fragmento y, a continuaci칩n, imprimir치 informaci칩n en el terminal sobre cada registro. Si compruebas el c칩digo del script de consumo, puedes ver que tambi칠n se usa Boto3. En la funci칩n PullShards, el consumidor recorre continuamente los fragmentos del flujo de datos, edita los registros con el m칠todo getRecords de Boto3 y, a continuaci칩n, imprime este texto en la terminal para explicar qu칠 registro se ley칩 desde qu칠 fragmento y en qu칠 posici칩n dentro del fragmento. En la primera parte del laboratorio, no editar치s los guiones para productores y consumidores, pero tendr치s la tarea de ejecutar estos guiones. Despu칠s de crear Kinesis Data Stream, primero ejecutar치 el script de consumidor en la terminal. Para ello, primero activar치 el entorno de JupyterLab, despu칠s ir치 a la carpeta SourceCLI o srcCLI y, por 칰ltimo, ejecutar치 el script de consumidor como se muestra aqu칤, proporcion치ndole el nombre de la transmisi칩n de Kinesis que cre칩. Observar치 que no se imprimir치 nada en el terminal porque el flujo de datos ahora est치 vac칤o. Mantendr치s este terminal abierto para que el consumidor siga funcionando y, a continuaci칩n, en otro terminal, ejecutar치s el script del productor para escribir un registro en el flujo de datos. De nuevo, en la nueva terminal, navegar치 hasta la carpeta SourceCLI o srcCLI y, a continuaci칩n, ejecutar치 el script del productor especificando el nombre de la transmisi칩n de Kinesis y la cadena JSON que representa el registro. Ahora, si compruebas el primer terminal en el que est치 funcionando el consumidor, ver치s que el consumidor ha le칤do el registro que acabas de enviar al flujo de datos. Cuando hayas terminado con la primera parte del laboratorio, volver치s al escenario original de comercio electr칩nico. Se le proporcionar치 una transmisi칩n de datos de Kinesis que representa su sistema de origen, por lo que estar치 del lado del consumidor al ingerir datos de la fuente de transmisi칩n. Implementar치 una canalizaci칩n de ETL de streaming, en la que primero aplicar치 una transformaci칩n simple en los registros ingeridos y, a continuaci칩n, continuar치 transmitiendo estos registros en su canalizaci칩n. Para ello, configurar치 dos transmisiones de datos de Kinesis. Enviar치 los registros que corresponden a clientes de EE. UU. a un flujo de datos y los que corresponden a clientes internacionales a otro flujo de datos. Suponiendo que su empresa haya notado que los clientes muestran diferentes comportamientos de compra en funci칩n de sus pa칤ses. Por lo tanto, si se encuentran en los EE. UU., sus actividades en l칤nea deben ser procesadas por un determinado motor de recomendaciones. De lo contrario, sus actividades en l칤nea deben ser procesadas por otro motor de recomendaciones. Para cada uno de estos dos flujos de datos, una manguera de datos recopilar치 autom치ticamente los datos y los entregar치 a su respectivo bucket de S3. En primer lugar, crear치 los dos flujos de datos, las dos instancias de Firehose y los dos dep칩sitos con Boto3. A continuaci칩n, escribir치 el c칩digo de transformaci칩n en el script de consumidor que se le proporciona en esta carpeta ETL. Este script tambi칠n se puede ejecutar desde el terminal y espera el nombre del flujo de datos de origen y los nombres de los dos flujos de datos de destino. El script contiene esta funci칩n de extracci칩n de fragmentos, en la que se le proporciona el c칩digo que recorre los fragmentos para extraer los registros. Aqu칤, una vez que se extrae un registro, tendr치 que completar esta parte del c칩digo para transformar el registro. La transformaci칩n consiste en a침adir tres campos, como se muestra aqu칤. El primer campo, cantidad total de productos, representa la suma de las cantidades de productos que aparecen en el historial de navegaci칩n. El segundo campo, en general en el carrito de la compra, representa la suma de las cantidades de productos de los productos que se colocan en el carrito de la compra. Y el tercer campo, el total de productos diferentes, representa el n칰mero de productos que aparecen en el historial de navegaci칩n. Y, por 칰ltimo, enviar치 el registro de transformaci칩n al flujo de datos correspondiente en funci칩n del valor del campo de pa칤s. Tras aplicar estas modificaciones al script del consumidor, ejecutar치 el script en el terminal especificando el nombre del flujo de datos de origen y los nombres de los dos flujos de datos de destino, como se muestra aqu칤. Al ejecutar este comando, el script del consumidor leer치 los registros de la transmisi칩n de datos de origen, los transformar치 y, a continuaci칩n, los enviar치 a las transmisiones de Kinesis correspondientes. Los 칤ndices Firehose de Kinesis entregar치n autom치ticamente los datos a los dep칩sitos de S3. Esta fue una descripci칩n general de las tareas que realizar치 en este laboratorio, y ahora est치 listo para comenzar. Cuando termines el laboratorio, acomp치침ame aqu칤 para ver un resumen r치pido de esta semana.


C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "C2W3_great_expectations.txt"  
 echo.
) 
En el siguiente laboratorio, utilizar치 Great Expectations para aplicar pruebas de calidad de datos. Pero antes de continuar, me gustar칤a ofreceros una visi칩n general de los componentes principales de Great Expectations con un ejemplo de flujo de trabajo. Cuando trabajas con Great Expectations, normalmente comienzas tu flujo de trabajo especificando los datos que deseas probar. A continuaci칩n, define las expectativas o las pruebas que desea realizar con los datos y, por 칰ltimo, valida los datos en funci칩n de sus expectativas. Para implementar un flujo de trabajo de este tipo, debe interactuar con los componentes principales de Great Expectations, que consisten en el contexto de los datos, las fuentes de datos, las expectativas y los puntos de control. Estos componentes se utilizan para acceder, almacenar y gestionar los objetos y procesos necesarios en el flujo de trabajo. Para iniciar el flujo de trabajo, primero debe crear una instancia de un objeto de contexto de datos. Un contexto de datos sirve como punto de entrada para la API Great Expectations, que consta de clases y m칠todos que permiten crear objetos para conectarse a las fuentes de datos, crear expectativas y validar los datos. Con el contexto de datos, puede configurar y acceder a las propiedades, como los objetos, as칤 como a los metadatos de su proyecto Great Expectations. Despu칠s de crear una instancia del objeto de contexto de datos, debe declarar el objeto de la fuente de datos, lo que indica las grandes expectativas de d칩nde obtener los datos que desea validar. La fuente de datos puede ser una base de datos SQL, un sistema de archivos local, un bucket de S3 o incluso un marco de datos de pandas. Despu칠s de conectarse a la fuente de datos, debe indicar a Great Expectations en qu칠 parte de los datos debe centrarse. Para ello, declare sus activos de datos desde la fuente de datos. Un activo de datos es una colecci칩n de registros dentro de una fuente de datos. Puede ser una tabla de una base de datos SQL o un archivo de un sistema de archivos. Tambi칠n puede ser un recurso de consulta que une datos de m치s de una tabla, o puede ser una colecci칩n de archivos que coincidan con un patr칩n de expresiones regulares en particular. Puede dividir a칰n m치s los datos de su activo en lotes. Por ejemplo, si su activo de datos representa los registros que corresponden a las ventas de un a침o determinado en una tabla, puede dividir los registros en lotes mensuales y validar cada lote. O puede particionar sus datos con respecto a los identificadores de la tienda. Tambi칠n puede trabajar con todos los registros de su activo de datos como un solo lote para recuperar los lotes de su activo de datos. Ya sea un lote o varios lotes, debe crear un objeto de solicitud por lotes a partir de su activo. Las solicitudes por lotes son la forma principal de recuperar datos del activo de datos y es lo que debe proporcionar para el resto de los componentes de Great Expectations. A continuaci칩n, debe definir sus expectativas. Una expectativa es una declaraci칩n que puedes usar para verificar si tus datos cumplen una condici칩n determinada. Por ejemplo, puede definir una expectativa para comprobar si una columna no contiene valores nulos. Puede definir su propia expectativa o usar una de las declaraciones disponibles en la galer칤a de expectativas. Por ejemplo, esperar que el m칤nimo de la columna est칠 entre, esperar que los valores de la columna sean 칰nicos y esperar que los valores de la columna sean nulos son ejemplos de pruebas que puedes usar directamente. Ver치s c칩mo puedes llamarlos en el ejemplo de flujo de trabajo en el que trabajaremos. Tambi칠n puede definir m치s de una expectativa para su activo de datos y recopilarlas en un objeto de conjunto de expectativas. Ahora, para validar sus datos, debe crear un objeto validador que espere una solicitud por lotes y su correspondiente conjunto de expectativas. Puede validar manualmente los datos interactuando directamente con el validador, o puede simplificar el proceso de validaci칩n mediante un objeto de punto de control. Un punto de control toma una solicitud por lotes y un conjunto de expectativas y los proporciona autom치ticamente a un validador que genera los resultados de la validaci칩n. A lo largo de este proceso, se generar치n metadatos sobre su proyecto y, con grandes expectativas, los guardar치n en algunos almacenes de back-end. Great Expectations es compatible con diferentes tipos de tiendas. Las tiendas m치s comunes son Expectation Store, donde puede encontrar sus suites de expectativas. El almac칠n de validaci칩n, donde puede encontrar informaci칩n sobre los objetos generados al validar los datos compar치ndolos con el conjunto de expectativas. La tienda Checkpoint, donde puede encontrar las configuraciones de sus puntos de control. Y una tienda de documentos de datos, donde puede encontrar informes sobre expectativas, puntos de control y resultados de validaci칩n. Puede acceder a estos almacenes y a sus ajustes a trav칠s del objeto de contexto de datos. Estos son los pasos de un flujo de trabajo t칤pico de Great Expectations. En el siguiente v칤deo, aplicaremos estos pasos en un conjunto de datos de ejemplo.

En el v칤deo anterior, ha aprendido acerca de los componentes b치sicos de las expectativas de calificaci칩n y c칩mo es un flujo de trabajo de validaci칩n t칤pico. Apliquemos ahora estos pasos en un conjunto de datos de ejemplo, que es la Base de datos de alquiler de DVD, que ha visto en uno de los laboratorios de la primera semana de este curso. S칩lo nos centraremos en comprobar las columnas de la tabla de pagos. En concreto, comprobaremos si la columna id de pago contiene ids 칰nicos, la columna id de cliente no contiene valores nulos, y todos los valores de la columna importe son no negativos. Ya he configurado una base de datos postgresSQL localmente en mi m치quina y cargado los datos en la base de datos. En el terminal har칠 pip install grandes expectativas. Despu칠s, para iniciar el proyecto grandes expectativas, escribir칠 great expectations init. Este comando inicializar치 el objeto de contexto de datos, configurar la estructura de la carpeta de su proyecto como se muestra aqu칤, y crear sus tiendas backend como los puntos de control, las expectativas, datos docs, y las tiendas de validaci칩n como directorios locales. Escribir칠 Y para proceder, siempre se puede cambiar la ubicaci칩n de sus tiendas backend. As칤, por ejemplo, en el laboratorio va a configurar sus tiendas como s tres cubos. En este video ill mantener estos como directorios locales. Ahora, para interactuar con los componentes de grandes expectativas, voy a lanzar un Jupyter notebook en el mismo directorio ra칤z y crear este archivo notebook, ejemplo. Aqu칤 en el archivo notebook, primero importar칠 el paquete de grandes expectativas y luego llamar칠 al m칠todo get context, para obtener el objeto context del proyecto. Usando este objeto, puedes conectarte a la fuente de datos, definir tus expectativas, crear un validador, y luego ejecutar tus puntos de control. As칤 que vamos a utilizar este contexto para crear primero el objeto fuente de datos. Create expectations proporciona diferentes m칠todos que te permiten conectarte a tus diferentes fuentes de datos. Para conectarme a mi base de datos SQL local, llamar칠 al m칠todo context sources add sql, y luego elegir칠 my datasource como nombre para la fuente de datos. Este m칠todo tambi칠n espera una cadena de conexi칩n que incluye la informaci칩n necesaria para conectarse a la base de datos, como el nombre de usuario, la contrase침a, el nombre de host, el n칰mero de puerto y el nombre de la base de datos. Este es el formato de la cadena que puedes usar para conectarte a una base de datos postgresql . Usando este formato, crear칠 la cadena de conexi칩n a mi base de datos usando la informaci칩n de mi base de datos local. A continuaci칩n, desde el origen de datos, crear칠 el activo de datos llamando al m칠todo add tableasset. Como s칩lo nos estamos centrando en la tabla de pagos, elegir칠 el nombre para el activo como payment tb y luego especificar칠 el nombre de la tabla dentro de la base de datos origen, que es payment. En este caso. Ahora, si quieres crear lotes en tu activo, grandes expectativas proporciona un conjunto de m칠todos que te permite dividir tu activo de datos basado en la fecha o un valor de columna. As칤 que por ejemplo, llamar칠 aqu칤 al m칠todo add splitter datetime part en el activo que acabo de crear, y luego indicar칠 el nombre de la columna que contiene la fecha, que es payment date, y especificar칠 month para el segundo argumento datetime parts. Y por 칰ltimo, crear칠 el objeto de solicitud de lote llamando al m칠todo build batch request sobre el objeto asset. Antes de continuar con el flujo de trabajo, echemos un vistazo r치pido a los lotes. Usando el objeto asset, llamar칠 al m칠todo get batch list from batch requests y pasar칠 el objeto de solicitud de lote que acabo de crear y luego iterar칠 sobre los lotes para comprobar la especificaci칩n de cada lote. Puedes ver que los datos contienen cuatro lotes donde cada lote corresponde a un mes. Ahora que tenemos el objeto batch request creado, vamos a definir las expectativas. Lo har칠 de forma interactiva trabajando directamente con un validador. Primero, crear칠 el conjunto de expectativas que contendr치 todas las expectativas que definir치. Sobre el objeto context llamar칠 al m칠todo add o update expectation suite y elegir칠 el nombre mysuite. Ahora, para crear el validador, utilizar칠 el objeto de contexto para llamar al m칠todo get validator y introducir칠 el objeto de solicitud de lote y el nombre del conjunto de expectativas. Ahora, utilizando el validador, puedes llamar a cualquiera de los m칠todos de expectativas desde la galer칤a de expectativas. As칤 que aqu칤 llamar칠 a expect column values to be unique para comprobar la unicidad de los valores dentro de la columna payment id. Los resultados que se muestran aqu칤, corresponden al 칰ltimo lote. Sin embargo, todos los lotes se probaron y, puesto que llegamos al 칰ltimo lote, significa que la prueba se ejecut칩 correctamente en todos los dem치s lotes. Ahora definir칠 dos expectativas m치s. Esperar que los valores de columna no sean nulos en la columna id de cliente para comprobar que la columna no contiene ning칰n valor nulo, y Esperar que el valor m칤nimo de columna est칠 entre en la columna importe para comprobar que todos los valores de la columna son nulos negativos. Puede ver que ambas pruebas se ejecutan correctamente. Ahora estas tres expectativas que he creado interactivamente mientras trabajaba con el validador s칩lo est치n disponibles para esta sesi칩n. Para poder utilizar estas expectativas en otras sesiones, es necesario guardarlas. Usando el validador, llamar칠 al m칠todo guardar conjunto de expectativas y especificar칠 para grandes expectativas que no descarte las expectativas fallidas, este m칠todo guarda el conjunto de expectativas, mi conjunto que contiene las tres expectativas en el almac칠n de expectativas. Ahora, vamos a automatizar el proceso de la validaci칩n utilizando el objeto checkpoint. Para crear el objeto checkpoint, llamar칠 al m칠todo add o update checkpoint en el objeto context y le asignar칠 el nombre mycheckpoint. Este m칠todo espera validaciones como segundo argumento que consiste en una lista de pares de un lote de datos y su correspondiente conjunto de expectativas de para especificar estas validaciones he iterado a trav칠s de los lotes y para cada lote he extra칤do la petici칩n del lote y he usado el nombre del conjunto de expectativas. Ahora ejecutar칠 el punto de comprobaci칩n llamando al m칠todo run, la prueba de los cuatro lotes superada. Para obtener informaci칩n m치s detallada sobre cada resultado, puedes consultar los documentos de datos llamando al m칠todo build data docs utilizando el objeto context. Este m칠todo devuelve un enlace que abrir칠 para consultar los documentos de datos. Aqu칤 puedes encontrar los resultados de las validaciones realizadas en cada lote. Puedes ver que todas las pruebas fueron exitosas. Si haces clic en cualquier fila, puedes encontrar estad칤sticas sobre las expectativas evaluadas, as칤 como las expectativas exitosas y no exitosas. Tambi칠n puedes encontrar las expectativas realizadas en cada columna y el resultado correspondiente. As칤 que vamos a hacer clic en mostrar m치s informaci칩n. Aqu칤 encontrar치s algunos metadatos sobre la ejecuci칩n del punto de control y los lotes utilizados. Ahora, volvamos a la p치gina de inicio y hagamos clic en la pesta침a suites de expectativas. Aqu칤 encontrar치s informaci칩n sobre la suite de expectativas, mi suite. Y ah칤 lo tienes. Ahora ya sabes c칩mo interactuar con los componentes principales de las expectativas de grado. Ahora, es tu turno de practicar el uso de grandes expectativas y validar algunos datos en el laboratorio.

[Opcional] Conversaci칩n y notas adicionales
Estado: Traducido autom치ticamente del Ingl칠s
Traducido autom치ticamente del Ingl칠s
Informaci칩n:
Este elemento incluye contenido que a칰n no se tradujo a tu idioma preferido.
Principales conclusiones de la conversaci칩n con Barr Moses

Los problemas con los datos son inevitables y pueden producirse en cualquier fase del proceso de datos. Cuanto antes se detecten, menos da침o causar치n a la organizaci칩n. Para detectar problemas con los datos, primero hay que elegir indicadores que eval칰en la calidad de los datos, de forma similar a c칩mo los equipos de software controlan los indicadores que eval칰an la salud de la infraestructura de su software.

En su libro(Data Quality Fundamentals

), Barr Moses sugiere empezar con las siguientes preguntas:

    쮼st치n actualizados los datos?

    쮼st치n completos los datos?

    쮼st치n los campos dentro de los rangos esperados?

    쮼s el 칤ndice de nulos mayor o menor de lo que deber칤a?

    쮿a cambiado el esquema?

Formul칩 estas preguntas en 5 pilares para la observabilidad de los datos, cuyo objetivo es describir completamente el estado de los datos. 
Los 5 pilares de Barr Moses

    Distribuci칩n/ Calidad interna: El pilar de la calidad se refiere a las caracter칤sticas internas de los datos, y comprueba m칠tricas como el porcentaje de elementos NULOS, el porcentaje de elementos 칰nicos, las estad칤sticas de resumen y si tus datos est치n dentro de lo esperado. Le ayuda a asegurarse de que sus datos son fiables en funci칩n de sus expectativas de datos.

    Frescura: La frescura de los datos se refiere a lo "frescos" o "actualizados" que est치n los datos dentro del activo final (tabla, informe BI), es decir, cu치ndo se actualizaron los datos por 칰ltima vez y con qu칠 frecuencia se actualizan. Los datos obsoletos suponen una p칠rdida de tiempo y dinero.

    Volumen: Volumen de datos se refiere a comprobar la cantidad de datos ingeridos y buscar picos o ca칤das inesperadas. Las ca칤das repentinas en el volumen de datos pueden indicar problemas como p칠rdida de datos o interrupciones del sistema, y los aumentos repentinos pueden indicar aumentos inesperados en el uso.

    Linaje: Seg칰n Barr

    , "cuando los datos se rompen, la primera pregunta es siempre "쯗칩nde?" El linaje de datos le ayuda a trazar el viaje de los datos desde su origen hasta su destino, visualizando c칩mo se transformaron los datos y d칩nde se almacenaron. De este modo, se puede identificar el origen de errores o anomal칤as.  

    Esquema: El esquema de datos se refiere a la supervisi칩n de los cambios en la estructura o los tipos de datos. Este pilar ayuda a evitar fallos en la canalizaci칩n de datos.

Recursos adicionales

Si desea obtener m치s informaci칩n sobre la observabilidad de los datos, puede consultar los siguientes recursos adicionales.

    Fundamentos de la calidad de datos 

, por Barr Moses, Lior Gavish, Molly Vorweck [libro]

The rise of data downtime

, por Barr Moses [art칤culo]

쯈u칠 es la observabilidad de los datos?
 por Andy Petrella [libro]


C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "cat ...awsaws_console_url.txt"  
 echo.
) 
cat ../.aws/aws_console_url

    username: postgres
    password: postgrespwrd o adminpwrd


aws rds describe-db-instances --db-instance-identifier de-c2w1a1-rds --output text --query "de-c2w1a1-rds.ct4yggiam6pg.us-east-1.rds.amazonaws.com" --region us-east-1

de-c2w1a1-rds.ch668gmkcdla.us-east-1.rds.amazonaws.com
de-c2w1a1-rds.ct4yggiam6pg.us-east-1.rds.amazonaws.com

psql --host=de-c2w1a1-rds.ct4yggiam6pg.us-east-1.rds.amazonaws.com --username=postgres --password --port=5432

s3
de-c2w1a1-395639430833-us-east-1-data



C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "draft.txt"  
 echo.
) 
38084256712
378.190,04


MYSQL_CONNECTION_STRING: mysql+pymysql://admin:adminpwrd@de-c2w3a1-rds.cdi6y0y2yhrw.us-east-1.rds.amazonaws.com:3306/taxi_trips


GXArtifactsS3Bucket
	
de-c2w3a1-891377276567-us-east-1-gx-artifacts
	
This is the S3 Bucket for GX Artifacts


GXDocsS3Bucket
	
de-c2w3a1-891377276567-us-east-1-gx-docs
	
This is the S3 Bucket for GX Docs



C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "resultado.txt"  
 echo.
) 

C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "C2W1-1.txt"  
 echo.
) 
Bienvenido al segundo curso de la especializaci칩n en ingenier칤a de datos. En el primer curso, obtendr치 una visi칩n general de alto nivel del campo de la ingenier칤a de datos, los principios de una buena arquitectura de datos y c칩mo traducir las necesidades de las partes interesadas en requisitos y herramientas para sus sistemas de datos. En este curso, aprender치 m치s sobre la ingesti칩n de datos de los sistemas de origen, as칤 como sobre DataOps y la orquestaci칩n de la canalizaci칩n de datos de extremo a extremo. Estoy aqu칤 nuevamente con su instructor, Joe Reis, quien tambi칠n es coautor del superventas Fundamentals of Data Engineering. Joe, 쯣odemos ver un poco sobre lo que ver치n los alumnos en este curso? Claro que s칤, Andrew. Este curso, como ha dicho, incluye un enfoque en las dos primeras etapas del ciclo de vida de la ingenier칤a de datos, que son la generaci칩n y los sistemas fuente de datos y la ingesti칩n de datos de esos sistemas fuente. Empezaremos por analizar los diferentes tipos de sistemas fuente, tal vez cosas como bases de datos o sistemas de almacenamiento y transmisi칩n de objetos. Analizaremos en detalle c칩mo interactuar치 con los sistemas fuente en su trabajo como ingeniero de datos. Despu칠s de eso, analizaremos la ingesta de datos de los sistemas de origen, as칤 como los aspectos de DataOps y orquestaci칩n de la creaci칩n de canalizaciones de datos. Eso suena genial. Para muchos sistemas de IA, la ingenier칤a de datos o la ingesta de datos representan aproximadamente el 80% del trabajo, y luego el modelado del aprendizaje autom치tico representa quiz치s el 20% del trabajo. Sin embargo, la atenci칩n de las personas sobre estos dos temas suele cambiar, ya que el 80% de la atenci칩n se centra en el modelado de la IA y no se presta suficiente atenci칩n , ni en las mejores pr치cticas, y, francamente, tambi칠n en la ingesti칩n de datos. De hecho, en algunos de mis trabajos anteriores, cuando trabajaba en una gran empresa de tecnolog칤a, era responsable del almac칠n de datos de los usuarios de la empresa, por lo que cada dato que afectaba a un usuario individual deb칤a entrar en mi almac칠n de datos, lo que creaba mucho valor para la empresa. Pero ese trabajo intelectual para dise침ar la base de datos, el esquema de la base de datos, el sistema de ingesta, los datos para mantenerla, result칩 ser una tarea bastante ingente. Es una empresa enorme. Siempre he descubierto que cuando hablamos de ingerir datos de los sistemas fuente, esto lo es todo. Si no puede obtener los datos, no hay mucho m치s que pueda hacer con ellos. Esto deber칤a parecer bastante simple, pero como usted se침ala, parece que, muchas veces, se ignora la ingesti칩n o se centra en otras cosas. As칤 que esto es algo fundamental que debes corregir. Si no puede obtener los datos, realmente no puede hacer nada m치s. En esta parte del curso, sin duda, hablaremos sobre la comprensi칩n de los sistemas de origen de los que va a obtener sus datos y las diferentes formas de ingerirlos. Adem치s de las diferentes formas de organizar estos flujos de trabajo de canalizaci칩n de datos y supervisarlos para garantizar que se preserva la calidad de los datos, as칤 como la ingesti칩n, el rendimiento y otras caracter칤sticas, etc., son muy importantes para su trabajo como ingeniero de datos. De hecho, creo que esto es v치lido para muchos flujos de trabajo de datos diferentes, desde los datos de las tablas de estructura hasta los datos estructurados, como textos e im치genes, a medida que la palabra procesa m치s datos estructurados. Esto parece haber permanecido igual. De hecho, incluso cuando hablo con mis amigos que entrenan a los grandes modelos ling칲칤sticos y cuando se trata de liderar equipos de IA, gran parte del tiempo, no todo, pero s칤 mucho tiempo, lo dedico a pensar en los datos. Tambi칠n hay algo relacionado con el entrenamiento de modelos, pero la ingesta de datos ocupa mucho tiempo para todas estas cargas de trabajo de IA. Es muy interesante. Supongo que lo que ven en t칠rminos de las complejidades de la ingesta de datos, porque me imagino que eso funciona a una escala enorme. De hecho, hay muchos datos de Internet. Cosas como Common Crawl contienen muchos datos. Pero teniendo en cuenta los datos, 쯖칩mo se ingieren, se procesan y se filtran para que sean de alta calidad? Adem치s, si tiene lagunas en los datos, si observa que su modelo no tiene 칠xito en estos temas, 쯖칩mo puede averiguar en qu칠 lugares no le va tan bien y en qu칠 parte de la Tierra va a obtener datos, si es que existen para llenar esos vac칤os? Por lo tanto, estas son muchas de las cosas intelectualmente desafiantes por las que la gente que se entrena sobre el terreno, incluso algunos de los principales OM, MM, grandes modelos multimodales y otros grandes modelos b치sicos, dedican tiempo a preocuparse. Eso es fascinante. Una de las cosas de las que hablamos en este curso y que aprender치s es la ingesti칩n de datos de varios tipos de sistemas de origen. Obviamente, vamos a hablar de la ingesta de datos tabulares, pero hoy en d칤a ese es un subconjunto muy peque침o del universo total de datos. Cuando hablamos de conjuntos de datos no estructurados, desde texto hasta im치genes y v칤deos, este universo se est치 convirtiendo cada vez m치s en un universo mucho m치s grande. Yo dir칤a que tradicionalmente hemos utilizado o pensado en el mundo de los datos. Una de las cosas que trataremos en este curso, una vez m치s, no es solo los conjuntos de datos estructurados de las bases de datos, sino que tambi칠n comenzaremos a trabajar con texto , datos de im치genes, etc. Esto lo ayudar치 a prepararse como ingeniero de datos, no solo para las cargas de trabajo actuales, tal vez si trabaja en un almac칠n de datos, sino tambi칠n para las cargas de trabajo del futuro. La mayor parte del valor de los datos probablemente haya sido informaci칩n estructurada hasta este momento. Sin embargo, a medida que crezca nuestra capacidad para procesar datos no estructurados, veremos si eso cambia. Quiz치 ya est칠 cambiando. El volumen de datos no estructurados en el mundo es mucho mayor que el volumen de datos estructurados en el mundo. Creo que esto supondr칤a a칰n m치s desaf칤os e incluso m치s puestos de trabajo para los ingenieros de datos. Exactamente. Esperamos que esta sea una introducci칩n muy interesante al ciclo de vida de la ingenier칤a de datos. Una vez m치s, la ingesti칩n de los sistemas de origen , la organizaci칩n de estas cargas de trabajo y la supervisi칩n de estas cargas de trabajo son fundamentales para su trabajo como ingeniero de datos. Una vez m치s, como se침al칩 Andrew, solo se volver치 m치s interesante, m치s emocionante y mucho m치s grande. Muchas cosas interesantes. Pasemos al siguiente v칤deo para profundizar en todos estos temas.

Programa
Estado: Traducido autom치ticamente del Ingl칠s
Traducido autom치ticamente del Ingl칠s
Informaci칩n:
Este elemento incluye contenido que a칰n no se tradujo a tu idioma preferido.
쮻e qu칠 trata este programa?

Este programa fue dise침ado por Joe Reis en colaboraci칩n con DeepLearning.IA y AWS para cubrir los fundamentos de la ingenier칤a de datos, tanto en t칠rminos de la teor칤a subyacente y marcos para pensar como un ingeniero de datos, as칤 como habilidades pr치cticas para la construcci칩n de soluciones de ingenier칤a de datos en la nube.
쮸 qui칠n va dirigido este programa?

Este programa est치 dise침ado para cualquier persona interesada en seguir una carrera en o adyacente a la ingenier칤a de datos. Puede que seas estudiante o que ya trabajes profesionalmente en un campo relacionado con los datos. En cualquiera de los casos, le interesa adquirir habilidades y conocimientos de ingenier칤a de datos para respaldar sus objetivos profesionales. Incluso si ya est치 trabajando como ingeniero de datos, encontrar치 valor en la combinaci칩n de conocimientos te칩ricos y aplicaciones t칠cnicas que se presenta aqu칤. 
쯈u칠 conocimientos previos necesito para realizar este curso?

    Se requierenconocimientos intermedios de programaci칩n en Python, incluida la familiaridad con la sintaxis, las estructuras de datos, las funciones y las clases de Python. Tambi칠n puede ser 칰til estar familiarizado con los dataframes de Pandas, aunque no es obligatorio. Para aprender los conceptos b치sicos de Pandas, recomendamos los tutoriales de Pandas de la Escuela W3

 o los tutoriales de Pandas de Kaggle

. 

Tambi칠n puede ser 칰tilestar familiarizado con SQL , pero no es necesario. No dudes en consultar el curso SQLBolt Tutorials

 si quieres aprender los fundamentos de SQL.

Lafamiliaridad b치sica con los fundamentos t칠cnicos de la nube de AWS ser치 칰til pero no necesaria. Para aprender los fundamentos de AWS recomendamos los cursos AWS Cloud Practitioner Essentials
 y AWS Cloud Technical Essentials

    .

쯈u칠 tiene de especial este programa?

    Este programa leense침a a pensar como un ingeniero de datos
    Este programa le ense침ar치 a pensar como un ingeniero de datos a la hora de dise침ar, crear y mantener sistemas que toman datos sin procesar, los convierten en algo 칰til y los ponen a disposici칩n de las partes interesadas. No se trata s칩lo de herramientas y tecnolog칤as En primer lugar, aprender치 a recopilar las necesidades de las partes interesadas y a comprender los problemas empresariales que intentan resolver con los datos. A continuaci칩n, traducir치 esas necesidades en requisitos del sistema y elegir치 las herramientas y tecnolog칤as adecuadas para las soluciones que pretende crear. Al final de este programa, dispondr치 de un s칩lido marco mental que podr치 aplicar a cualquier proyecto de ingenier칤a de datos.

    Pr치ctica
    Tendr치s muchas oportunidades de practicar la aplicaci칩n del marco mental para pensar como un ingeniero de datos a trav칠s de actividades pr치cticas. Participar치 en conversaciones simuladas con las partes interesadas y se le pedir치 que re칰na requisitos para sus sistemas de datos. Dise침ar치 e implementar치 canalizaciones de datos de streaming y por lotes de extremo a extremo en la nube de AWS, solucionar치 problemas comunes a los que se enfrentan muchos nuevos ingenieros de datos, utilizar치 herramientas populares de c칩digo abierto para orquestar y monitorizar sus canalizaciones de datos, crear치 arquitecturas de almacenamiento de lago de datos y data lakehouse, consultar치, modelar치 y transformar치 sus datos utilizando varios marcos de procesamiento, y servir치 datos a las partes interesadas aguas abajo para casos de uso de an치lisis empresarial y aprendizaje autom치tico. Este programa adopta un enfoque justo a tiempo para presentarle las herramientas y tecnolog칤as que necesitar치 para cada ejercicio, y se le guiar치 a trav칠s de cada paso de los laboratorios con instrucciones detalladas y gu칤as en v칤deo.

Libro de texto y lecturas

    Fundamentos de la ingenier칤a de datos

    A lo largo del curso se proporcionar치 material de lectura complementario

Esquema del programa

Este programa est치 estructurado en 4 cursos.

Curso 1 - Introducci칩n a la ingenier칤a de datos

Este curso consta de 4 semanas de contenido y cubre estos objetivos principales de aprendizaje:

    Identificar los principales colaboradores y partes interesadas para los ingenieros de datos

    Articular un marco mental para construir soluciones de ingenier칤a de datos

    Identificar algunas de las consideraciones necesarias para la recopilaci칩n de requisitos al inicio de un nuevo proyecto

    Describir la estructura del ciclo de vida de la ingenier칤a de datos y sus corrientes subyacentes, y c칩mo pensar en los problemas de ingenier칤a de datos a trav칠s de esta lente

    Identificar algunas de las tecnolog칤as clave que pueden emplearse en las distintas fases del ciclo de vida de la ingenier칤a de datos

    Evaluar tecnolog칤as y herramientas en el contexto de los requisitos y una buena arquitectura de datos

    Dise침ar una arquitectura de datos en AWS basada en los requisitos de las partes interesadas

    Implementar una canalizaci칩n por lotes y de streaming en AWS para respaldar un sistema de recomendaci칩n de productos

Curso 2 - Sistemas de origen, ingesti칩n de datos y canalizaciones

Este curso consta de 4 semanas de contenido y cubre estos objetivos de aprendizaje principales:

    Identificar diferentes formatos de datos y determinar los sistemas de origen apropiados para generar cada tipo de datos

    Explicar a alto nivel c칩mo se generan, almacenan y recuperan los datos en varios sistemas fuente, incluyendo bases de datos relacionales, bases de datos NoSQL, almacenamiento de objetos y sistemas de streaming

    Explicar los fundamentos de las redes en la nube

    Solucionar errores de conexi칩n a bases de datos

    Explicar la diferencia entre las ingestas por lotes y por flujo e identificar casos de uso para cada patr칩n

    Diferenciar entre los dos patrones de ingesta por lotes: Extracci칩n-Transformaci칩n-Carga (ETL) y Extracci칩n-Carga-Transformaci칩n (ELT)

    Crear un script para ingestar datos desde una API REST

    Describir los componentes b치sicos de una plataforma de flujo de eventos

    Interactuar con una plataforma de transmisi칩n de eventos como sistema de origen y como herramienta de ingesti칩n

    Utilizar Terraform para aprovisionar recursos de AWS para su canalizaci칩n de datos

    Identificar herramientas para monitorizar sus sistemas de datos y la calidad de los datos

    Identificar y monitorizar m칠tricas de calidad de datos relevantes

    Explicar c칩mo se puede aplicar la orquestaci칩n a un canal de datos y enumerar sus beneficios

    Construir canalizaciones de datos con DAGs en Airflow utilizando funciones como Taskflow API, operadores, variables XCom, etc.

Curso 3 - Almacenamiento de datos y consultas

Este curso consta de 3 semanas de contenido y cubre estos objetivos de aprendizaje principales:

    Explicar c칩mo se almacenan f칤sicamente los datos en el disco y en la memoria

    Comparar c칩mo se almacenan y consultan los datos en sistemas de almacenamiento de objetos, bloques y archivos

    Explicar c칩mo se almacenan los datos en bases de datos orientadas a filas frente a bases de datos orientadas a columnas

    Explicar c칩mo almacenan y recuperan datos las bases de datos gr치ficas y vectoriales

    Explicar las caracter칤sticas arquitect칩nicas clave de los almacenes de datos, los lagos de datos y los data lakehouses

    Implementar un lago de datos con AWS Glue

    Implementar un lago de datos con una arquitectura tipo medall칩n utilizando Lake Formation e Iceberg

    Explicar las etapas de la vida de una consulta

    Implementar consultas SQL avanzadas

    Explicar el papel de un 칤ndice y su impacto en el rendimiento de la consulta

    Resumir los enfoques para procesar consultas agregadas y join

    Comparar los tiempos de ejecuci칩n de consultas agregadas entre almacenamiento en filas y en columnas

    Enumerar algunas estrategias para mejorar el rendimiento de las consultas

    Agregaci칩n y uni칩n de flujos de datos

Curso 4 - Modelamiento de datos, Transformaci칩n y Servicio de Datos

Este curso consta de 4 semanas de contenido y cubre estos objetivos principales de aprendizaje:

    Definir el Modelamiento de datos y su rol en reflejar la l칩gica del negocio

    Aplicar las etapas de normalizaci칩n a una tabla desnormalizada

    Describir las tablas de hechos y dimensiones de un esquema en estrella y transformar datos en tercera forma normal a un esquema en estrella

    Describir los enfoques de modelado de almacenes de datos como Inmon, Kimball, Data Vault y One Big Table

    Utilizar la Ingenier칤a de caracter칤sticas para convertir un conjunto de datos en una forma tabular esperada por los algoritmos cl치sicos de Aprendizaje autom치tico

    Preprocesar y vectorizar datos textuales

    Enumerar t칠cnicas para procesar y aumentar los datos de im치genes

    Comparar un marco de procesamiento en memoria como Spark y un marco de procesamiento basado en disco como Hadoop

    Describir las consideraciones t칠cnicas para elegir un marco de procesamiento distribuido, como Spark, frente a un marco no distribuido como Pandas dataframes

    Describir las consideraciones t칠cnicas para utilizar Spark SQL frente a Spark DataFrames al transformar datos utilizando PySpark

    Describir c칩mo funciona la transformaci칩n de flujo con un motor de procesamiento casi en tiempo real como Spark Structured Streaming

    Identificar diferentes formas de servir datos para casos de uso de anal칤tica y aprendizaje autom치tico

    Describir el prop칩sito de una capa sem치ntica que se asienta sobre el modelo de datos

    Creaci칩n de vistas y vistas materializadas

    Describir los beneficios e inconvenientes de servir datos utilizando vistas y vistas materializadas

Actividades de aprendizaje utilizadas en este programa

    V칤deos de conferencias: Una colecci칩n de videos cortos que cubren la teor칤a subyacente, as칤 como demostraciones de herramientas y tecnolog칤as importantes que necesita para cada semana. Tambi칠n hay v칤deos "Lab Walkthrough" que le dan una visi칩n general de alto nivel de los laboratorios antes de sumergirse en ellos. Algunos de los v칤deos est치n etiquetados como "[Opcional]", y est치n dise침ados para complementar su experiencia de aprendizaje, pero no ser치 evaluado en este contenido. Algunos de estos v칤deos opcionales est치n protagonizados por expertos del sector y su objetivo es proporcionarle informaci칩n pr치ctica de veteranos en el campo de opciones.

    Laboratorios: Ejercicios pr치cticos que le permitir치n aplicar lo aprendido en los v칤deos de las clases. Est치n dise침ados para ayudarle a desarrollar habilidades para determinadas tecnolog칤as de c칩digo abierto o AWS que se utilizan habitualmente al crear soluciones de ingenier칤a de datos. Hay dos tipos de laboratorios:

        Tareas de programaci칩n calificadas: Estas tareas de laboratorio cubren conceptos cr칤ticos para esa semana, y por lo general representan un mayor porcentaje de su calificaci칩n.

        Laboratorios de pr치ctica: Estos laboratorios no se califican. Los conceptos cubiertos en estos laboratorios siguen siendo importantes. Sin embargo, para reducir la presi칩n de tener que sobresalir en todos los laboratorios, hemos decidido designar algunos de ellos como "pr치cticas". De esta manera, usted puede centrarse en el aprendizaje de los laboratorios en lugar de simplemente completarlos para obtener una calificaci칩n. Te animamos a que pruebes todos los laboratorios, tanto si se califican como si son pr치cticos

    Cuestionarios: Una colecci칩n de preguntas para ayudarle a reforzar su aprendizaje sobre los conceptos cubiertos en cada semana. Encontrar치 un cuestionario calificado al final de cada semana, y la calificaci칩n que obtenga en esos cuestionarios contribuir치 a su calificaci칩n general para cada curso. Ocasionalmente encontrar치 cuestionarios de pr치ctica que contienen preguntas de reflexi칩n o breves cuestionarios calificados que contienen preguntas para comprobar su comprensi칩n a lo largo de la semana. Despu칠s de completar cada cuestionario, aseg칰rese de leer atentamente los comentarios.

    Lectura: Contenido presentado en formato textual para que pueda consultar la informaci칩n m치s adelante con mayor facilidad. Algunos de estos elementos de lectura incluir치n enlaces adicionales para que pueda obtener m치s informaci칩n sobre el tema. A menos que se especifique lo contrario, no es necesario que revise los materiales de estos enlaces externos para tener 칠xito en este programa. Algunas de estas lecturas est치n etiquetadas como "[Opcional]" y cubren material secundario que no es cr칤tico para el programa. Usted puede revisar estas lecturas si est치 interesado, pero no ser치 evaluado en este contenido.

쮺칩mo se califican las evaluaciones?

Se requiere una calificaci칩n de 80% o m치s para aprobar todos los elementos de evaluaci칩n (cuestionarios, laboratorios de pr치ctica y tareas de programaci칩n calificadas). Por favor, consulte la pesta침a"Calificaciones" de la p치gina principal del curso para ver sus calificaciones para cada evaluaci칩n.
Obtener y dar ayuda

Preguntas relacionadas con el contenido del curso: Se le anima a unirse a la comunidad DeepLearning.IA donde se puede llegar a los mentores del curso y compa침eros de clase para obtener ayuda con cualquier problema relacionado con el contenido del curso. 춰Puedes hacer clic 游댕 este enlace

 para crear tu cuenta gratuita y conectarte con la comunidad global de IA!

Preguntas relacionadas con la plataforma Coursera: Puedes consultar el Centro de ayuda al alumno
 para encontrar informaci칩n relacionada con problemas t칠cnicos espec칤ficos. Por ejemplo, los problemas t칠cnicos incluir칤an mensajes de error, dificultad para enviar tareas o problemas con la reproducci칩n de v칤deos.

Bienvenido a la primera semana de este curso sobre sistemas de origen, ingesti칩n y canalizaciones de datos. Esta semana, vamos a empezar por analizar los diferentes tipos de sistemas fuente y c칩mo puedes interactuar con estos sistemas. Como vio en el primer curso de la especializaci칩n, la generaci칩n de datos en los sistemas fuente es la primera etapa del ciclo de vida de la ingenier칤a de datos. Y como ingeniero de datos, normalmente no es responsable de generar estos datos usted mismo ni de mantener estos sistemas de origen. Sin embargo, la ingesti칩n desde los sistemas fuente es donde comenzar치n todas sus canalizaciones de datos. Por lo tanto, es importante que comprenda c칩mo se generan estos datos, d칩nde y c칩mo se almacenan. Y algunas de sus caracter칤sticas permiten crear canalizaciones de datos s칩lidas con estos sistemas ascendentes como fuente de datos. Por eso, en esta primera semana de este curso, profundizaremos en algunos de los detalles de algunos sistemas fuente comunes, incluidos los diferentes tipos de bases de datos, almacenamiento de objetos y fuentes de streaming. En los laboratorios, podr치 trabajar con estos sistemas fuente en AWS. Luego, en la segunda semana, nos centraremos en configurar diferentes tipos de ingesti칩n desde los sistemas fuente. Despu칠s de eso, en la semana 3 de este curso, analizaremos las DataOps actuales. Utilizar치 la infraestructura como c칩digo para automatizar algunas de las tareas de canalizaci칩n y utilizar치 varias herramientas para supervisar la calidad de los datos. Y, por 칰ltimo, en la cuarta y 칰ltima semana del curso, nos ocuparemos de la organizaci칩n para coordinar las tareas de sus canalizaciones de datos. Configurar치 gr치ficos ac칤clicos dirigidos, o DAG, mediante el flujo de aire, trabajar치 con la infraestructura como marcos de c칩digo e implementar치 soluciones de monitoreo para sus canalizaciones de datos. As칤 que vamos a cubrir mucho terreno en este curso. Acomp치침eme en el siguiente v칤deo para empezar a analizar m치s de cerca los diferentes tipos de sistemas fuente.

Los sistemas fuente espec칤ficos con los que trabajar치 como ingeniero de datos suelen variar seg칰n el tipo de datos que ingiera de esos sistemas. El tipo de datos m치s com칰n con el que trabajar치 son los datos estructurados, es decir, los datos organizados como tablas de filas y columnas. Lo m치s probable es que hayas trabajado con datos estructurados en el pasado, ya sea en una hoja de c치lculo o en una base de datos relacional, o tal vez incluso hayas usado Python para leer un archivo CSV. Los otros tipos de datos que encontrar치 como ingeniero de datos son los datos semiestructurados y no estructurados. Los datos semiestructurados son datos que no est치n en forma tabular, por lo que no se componen de filas y columnas, pero a칰n tienen cierta estructura. Un formato de datos semiestructurados com칰n con el que te encontrar치s es lo que se conoce como notaci칩n de objetos de JavaScript o JSON. Un archivo JSON contiene una serie de pares clave-valor. En este ejemplo, la primera clave es FirstName con el valor correspondiente Joe. La siguiente clave es LastName con el valor correspondiente Reis. Y con el formato JSON, todos estos pares clave-valor se enumeran entre llaves como este. Y puedo tener m치s pares clave-valor, incluida la informaci칩n que quiera registrar en este formato. Cada valor puede adoptar un tipo de datos diferente, por ejemplo, un n칰mero, una cadena o una matriz. De hecho, el valor de un par clave-valor puede ser incluso otra serie de pares clave-valor. Y como puedes ver en este ejemplo, el valor de direcci칩n tiene una clave: ciudad , c칩digo postal y pa칤s con los valores correspondientes. Esto crea lo que se denomina un formato JSON anidado. Por lo tanto, como puede ver, aunque los datos no se presenten como una tabla, estos datos todav칤a tienen cierta estructura. Los datos no estructurados, por otro lado, no tienen una estructura predefinida. Por ejemplo, el texto, el v칤deo, el audio y las im치genes son ejemplos de datos no estructurados. Sin embargo, observar치 que cosas como el v칤deo, el audio y las im치genes tienen una estructura inherente detr치s de escena, en el sentido de que hay dimensiones de p칤xeles y colores como el rojo, el azul y el verde. A lo largo de este curso, profundizaremos en los datos no estructurados. Cuando se trata de ingerir estos diferentes tipos de datos, me gustar칤a clasificar los sistemas fuente relevantes que puede encontrar en tres tipos generales: bases de datos, archivos y sistemas de transmisi칩n. Si bien estos tres tipos de sistemas fuente no se corresponden necesariamente uno a uno con los tres tipos de datos que acabo de mencionar, se podr칤a decir que de las bases de datos la mayor칤a de las veces se ingieren datos estructurados y semiestructurados. Desde los sistemas de streaming, con frecuencia ingerir치s mensajes semiestructurados en formato de datos. Y los archivos, bueno, los archivos pueden ser cualquier cosa, desde texto hasta im치genes , audio, v칤deo o incluso filas y columnas antiguas normales de datos tabulares. Empecemos por echar un vistazo m치s de cerca a las bases de datos. Las bases de datos almacenan la informaci칩n de forma organizada que permite buscar, recuperar, actualizar y eliminar datos. Esto funciona mediante un patr칩n transaccional conocido como CRUD, que significa crear, leer, actualizar y eliminar. La C es lo primero porque, por supuesto, los datos deben crearse antes de poder leerlos, actualizarlos o eliminarlos. Tiene sentido, 쯨erdad? Por lo general, hay una interfaz de software llamada sistema de administraci칩n de bases de datos, o DBMS, que se encuentra entre el almacenamiento de la base de datos f칤sica y la persona o la aplicaci칩n que interact칰a con la base de datos. El DBMS es lo que le permite acceder y manipular los datos almacenados en la base de datos. Hay dos tipos de bases de datos que analizaremos esta semana. Bases de datos relacionales, que almacenan informaci칩n en tablas con filas y columnas, y bases de datos no relacionales, tambi칠n conocidas como NoSQL o no solo SQL, que almacenan datos no tabulares. Analizaremos m치s de cerca estos dos tipos de bases de datos a finales de esta semana. Adem치s de las bases de datos, el siguiente tipo de sistema fuente m치s com칰n con el que interactuar치 son los archivos. Sin duda, ya tiene mucha experiencia trabajando con archivos de varios tipos. Pueden ser documentos que almacenas en tu computadora, im치genes o videos que grabas con tu tel칠fono, o incluso un archivo CSV que recibes en un correo electr칩nico de un compa침ero de trabajo. Puede parecer extra침o pensar en los archivos antiguos normales como un sistema fuente para la ingenier칤a de datos, pero en esencia, un archivo es solo una secuencia de bytes que representan informaci칩n. Las aplicaciones de todo tipo escriben datos en los archivos, por lo que los archivos son un medio universal de intercambio de datos. Y aunque no lo crea, son uno de los sistemas fuente m치s comunes con los que trabajar치 como ingeniero de datos. Los archivos, al igual que los datos, pueden estar estructurados como una hoja de c치lculo, semiestructurados como un archivo JSON o XML, o desestructurados como un archivo de texto, imagen, v칤deo o audio. Es posible que recibas estos archivos o accedas a ellos desde un sistema de archivos como Google Drive o un sistema de almacenamiento de objetos como Amazon S3, o simplemente como un archivo adjunto a un correo electr칩nico. El tercer tipo de sistema fuente del que es probable que ingieras datos son los sistemas de streaming. Y puede pensar que los sistemas de streaming proporcionan un flujo continuo de datos, grabados como mensajes que contienen informaci칩n sobre eventos. Y esos eventos incluyen algo que ocurri칩 en el mundo o un cambio en el estado de un sistema. En la pr치ctica, es posible que interact칰es con una secuencia de eventos a trav칠s de colas de mensajes u otras plataformas de transmisi칩n. Por ejemplo, un dispositivo de IoT, como un termostato inteligente, puede registrar un evento que contenga la lectura de temperatura m치s reciente y publicar ese evento como un mensaje en una plataforma de transmisi칩n como Kinesis o Kafka. Luego, como ingeniero de datos, puede configurar otro servicio para incorporar este mensaje y enviar una actualizaci칩n a un panel de an치lisis integrado. En este caso, puedes pensar en la plataforma de streaming como un sistema fuente del que est치s extrayendo datos sin procesar. En las 칰ltimas semanas de este curso, ver치 c칩mo estos sistemas de transmisi칩n tambi칠n pueden abarcar todo el ciclo de vida de la ingenier칤a de datos y usarse en las etapas de ingesti칩n y transformaci칩n para procesar datos para varios casos de uso posteriores. De hecho, puede ver lo mismo en todos los tipos de sistemas fuente, ya sea que se trate de bases de datos, archivos o sistemas de transmisi칩n. Pueden ser sistemas de los que est치 ingiriendo datos sin procesar o pueden ser sistemas que incorpore a sus canalizaciones de datos en otra etapa del ciclo de vida. En resumen, como ingeniero de datos, extraer치 datos sin procesar de diferentes sistemas de origen. Estos datos sin procesar pueden estar estructurados, semiestructurados o no estructurados, y los sistemas fuente pueden ser bases de datos, archivos o sistemas de transmisi칩n. En la pr칩xima serie de v칤deos, profundizar칠 un poco m치s en las caracter칤sticas de cada uno de estos diferentes tipos de sistemas fuente. Empezaremos con las bases de datos, despu칠s veremos c칩mo funciona el almacenamiento de objetos como fuente de datos para los archivos y, a continuaci칩n, profundizaremos en las colas de mensajes, los registros y las plataformas de streaming con m치s detalle. Acomp치침eme en el siguiente v칤deo para empezar a echar un vistazo a las bases de datos relacionales.

Como ingeniero de datos, el tipo de sistema fuente m치s com칰n con el que interactuar치 es una base de datos relacional, y esto se debe a que las bases de datos relacionales est치n en todas partes. Muchas aplicaciones web y m칩viles utilizan bases de datos relacionales en el backend, y usted tambi칠n las encontrar치. En muchos sistemas corporativos, como los sistemas de gesti칩n de relaciones con los clientes , recursos humanos y planificaci칩n de recursos empresariales, tambi칠n se suelen utilizar para lo que se denomina procesamiento de transacciones en l칤nea o sistemas OLTP, en los que es necesario ejecutar un gran volumen de transacciones de forma simult치nea, como en el caso de las reservas bancarias o en l칤nea. El nombre base de datos relacional proviene del hecho de que este tipo de base de datos se usa con mayor frecuencia para almacenar datos en diferentes tablas que est치n relacionadas entre s칤 a trav칠s de un conjunto de claves o atributos comunes. Estas tablas suelen organizarse en funci칩n de c칩mo se estructura la informaci칩n en la empresa. Por ejemplo, como ingeniero de datos que trabaja en una empresa de comercio electr칩nico, es posible que est칠 trabajando con una base de datos relacional en la que una tabla captura la informaci칩n de los clientes, otra tabla captura la informaci칩n de los productos y una tercera tabla captura la informaci칩n de los pedidos. Estructurar una base de datos de esta manera reduce la redundancia y facilita la administraci칩n de los datos al no tener la misma informaci칩n duplicada en varias filas o tablas de la base de datos. Para tener una idea de lo que quiero decir con eso, imagine por un momento que, en lugar de varias tablas, cre칩 una tabla grande para almacenar los datos de cada pedido individual de un cliente. En este caso, la tabla puede contener un gran n칰mero de columnas, incluyendo todo lo relacionado con el cliente, como su nombre, direcci칩n, n칰mero de tel칠fono, etc. Y luego tendr칤as toda la informaci칩n sobre el producto que compraron, como la marca, el n칰mero de SKU, la descripci칩n del producto y otros detalles, as칤 como los detalles del pedido, como la fecha y la hora, el importe de la compra y cu치nto pagaron. Por lo tanto, en este escenario, si un cliente hiciera un pedido de tres productos diferentes, esto se registrar칤a como tres filas independientes en tu base de datos y tendr칤as tres filas en la tabla que contienen todos los mismos datos del cliente. O si tres clientes diferentes compraron el mismo producto, entonces tendr칤as tres filas en la base de datos que contienen informaci칩n id칠ntica para ese producto. En resumen, la informaci칩n se duplicar칤a en varias filas de la tabla y, adem치s, podr칤a haber incoherencias con los datos de las distintas filas. Por ejemplo, si un cliente cambiara su direcci칩n, habr칤a una incoherencia entre las filas, a menos que volvieras y actualizaras todas las filas que contienen su direcci칩n anterior, o si cambiaran los detalles de un producto en particular, tendr칤as que volver y actualizar todas las filas que contienen la informaci칩n anterior. Si, en cambio, separas la informaci칩n de los clientes , los productos y los pedidos en varias tablas, una fila de la tabla de clientes representa a un solo cliente y una fila de la tabla de productos contiene informaci칩n sobre un solo producto. Si un cliente cambia su direcci칩n o cambian los detalles de un producto, solo tienes que actualizar la 칰nica fila que contiene la informaci칩n sobre ese cliente o producto. La forma en que una base de datos se organiza en tablas relacionadas como esta se denomina esquema de base de datos. Las bases de datos relacionales representan estas relaciones entre tablas mediante el uso de claves. Una clave principal es una columna especial o un conjunto de columnas que identifican de forma exclusiva cada fila de una tabla. Para la tabla del cliente, la clave principal podr칤a ser esta columna denominada Id. La relaci칩n entre el cliente y el pedido se puede establecer haciendo que la columna Customer_id y la tabla de pedidos hagan referencia a la columna id de la tabla de clientes. En este caso, la columna de identificador de cliente de la tabla de pedidos se denomina clave externa y hace referencia a la columna de clave principal o identificador de la tabla de clientes. M치s all치 de la estructura de filas de una base de datos relacional, cada columna tiene un nombre 칰nico y un tipo de datos espec칤fico. Por ejemplo, en la tabla de clientes puede tener columnas como ID, nombre y apellidos que contienen cadenas, y otra columna llamada edad que contiene un entero. A continuaci칩n, cada nueva fila de una tabla debe seguir la misma estructura de columnas, es decir, la misma secuencia de columnas y tipos de datos. Esto tambi칠n forma parte del esquema de la base de datos. Ahora, en una base de datos con un esquema como este, para registrar informaci칩n sobre un nuevo pedido de un cliente existente, puedes crear un nuevo registro en la tabla de pedidos e indicar el identificador del cliente de la tabla del cliente y el identificador del producto de la tabla de productos y los detalles del pedido, como la fecha, la hora y el pago, etc. Adem치s, si ese cliente cambia su direcci칩n o cambia el n칰mero de SKU del producto que ha pedido, esos cambios solo afectar치n a una fila de la tabla de clientes o productos, y la informaci칩n se mantendr치 coherente. Como puede imaginar, hay muchas maneras diferentes de establecer relaciones entre tablas, y aqu칤 es donde entra en juego el concepto de normalizaci칩n de datos. La normalizaci칩n de datos es un enfoque que se desarroll칩 en la d칠cada de 1970 para minimizar la redundancia y garantizar la integridad de los datos mediante el almacenamiento de datos en tablas de forma l칩gica. Pero ahora creo que vale la pena hacer una pausa para preguntarnos: 쯣or qu칠 preocuparse tanto por la redundancia o la informaci칩n duplicada en primer lugar? Parece l칩gico y ordenado estructurar los datos como lo he estado describiendo. Pero, 쯛ay alg칰n inconveniente? Bueno, resulta que, si bien una estructura de base de datos relacional normalizada proporciona un alto grado de integridad y minimiza la redundancia, en realidad puede resultar lenta a la hora de consultar los datos. Hoy en d칤a, el almacenamiento es relativamente barato y la velocidad suele ser fundamental. La integridad de los datos es fundamental, por supuesto, pero la respuesta a la forma exacta en que se almacenan los datos tabulares podr칤a depender del objetivo para el que se est칠 intentando optimizar. Como ingeniero de datos, es posible que est칠 ingiriendo datos normalizados de un sistema de base de datos relacional, pero seg칰n el caso de uso final que est칠 atendiendo, puede decidir organizar los datos seg칰n un modelo diferente en sus propios sistemas de almacenamiento. Hoy en d칤a, incluso hay algunos casos de uso en los que los ingenieros de datos eligen adoptar el denominado enfoque de una gran tabla u OBT, en el que todos los datos se registran en una sola tabla para un procesamiento m치s r치pido de lo que ser칤a posible si se unieran varias tablas en una base de datos relacional tradicional. Profundizaremos en los detalles del modelado de datos en el Curso 4, especializaci칩n. Cuando se trata de interactuar con la base de datos, utilizar치 un sistema de administraci칩n de bases de datos relacionales (RDBMS). Es una capa de software que se encuentra en la parte superior de una base de datos relacional. Existen muchos RDBM populares, incluidos MySQL, PostgreSQL, Oracle y SQL Server. La mayor칤a de los RDBMS admiten el lenguaje de consulta estructurado, tambi칠n conocido como secuela o SQL para abreviar. Algunas personas dicen SQL, otras dicen SQL. Puedes elegir lo que prefieras. A veces digo ambas cosas. Lo importante que debe saber es que SQL proporciona un conjunto de comandos para realizar diversas operaciones en las bases de datos relacionales. Y como ingeniero de datos, SQL formar치 parte de su trabajo diario. En el siguiente v칤deo, te explicar칠 algunos de los comandos SQL que necesitar치s para el laboratorio. Luego, en el laboratorio, tendr치 la oportunidad de practicar la consulta de datos en una base de datos relacional mediante consultas SQL. Despu칠s de eso, 칰nase a m칤 en el siguiente v칤deo, eche un vistazo a las bases de datos NoSQL.

En el laboratorio, trabajar치 con una base de datos transaccional para una empresa ficticia de alquiler de DVD llamada Rentio. Esta base de datos incluye tablas que contienen informaci칩n sobre las tiendas, el personal, los clientes, el inventario de DVD y las transacciones de alquiler de Rentio. Escribir치s sentencias o consultas SQL en un Jupyter Notebook para recuperar informaci칩n de esta base de datos con el fin de responder a preguntas empresariales. Para consultar los datos, tendr치 que entender el esquema de la base de datos. En otras palabras, tendr치 que saber los nombres de las tablas, las columnas que contienen y c칩mo se relacionan las tablas entre s칤 a trav칠s de claves principales y externas. Esta base de datos est치 normalizada, lo que significa que los datos, como las direcciones de las tiendas, el personal y los clientes, se almacenan en tablas separadas para reducir la redundancia y facilitar la actualizaci칩n de los datos cuando cambian. Del mismo modo, los datos sobre las pel칤culas y las propias transacciones de alquiler tambi칠n se almacenan en tablas separadas. Puede consultar este modelo de relaciones entre entidades que muestra las relaciones y los atributos de las tablas de la base de datos de Rentio. Para explicarte los conceptos b치sicos de SQL, solo me centrar칠 en tres tablas. La tabla de pel칤culas que contiene informaci칩n como el t칤tulo y la duraci칩n de una lista de pel칤culas, la tabla de categor칤as que contiene una lista de categor칤as de pel칤culas y la tabla film_category que muestra los films_id, junto con sus correspondientes category_id de pel칤cula. La sentencia SQL m치s b치sica comienza con una cl치usula SELECT, en la que se especifican los datos que se desean, seguida de una cl치usula FROM, en la que se especifica de qu칠 tabla se desean recuperar estos datos. Por ejemplo, supongamos que quiero explorar los t칤tulos y los a침os de estreno sin tener en cuenta la mesa cinematogr치fica. Puedo escribir SELECT title, release_year FROM film. Una vez que ejecute esta consulta, obtendr칠 una lista de todos los t칤tulos y a침os de lanzamiento. Resulta que hay 1000 pel칤culas en esta mesa de filmaci칩n. Si no quiero ver la lista completa, puedo limitar el n칰mero de resultados devueltos mediante una cl치usula LIMIT. Por ejemplo, si a침ado LIMIT 10 al final de esta consulta, solo obtendr칠 los 10 primeros t칤tulos y los a침os de estreno de la tabla de pel칤culas. Pero, 쯤u칠 sucede si desea recuperar datos de todas las columnas de una tabla? Bueno, puedes enumerar todos los nombres de las columnas en la cl치usula SELECT de esta manera, o puedes usar un atajo y escribir SELECT * FROM film. Esto recuperar치 los datos de todas las filas y columnas de la tabla de pel칤culas. En el pr칩ximo curso, aprender치 c칩mo se ejecutan las canteras entre bastidores. Pero por ahora, ten en cuenta que recuperar todos los datos de todas las columnas puede requerir muchos recursos de procesamiento, especialmente si tu conjunto de datos es muy grande. Recomiendo usar SELECT * 칰nicamente para recuperar todos los datos de una tabla, donde puede filtrar los resultados con alguna condici칩n booleana. Por ejemplo, supongamos que solo te interesa explorar pel칤culas de menos de 60 minutos de duraci칩n. Puede a침adir una cl치usula WHERE despu칠s de la cl치usula FROM para filtrar los resultados en funci칩n de la longitud de la columna. Escribir칠 SELECT * FROM pel칤cula cuya duraci칩n sea inferior a 60. Esto devolver치 una lista de las 96 pel칤culas que tienen menos de una hora de duraci칩n. Tambi칠n puedes ordenar los resultados por cualquier columna que desees. Por ejemplo, puedo a침adir ORDER BY length despu칠s de la cl치usula WHERE para almacenar los resultados en orden ascendente seg칰n la duraci칩n de la pel칤cula. Si quiero ver los resultados en orden descendente por duraci칩n de la pel칤cula, puedo a침adir la palabra clave D-E-S-C o DESC al final de la cl치usula ORDER BY. Si quisiera limitar los resultados a, digamos, 10 registros, puedo agregar un L칈MITE 10 al final de esta consulta. Acabas de ver c칩mo puedes recuperar datos de una sola tabla usando las cl치usulas SELECT, FROM, WHERE, ORDER BY y LIMIT. 쯈u칠 sucede si desea explorar los datos de m치s de una tabla? Puede usar la cl치usula JOIN para combinar registros de dos o m치s tablas en funci칩n de las columnas compartidas entre esas tablas. Por ejemplo, supongamos que quiero obtener una lista de t칤tulos de pel칤culas y su correspondiente categor칤a_pel칤cula para todas las pel칤culas de menos de 60 minutos de duraci칩n. Modifiquemos la consulta anterior, donde seleccion칠 una estrella de una pel칤cula cuya longitud es inferior a 60. Quiero combinar las filas de la tabla de pel칤culas con las filas de la tabla film_category en funci칩n de los identificadores de pel칤culas coincidentes. Al final de la cl치usula FROM, escribir칠 JOIN film_category ON film.film_id = film_category.film_id. De esta forma, los resultados devueltos incluir치n todas las columnas de la tabla de pel칤culas, junto con las columnas de la tabla film_category para cada par de film_id coincidentes en ambas tablas. Tenga en cuenta que la tabla film_category solo incluye el category_id, pero no el nombre de la categor칤a. Por lo tanto, necesitamos hacer otra combinaci칩n para combinar estos registros con las filas de la tabla de categor칤as en funci칩n de los category_id. A침adir칠 la categor칤a JOIN ON film_category.category _id = category.category_id. Ahora los resultados incluir치n todas las columnas de la tabla de pel칤culas, todas las columnas de la tabla film_category y todas las columnas de la tabla de categor칤as. Como solo quiero el t칤tulo de la pel칤cula y la categor칤a de pel칤cula correspondiente, puedo modificar la instrucci칩n SELECT para especificar que solo quiero la columna film.title en la columna category.name. Tenga en cuenta que, de forma predeterminada, la cl치usula JOIN combina solo los registros de ambas tablas que tienen un valor de columna coincidente especificado en la instrucci칩n ON. No incluir치 ning칰n registro de ninguna de las tablas que no tenga valores coincidentes. Por ejemplo, si la tabla de pel칤culas tiene una fila con film_id que no aparece en la tabla film_category, esa fila no se incluir치 en los resultados. Este tipo de uni칩n tambi칠n se conoce como INNER JOIN, y puede pensar en los resultados de la uni칩n como la parte central superpuesta de un diagrama de Venn. Los otros tipos de combinaciones incluyen LEFT JOIN, que devuelve todos los registros de la primera tabla, junto con los registros coincidentes de la segunda tabla, RIGHT JOIN, que devuelve todos los registros de la segunda tabla, junto con los registros coincidentes de la primera tabla, y FULL JOIN, que devuelve todos los registros de ambas tablas y combina los que tienen valores coincidentes. Volviendo a los resultados de la 칰ltima consulta, puedo ver que bastantes de los cortometrajes pertenecen a la categor칤a infantil o documental. Digamos que quiero saber con certeza cu치l es la categor칤a m치s popular de cortometrajes. Puedo usar el comando GROUP BY para agrupar las filas seg칰n la categor칤a_pel칤cula. A continuaci칩n, utilice el comando COUNT para contar el n칰mero de registros de cada una de las categor칤as de pel칤culas. El comando GROUP BY se escribe despu칠s de la cl치usula WHERE. Aqu칤 a침adir칠 GRUPO POR CATEGOR칈A.nombre. Luego modificar칠 la instrucci칩n SELECT para seleccionar category.name y COUNT (*), que cuenta todas las filas de cada categor칤a. Tambi칠n usar칠 el comando AS para dar a la salida de este recuento el nombre de alias film_count. Por 칰ltimo, ordenar칠 los resultados seg칰n el recuento de pel칤culas en orden descendente. Como puede ver, la categor칤a m치s popular para cortometrajes de menos de una hora es la de documental, seguida de acci칩n y luego de ni침os. Estos son algunos de los comandos SQL m치s comunes. Ahora est치 listo para empezar con el laboratorio. El laboratorio tambi칠n cubre algunas operaciones de manipulaci칩n de datos, como CREATE, INSERT INTO, UPDATE y DELETE. Aseg칰rese de leer las instrucciones detenidamente cuando intente cada uno de los ejercicios. Cuando termine el laboratorio, 칰nase a m칤 para echar un vistazo a las bases de datos NoSQL.

Pautas antes de empezar los laboratorios de este curso
Estado: Traducido autom치ticamente del Ingl칠s
Traducido autom치ticamente del Ingl칠s
Informaci칩n:
Este elemento incluye contenido que a칰n no se tradujo a tu idioma preferido.

Por favor, consulte las siguientes indicaciones importantes al iniciar cualquier laboratorio en este curso.

    No utilice una cuenta personal de AWS. Los laboratorios ser치n aprovisionados para usted sin costo alguno en este curso. Si tiene una cuenta personal de AWS, aseg칰rese de cerrar la sesi칩n de esa cuenta antes de comenzar los laboratorios para evitar que se le cobren los servicios de AWS utilizados durante los laboratorios.

    En la medida de lo posible, evite utilizar ordenadores port치tiles proporcionados por la empresa o conectarse a Internet en su lugar de trabajo para trabajar en los laboratorios. Estos pueden tener configuraciones de seguridad que inadvertidamente bloqueen el tr치fico a los recursos que usted utilizar치. Varios alumnos han informado de problemas que no hemos podido reproducir, y la soluci칩n ha sido simplemente cambiar de un port치til de trabajo a un dispositivo personal.

    Si encuentras alg칰n problema con los laboratorios, puedes buscar o crear un tema en esta categor칤a

 en nuestro Foro de la Comunidad. Nuestro personal y nuestros mentores estar치n encantados de ayudarte. 쮸칰n no eres miembro? Puedes hacer clic en este enlace

     para crear tu cuenta gratuita. Si no puedes acceder al laboratorio o a sus recursos, consulta el p치rrafo siguiente.

    Por favor, evita utilizar la opci칩n "Informar de un problema" de Coursera en la parte inferior de la p치gina cuando tengas problemas con los laboratorios. Esto no se controla con tanta frecuencia como el Foro, por lo que la respuesta puede ser m치s lenta.

Problemas para acceder a los laboratorios

Si te encuentras con alguno de los siguientes problemas (enumerados a continuaci칩n) y no puedes acceder al laboratorio, rellena este formulario de google

. Tu cuenta de AWS se actualizar치 en un plazo de 2 d칤as laborables.

Si su problema no es ninguno de los que se indican a continuaci칩n, cree o busque un tema en la plataforma de la Comunidad en lugar de utilizar el formulario (puede hacer clic en este enlace

 para crear su cuenta). Enviar el formulario para otros problemas puede no ayudar a resolverlos.

    El laboratorio se bloquea en la p치gina de carga durante m치s de 20 minutos: intente primero actualizar la p치gina web. Si el problema persiste, rellena el formulario

    .

    El enlace a la consola de AWS est치 vac칤o: En algunos de los laboratorios, se le pedir치 que ejecute este comando para obtener el enlace a la consola de AWS. Si recibes un enlace vac칤o, rellena el formulario

    .

    Si ejecutas un comando en el terminal para obtener el endpoint/direcci칩n de una base de datos u otro recurso y no obtienes ning칰n recurso, por favor rellena el formulario

.

Problemas de formaci칩n de lagos insuficientes (este es un problema temporal en el que estamos trabajando para solucionarlo, pero por ahora rellena el formulario

    ).

Cuando rellenes el formulario, se te guiar치 para que introduzcas la siguiente informaci칩n:

    La direcci칩n de correo electr칩nico de tu cuenta de Coursera y tu nombre de usuario en la comunidad DeepLeaning.IA (Puedes hacer clic en este enlace

     para crear tu cuenta).

    ID del laboratorio: puedes obtener el ID del laboratorio haciendo clic en el signo de interrogaci칩n (ayuda del laboratorio) y luego copiando el ID del laboratorio.

 3.  Si puede acceder al terminal de VS Code, deber치 ejecutar este comando en el terminal:

cat /home/coder/vocapi_logger > vocapi_logger

Este comando crear치 el archivo vocapi_logger que contiene la informaci칩n de registro. Tendr치s que copiar el texto de este archivo y pegarlo en el lugar correspondiente del formulario.



C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "C2W1-2.txt"  
 echo.
) 
A principios de la d칠cada de 2000, gigantes tecnol칩gicos como Google y Amazon comenzaron a superar sus bases de datos relacionales. Necesitaban procesar grandes vol칰menes de datos de fuentes dispares que no encajaban perfectamente en el modelo de base de datos relacional. La aplicaci칩n de estructuras tabulares generar칤a problemas de redundancia de datos y rendimiento a escala, por lo que estas empresas lideraron el desarrollo de nuevas bases de datos no relacionales distribuidas para escalar sus plataformas web. De esta manera, las bases de datos NoSQL se desarrollaron para superar las limitaciones de las bases de datos relacionales, intercambiando ciertas caracter칤sticas del RDBMS, como una s칩lida consistencia, uniones y un esquema fijo, para obtener m치s flexibilidad, escalabilidad y un rendimiento mejorado. Antes de continuar, aclaremos una cosa: NoSQL no significa No SQ, significa no solo SQL. Es una categor칤a de bases de datos que rompe con el marco relacional que vimos en el v칤deo anterior. Sin embargo, algunas bases de datos no relacionales a칰n admiten SQL o SQL, como los lenguajes de consulta. Repasemos los conceptos b치sicos de las bases de datos NoSQL. Las bases de datos NoSQL tienen estructuras no tabulares. Pueden admitir varios formatos de datos, incluidos valores clave, documentos de columna ancha, gr치ficos y otros. Hablar칠 sobre la clave, el valor y la fuente del documento m치s adelante en este v칤deo y ver치s algunos de estos otros tipos en los pr칩ximos cursos. A diferencia de las bases de datos relacionales, las bases de datos NoSQL no requieren esquemas predefinidos, lo que significa que tiene m치s flexibilidad a la hora de decidir c칩mo desea almacenar los datos. Las bases de datos NoSQL destacan por su escalado horizontal, lo que significa distribuir autom치ticamente los datos y las cargas de trabajo en varios servidores para satisfacer las crecientes demandas de tr치fico. Cuando un usuario escribe datos en una base de datos NoSQL que est치 distribuida en varios servidores o nodos, la operaci칩n de escritura se realiza primero en un solo nodo de este sistema distribuido, que es una ubicaci칩n en la que se ejecuta una versi칩n de la base de datos. En ese caso, es posible que se produzca un ligero retraso antes de que esos cambios se propaguen a todos los dem치s nodos del sistema. A diferencia de las bases de datos relacionales, las bases de datos NoSQL funcionan seg칰n el principio de una coherencia eventual en lugar de una s칩lida, lo que significa que la base de datos le permitir치 leer desde un nodo que no haya recibido la 칰ltima actualizaci칩n de escritura y es posible que no obtenga los datos m치s actualizados. Sin embargo, con el tiempo suficiente, la base de datos ser치 coherente y la lectura de los datos de cualquier nodo le proporcionar치 los mismos datos. Con una base de datos relacional que proporciona una coherencia s칩lida, no podr치 leer los datos hasta que se hayan actualizado todos los nodos del sistema. De esta manera, la coherencia final no permite que ninguna base de datos individual priorice la velocidad. Esto es perfecto para aplicaciones en las que la disponibilidad y la escalabilidad del sistema son m치s importantes que la coherencia en tiempo real, como las plataformas de redes sociales o las redes de distribuci칩n de contenido. En cuanto a la integridad de los datos, no todas las bases de datos NoSQL garantizan los principios de atomicidad, coherencia, aislamiento y durabilidad, tambi칠n conocidos como conformidad con ACID, que veremos en el siguiente v칤deo, pero algunas s칤, por ejemplo, MongoDB. Esto significa que si obtiene datos de una base de datos NoSQL, es posible que deba tomar medidas adicionales para garantizar la integridad de los datos. Por 칰ltimo, las bases de datos NoSQL utilizan lenguajes de consulta especializados adaptados a su modelo de datos, que a menudo, pero no siempre, son diferentes de los de SQL. Analicemos m치s de cerca dos tipos comunes de bases de datos NoSQL, las bases de datos de valores clave y las bases de datos de documentos. Una base de datos clave-valor almacena los datos como una colecci칩n de pares clave-valor, similar a lo que puede encontrar en un archivo JSON o en una estructura de diccionario de Python. La clave sirve como identificador 칰nico para recuperar el valor correspondiente. Tanto las claves como los valores pueden ser objetos simples o complejos. Este tipo de base de datos NoSQL es perfecto para escenarios en los que se necesita una b칰squeda r치pida de datos, como el almacenamiento en cach칠 de los datos de las sesiones de usuario en una aplicaci칩n web o m칩vil. Por ejemplo, cuando un usuario inicia sesi칩n en una aplicaci칩n de comercio electr칩nico, acciones como ver diferentes productos, a침adir art칤culos al carrito de la compra y finalizar la compra se pueden almacenar en una base de datos de valores clave con el identificador de sesi칩n del usuario como identificador 칰nico. Los almacenes de documentos son un tipo especial de base de datos de valores clave que almacenan datos en JSON como documentos. Cada documento tiene una clave 칰nica que identifica un documento y le permite recuperar los datos de ese documento. Los documentos se organizan en colecciones, por lo que puede pensar en una colecci칩n como una tabla en una base de datos relacional y en un documento como una fila. En este ejemplo, los datos se almacenan en una colecci칩n denominada usuarios. Cada documento representa a un 칰nico usuario y el identificador es la clave que identifica de forma exclusiva a cada usuario. Esta localidad facilita la recuperaci칩n de toda la informaci칩n sobre un usuario en particular en comparaci칩n con una base de datos relacional, donde la informaci칩n del usuario puede estar distribuida en varias tablas. Sin embargo, los almacenes de documentos no admiten combinaciones, por lo que es m치s dif칤cil y menos eficiente combinar informaci칩n de varios documentos en comparaci칩n con combinar informaci칩n de varias tablas en una base de datos relacional. La ventaja, sin embargo, es la noci칩n de un esquema flexible. Como ha visto con las bases de datos relacionales, todos los registros deben ajustarse a un esquema fijo, pero con las bases de datos de valores clave y los almacenes de documentos, los registros de datos no tienen una estructura fija o predefinida. Los almacenes de documentos se utilizan habitualmente para aplicaciones que incluyen cat치logos de administraci칩n de contenido y lecturas de sensores. Cada interacci칩n, producto o lectura de sensor de un dispositivo de IoT, por ejemplo, se puede almacenar como un 칰nico documento con un esquema flexible. Pero ten cuidado, esta flexibilidad puede tener un inconveniente. He visto c칩mo las bases de datos de documentos se convierten en pesadillas absolutas para gestionar las consultas. Y si est치 ingiriendo datos de un almac칠n de documentos NoSQL como sistema de origen, la flexibilidad del esquema hace que sea a칰n m치s f치cil para los propietarios del sistema de origen cambiar algo que interrumpa sus canalizaciones de datos. Tanto las bases de datos relacionales como las bases de datos NoSQL se pueden utilizar como una especie de amplia gama de aplicaciones. Cuando se trata de aplicaciones que procesan transacciones en l칤nea en 치reas como la banca, las finanzas y el comercio electr칩nico, entre otras, las cosas suceden r치pidamente, el dinero cambia de manos y los productos est치n en movimiento. Y en este tipo de aplicaciones de procesamiento de transacciones en l칤nea o OLTP, cualquier error o inconsistencia en los datos puede causar problemas importantes. En el siguiente v칤deo, analizaremos los principios de atomicidad, coherencia, aislamiento y durabilidad, tambi칠n conocidos como principios ACID, que son de vital importancia para sus fuentes de datos y canalizaciones de datos cuando trabaje con sistemas OLTP. Nos vemos all칤.

Tanto las bases de datos relacionales como las no relacionales pueden soportar tasas de transacci칩n muy altas. Se usan com칰nmente en el procesamiento de transacciones en l칤nea o en los sistemas OLTP. Por lo general, estos sistemas necesitan almacenar los estados de las aplicaciones que cambian r치pidamente, como los detalles de los saldos de las cuentas bancarias o los pedidos en l칤nea. La mayor칤a de los sistemas de bases de datos relacionales cumplen con lo que se conoce como compatibles con ACID, lo que significa que cumplen con los principios de atomicidad, coherencia, aislamiento y durabilidad, lo que ayuda a garantizar que las transacciones se procesen de manera confiable y precisa en un sistema OLTP. Por el contrario. Muchas bases de datos NoSQL no cumplen con los activos de forma predeterminada, pero muchas le ofrecen la posibilidad de configurarlas para que cumplan con los activos. En este v칤deo, voy a hablar sobre qu칠 es cada principio de activo para que pueda tener una mejor idea de cu치ndo se aplicar치n estos principios al trabajo que realiza como ingeniero de datos. Pero primero, pensemos por un momento en lo que ocurre si las transacciones, por ejemplo, se realizan en un sistema bancario o no se procesan de manera confiable o precisa. Por ejemplo, 쯤u칠 pasa si intentas transferir dinero en l칤nea de una cuenta a otra y el sitio web del banco simplemente deja de funcionar mientras intentas transferir dinero? Cuando finalmente regreses a tu cuenta, esperar치s descubrir que la transacci칩n se realiz칩 o no, pero no que el dinero se haya deducido de una cuenta, pero no que el dinero se haya deducido de una cuenta, pero no se haya agregado a la otra. Lo mismo se aplica a todos los sistemas, desde la banca hasta las compras en l칤nea y muchos m치s. El primer principio de los activos es la atomicidad, que garantiza que las transacciones sean at칩micas o, en otras palabras, que se traten como una sola unidad indivisible. Una transacci칩n puede constar de varias operaciones. Sin embargo, el principio de atomicidad garantiza que todas las operaciones de una transacci칩n se ejecuten correctamente o que ninguna de ellas lo haga. Por ejemplo, imagine la transacci칩n de un cliente que hace un pedido de un art칤culo. Supongamos que esta transacci칩n implica dos operaciones: deducir el costo total de la cuenta del cliente y actualizar el inventario para reflejar el art칤culo comprado. Supongamos que el cliente experimenta un error de conexi칩n de red justo despu칠s de deducir el coste total de su cuenta. Pero antes de que se completara la actualizaci칩n del inventario. El principio de atomicidad garantiza que ambas operaciones se realicen como una sola transacci칩n. Si el error de conexi칩n de red impide que se complete la segunda operaci칩n, la primera se anular치, por lo que no se cobrar치 al cliente y se producir치 un error en toda la transacci칩n. El segundo principio es la coherencia, lo que significa que cualquier cambio en los datos realizado dentro de una transacci칩n debe seguir el conjunto de reglas o restricciones definidas por el esquema de la base de datos. Esto garantiza que la base de datos pasar치 de un estado v치lido a otro. Por ejemplo, supongamos que el esquema de la base de datos de inventario evita que cualquier nivel de existencias descienda por debajo de cero. Supongamos que el nivel de existencias de un art칤culo en particular es actualmente uno. Si un cliente intenta hacer un pedido de dos de esos art칤culos, la operaci칩n fallar치 y se descartar치 toda la transacci칩n para garantizar que la base de datos siga siendo coherente con el esquema predefinido. Como ya he mencionado, este es el valor predeterminado en las bases de datos relacionales, pero tendr칤a que configurarse en la mayor칤a de los sistemas NoSQL. Solo una nota para mayor claridad, la palabra coherencia termina sobrecarg치ndose un poco aqu칤. En el v칤deo anterior, describ칤 la s칩lida propiedad de consistencia de las bases de datos relacionales, que hace referencia a la idea de que todos los nodos de un sistema distribuido proporcionar치n los mismos datos actualizados. Resulta que la consistencia s칩lida de un sistema de base de datos es el resultado del cumplimiento de los principios de ACID, pero es un concepto ligeramente diferente al de la consistencia representada por la C en ACID. El siguiente principio es el aislamiento, que garantiza que cuando varios clientes intentan ejecutar transacciones simult치neamente, cada transacci칩n se ejecute de forma independiente en orden secuencial. Por ejemplo, supongamos que el inventario muestra que quedan 10 unidades de un art칤culo. Supongamos que dos clientes hacen cada uno un pedido de cinco de estos art칤culos al mismo tiempo. El principio de aislamiento garantiza que, aunque la marca de tiempo de estas dos transacciones sea la misma, ambas transacciones se realizar치n de forma independiente en secuencia, de modo que cuando se completen las dos transacciones, el nivel de inventario de ese art칤culo ser치 cero y no cinco. Del mismo modo, si un cliente pide cinco y otro pide 10 del art칤culo al mismo tiempo, el pedido que se procese primero, lo procesaremos y el segundo fallar치, lo que dar치 como resultado un nivel de inventario de cinco o cero. El principio final es la durabilidad, que garantiza que, una vez que se complete una transacci칩n, sus efectos sean permanentes y sobrevivan a cualquier fallo posterior del sistema, como una p칠rdida de energ칤a. Esto es esencial para mantener la confiabilidad de la base de datos, incluso cuando se enfrenta a un evento inesperado, como un desastre natural. En resumen, los principios ACID garantizan que una base de datos mantendr치 una imagen coherente del mundo. Eso puede parecer l칩gico y relativamente sencillo. Sin embargo, en el mundo real, una base de datos puede particionarse en varios servidores debido a su tama침o o replicarse en varios centros de datos para lograr redundancia y velocidad. En estos casos, es especialmente importante saber que los datos que est치 leyendo y escribiendo permanecen consistentes en toda la red de servidores. Este es el principio de lo que se denomina consistencia s칩lida, que es una caracter칤stica clave del cumplimiento de ACID que se mantiene incluso en un sistema de base de datos distribuido. Ahora bien, es importante tener en cuenta que, si bien las bases de datos relacionales suelen cumplir con ACID, no todas las bases de datos deben cumplir con todos los principios de ACID para poder soportar los backends de las aplicaciones. Algunas bases de datos NoSQL solo poseen cierto grado de conformidad con ACID. Sin embargo, al relajar una o m치s de estas restricciones, puede mejorar ciertos aspectos del rendimiento de las bases de datos y hacerlas m치s escalables. Como ingeniero de datos, comprender cu치ndo su base de datos debe cumplir con ACID puede ayudarlo a prevenir desastres. En el siguiente laboratorio, trabajar치 con DynamoDB, una base de datos de valores clave de NoSQL. Acomp치침eme en el siguiente v칤deo para echar un vistazo r치pido al laboratorio antes de entrar.

En el siguiente laboratorio, trabajar치 con Amazon DynamoDB como base de datos de valores clave y aplicar치 algunas operaciones de creaci칩n, lectura, actualizaci칩n y eliminaci칩n o CRUD a los datos de esta base de datos NoSQL. En este v칤deo, le proporcionar칠 una descripci칩n general de algunas de las funciones de DynamoDB, los datos con los que trabajar치 y algunos de los m칠todos de DynamoDB que utilizar치 para aplicar operaciones CRUD a los datos. DynamoDB es una base de datos de valores clave que almacena un conjunto de elementos de valores clave en una tabla. Cada fila contiene los atributos de un elemento y se identifica de forma exclusiva mediante la clave del elemento. Por ejemplo, aqu칤 hay dos elementos de valor clave en los que cada uno corresponde a una persona. La clave representa el identificador de la persona y el valor consiste en un conjunto de atributos que describen a la persona. DynamoDB almacena estos datos en una tabla, como se muestra aqu칤. Cada fila contiene el identificador de la persona y los atributos correspondientes. Dado que una columna de ID de persona identifica de forma exclusiva cada fila, representa la clave principal de esta tabla. Cuando trabaja con DynamoDB, tambi칠n puede hacer referencia a la clave principal como clave de partici칩n. Esto se debe a que DynamoDB usa la clave principal para determinar la partici칩n o, dicho de otro modo, el almacenamiento f칤sico en el que se almacenar치 el elemento. Tambi칠n puede definir una clave principal compuesta para una tabla de DynamoDB. Por ejemplo, esta es una tabla en la que cada fila representa el art칤culo de un pedido y se identifica de forma 칰nica mediante una clave principal compuesta. Esta clave compuesta se compone de dos claves. La primera representa el identificador del pedido y se denomina clave de partici칩n. La segunda representa el n칰mero de l칤nea del art칤culo con un pedido y se denomina clave de clasificaci칩n. Con esta clave compuesta, puede tener dos elementos con la misma clave de partici칩n, pero deben tener diferentes claves de clasificaci칩n, de modo que pueda identificar cada elemento de forma 칰nica. DynamoDB usa la clave de partici칩n para determinar en qu칠 partici칩n se almacenar치 el elemento y la clave de clasificaci칩n para ordenar los elementos de la misma partici칩n. En estas dos tablas, puedes ver que cada elemento puede tener sus propios atributos distintos. Esto se debe a que las tablas de DynamoDB no tienen esquemas, lo que significa que no es necesario definir los atributos de antemano. En este laboratorio, crear치 cuatro tablas de DynamoDB e interactuar치 con ellas mediante c칩digo Python en un cuaderno Jupyter. Para ello, utilizar치 Boto3, que es el kit de desarrollo de software de AWS que le permite crear y configurar los servicios de AWS mediante Python. Esta es la documentaci칩n de Boto3, donde encontrar치 una lista de m칠todos que puede utilizar para interactuar con varios recursos de AWS. Si hace clic en DynamoDB aqu칤, encontrar치 la lista de todos los m칠todos disponibles que puede usar al trabajar con una tabla de DynamoDB. En el laboratorio, se centrar치 en los m칠todos que puede usar para aplicar las operaciones CRUD, como CreateTable para crear las tablas, putItem, WriteBatchItems, updateItem para agregar o actualizar elementos en las tablas creadas, scan, getItem, query para leer los elementos de las tablas y DeleteItem para eliminar elementos de las tablas. Para llamar a estos m칠todos, primero debe crear un objeto de cliente, como se muestra aqu칤, que le proporciona una interfaz que representa una tabla de Amazon DynamoDB. Con este objeto de cliente, llamar치 a cualquiera de estos m칠todos de DynamoDB. Para crear las tablas, se proporcionan estos cuatro archivos JSON que se descargan de la Gu칤a para desarrolladores de Amazon DynamoDB. Leer치 y cargar치 cada uno de los contenidos en una tabla de DynamoDB. El archivo del cat치logo de productos contiene informaci칩n sobre algunos productos que se venden en Amazon. Cada producto se define por su identificador, que utilizar치s como clave principal simple para la tabla correspondiente. El archivo del foro contiene informaci칩n sobre algunos foros de AWS en los que los usuarios publican preguntas o inician hilos sobre los servicios de AWS. Para cada foro, puedes encontrar el n칰mero total de hilos, mensajes y vistas. Cada foro se define por su nombre, que utilizar치s como clave principal simple para la tabla correspondiente. El archivo de hilo contiene informaci칩n sobre cada hilo del foro, como el asunto del hilo, el mensaje del hilo, el n칰mero total de visitas y respuestas al hilo determinado. Cada hilo se define por el nombre del foro y el asunto del hilo, que utilizar치s como clave principal compuesta para la tabla correspondiente. El archivo de respuesta contiene informaci칩n sobre las respuestas de cada hilo, como la hora de la respuesta, el mensaje de respuesta, el usuario que public칩 la respuesta y el ID de la respuesta, que es una concatenaci칩n del foro y el asunto del hilo. Cada respuesta se define por el identificador y la hora de la respuesta, que utilizar치s como clave principal compuesta para la tabla correspondiente. Echemos un vistazo al archivo del foro. Contiene una lista de elementos de solicitud de venta, cada uno de los cuales contiene un 칰nico elemento. Extraer치 los atributos de cada elemento y, a continuaci칩n, los cargar치 en la tabla de DynamoDB. Tenga en cuenta que cada atributo est치 definido por otro par clave-valor para definir el tipo del atributo y su valor. Por lo tanto, la letra N, que significa n칰mero, y la S, que significa cadena, son ejemplos de descriptores de tipos de datos de DynamoDB que indican a DynamoDB c칩mo interpretar el tipo de cada atributo. Todos los archivos JSON restantes siguen el mismo patr칩n, por lo que te animo a que hojees r치pidamente los dem치s archivos JSON para comprender mejor cada uno de ellos. Repasemos ahora el primer ejercicio del laboratorio. Ya empec칠 el laboratorio y abr칤 este cuaderno de Jupyter. Ejecutar칠 esta celda para importar los paquetes necesarios y luego esta celda para definir esta variable que usar치 en el laboratorio con fines de etiquetado. Tras estas dos celdas, encontrar치 una explicaci칩n sobre los datos y la tabla de DynamoDB, que ya he explicado en este v칤deo. El primer ejercicio consiste en crear las cuatro tablas en las que utilizar치 el m칠todo CreateTable de Boto3. Si consulta la documentaci칩n, ver치 que este m칠todo espera un nombre para la tabla y una definici칩n para la clave principal. Puede definir los atributos que componen la clave principal mediante el argumento de definiciones de atributos. Con el argumento del esquema de claves, puede especificar para cada atributo si es una clave de partici칩n o una clave de clasificaci칩n. Hash aqu칤 significa clave de partici칩n y rango significa clave de clasificaci칩n. En el laboratorio, no necesitar치 codificar estos argumentos para cada tabla. En su lugar, se le proporcionan estos diccionarios que representan los argumentos de cada tabla. Para crear las tablas, se le proporciona esta funci칩n CreateTableDB dentro de la cual llamar치 al m칠todo CreateTable de Boto3. La funci칩n CreateTableDB toma el nombre de la tabla y quargs, que es la abreviatura de argumentos de palabras clave, es un diccionario que contiene los argumentos adicionales del m칠todo Boto3. Las dos estrellas situadas junto a los cuarteles significan que los elementos del diccionario est치n desempaquetados en una secuencia de argumentos. Aqu칤 tendr치s que completar esta l칤nea de c칩digo en la que se llama al m칠todo Boto3 mediante el objeto cliente. Para el nombre de la tabla, usar칠 la entrada TableName y luego pasar칠 los argumentos restantes usando quargs. Ahora ejecutar칠 la celda y las dos celdas siguientes para crear las tablas. Y desde aqu칤, puedes seguir trabajando en los ejercicios restantes. Ten en cuenta que algunos de los ejercicios est치n marcados como opcionales, as칤 que no dudes en saltarte las partes opcionales. Despu칠s del laboratorio, acomp치침eme en el siguiente v칤deo para ver el segundo tipo principal de sistema de almacenamiento con el que trabajar치, los archivos y, m치s espec칤ficamente, los archivos en el almacenamiento de objetos en la nube.


C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "C2W1-3.txt"  
 echo.
) 
>> Como dije a principios de esta semana, los archivos son uno de los sistemas fuente m치s comunes con los que se enfrentar치 todos los d칤as como ingeniero de datos. Y es posible que recibas estos archivos o accedas a ellos desde un sistema de archivos como Google Drive o un sistema de almacenamiento de objetos como sthree, o simplemente como un archivo adjunto a un correo electr칩nico. Si bien los archivos pueden venir de muchos lugares diferentes, el almacenamiento de objetos es sin duda el mecanismo m치s importante para el almacenamiento y la recuperaci칩n de archivos en su trabajo como ingeniero de datos. Object Storage trata los datos, en este caso los archivos, como objetos individuales y los almacena en una estructura plana que no se ajusta a la jerarqu칤a tradicional del sistema de archivos. Esto significa que, aunque est칠 acostumbrado a almacenar archivos en una jerarqu칤a de carpetas y subcarpetas en su equipo local, Object Storage no tiene ninguna jerarqu칤a. Ahora, solo como nota al margen, esto de la estructura plana puede resultar confuso. Si vas a Amazon S3, por ejemplo, hay un bot칩n Crear carpeta y puedes crear carpetas y subcarpetas a tu antojo y almacenar tus archivos sin problemas en lo que se parece mucho a un sistema de archivos jer치rquico. Sin embargo, resulta que esto es solo una caracter칤stica de la interfaz de usuario para mantener las cosas organizadas de una manera que resulte familiar. El mecanismo de almacenamiento actual es plano, lo que significa que, aunque parezca que tienes carpetas y subcarpetas en la interfaz de usuario, todos los archivos se almacenan en el nivel superior. Y esto es por dise침o, ya que permite un acceso r치pido y sencillo a todos los objetos sin preocuparse por la sobrecarga de la estructura de carpetas. De todos modos, estoy divagando. Por lo tanto, los objetos pueden ser cualquier cosa, desde archivos CSV, JSON, texto, v칤deo, imagen o audio hasta datos binarios legibles por m치quina. Esta versatilidad convierte a Object Storage en el repositorio perfecto para datos semiestructurados y no estructurados, lo que puede resultar 칰til cuando se admiten aplicaciones como la entrega de datos para entrenar modelos de aprendizaje autom치tico. El almacenamiento de objetos desempe침a un papel crucial como fuente de datos. En semanas y cursos posteriores, ver치 c칩mo el almacenamiento de objetos tambi칠n se integra a lo largo de todo el ciclo de vida de la ingenier칤a de datos. Pero por ahora, echemos un vistazo a algunos de los componentes clave del almacenamiento de objetos. En Object Storage, a cada objeto se le asigna un identificador 칰nico universal, o UUID, que es una especie de clave. Esta clave es necesaria para acceder al objeto correspondiente y administrarlo. Cada objeto tambi칠n tiene metadatos asociados, que son informaci칩n adicional sobre el objeto, como la fecha de creaci칩n, el tipo de archivo o el propietario. Vale la pena se침alar que despu칠s de la escritura inicial, los objetos t칠cnicamente se vuelven inmutables y no admiten operaciones aleatorias de escritura o adici칩n. En este sentido, un archivo de Object Storage no es como una tabla de una base de datos relacional o un documento de una base de datos no relacional que se puede actualizar o anexar. Para cambiar los datos almacenados en un objeto, debe volver a escribir el objeto completo y hacer que el uuid apunte a este nuevo objeto. Con el almacenamiento de objetos, puede habilitar el control de versiones de objetos, lo que le permite agregar metadatos a un objeto para especificar su versi칩n. Por lo tanto, cuando actualizas un objeto, en lugar de sobrescribir el objeto anterior con el mismo uuid, puedes conservar varias versiones de ese objeto. Entonces, 쯣or qu칠 usar el almacenamiento de objetos? Bueno, Object Storage le permite almacenar archivos de varios formatos de datos sin una estructura de sistema de archivos espec칤fica. Esto elimina la complejidad asociada con las bases de datos y los sistemas de carpetas jer치rquicos. En un entorno de nube, el almacenamiento de objetos puede ampliarse f치cilmente para proporcionar un espacio de almacenamiento pr치cticamente ilimitado para cantidades masivas de datos. En t칠rminos de disponibilidad, los datos del almacenamiento de objetos en la nube suelen replicarse en varias zonas de disponibilidad, lo que significa que los datos se replican en varios centros de datos f칤sicos que est치n aislados unos de otros. Esto hace que los datos sean muy duraderos y est칠n disponibles incluso en el caso de desastres naturales. Por ejemplo, como mencion칠 en el curso anterior, Amazon S3 ofrece una durabilidad de datos de 11 nueves, lo que significa que el almacenamiento de objetos en S3 puede soportar fallos simult치neos de dispositivos o centros de datos. Adem치s, el almacenamiento de objetos suele ser m치s econ칩mico que otras opciones de almacenamiento, especialmente si almacenas datos a los que no necesitas acceder de forma regular. Cloud Object Storage se usa en muchas aplicaciones y es el almacenamiento subyacente para los dise침os de arquitectura m치s nuevos, como lagos de datos y casas de lagos de datos, debido a su flexibilidad, alta escalabilidad, rentabilidad y durabilidad. A continuaci칩n, tendr치 la oportunidad de trabajar con el almacenamiento de tres objetos de Amazon. Crear치 una consulta de datos de tres cubos a partir del dep칩sito y trabajar치 con el control de versiones de objetos. Despu칠s del laboratorio, acomp치침eme en el siguiente v칤deo para ver los registros de las aplicaciones como fuentes de datos del sistema de streaming.

El tipo de sistema de transmisi칩n m치s simple que se me ocurre es un registro. De hecho, el registro ni siquiera es un sistema. Es solo un registro de informaci칩n sobre eventos que puede servir para rastrear la actividad de un sistema o una aplicaci칩n. En el curso anterior, mencion칠 que sol칤a ser com칰n que los desarrolladores consideraran los datos provenientes de las aplicaciones de software como un escape o un subproducto, que no necesariamente ten칤an ning칰n valor intr칤nseco por s칤 mismos, pero eran 칰tiles para monitorear o depurar un sistema. Los datos espec칤ficos que se consideran m치s com칰nmente como datos de escape son los datos contenidos en los registros producidos por las aplicaciones de software. Cuando un desarrollador implementa un producto o una plataforma, como un sitio web o una aplicaci칩n m칩vil, lo configurar치 de manera que toda la actividad que se produzca dentro de la aplicaci칩n se registre en un registro. El registro puede incluir la actividad de un usuario, como un usuario que inicia sesi칩n o navega a una p치gina en particular. Tambi칠n puede incluir un registro de eventos en el back-end, como una actualizaci칩n de una base de datos o un error que se gener칩 al intentar ejecutar un procedimiento en particular. Los registros se utilizan con mayor frecuencia en la pr치ctica como medio para supervisar el estado de los sistemas. Los ingenieros usar치n los registros para activar alertas o para depurar lo que sali칩 mal cuando se produce un error. En este sentido, los troncos pueden parecer aburridos y la caracterizaci칩n de los troncos como gases de escape para aplicaciones puede parecer apropiada. Sin embargo, los registros son una fuente rica de datos que pueden ser 칰tiles para mucho m치s que solo monitorear el estado de una aplicaci칩n. Como tales, pueden ser un sistema fuente importante. Absorber치s datos de tu trabajo como ingeniero de datos. En esencia, un registro es una secuencia de registros que solo se pueden anexar ordenados por tiempo que captura informaci칩n sobre los eventos que ocurren en los sistemas. Por ejemplo, si eres el ingeniero de datos de una empresa de comercio electr칩nico, los registros de tu servidor web pueden capturar datos detallados de la actividad de los usuarios que podr칤an usarse para respaldar el an치lisis posterior de los patrones de comportamiento de los usuarios. Muchos sistemas de bases de datos tendr치n registros que puede usar para rastrear los cambios en el proceso de la base de datos, conocido como captura de datos de cambios o CDC, por sus siglas en ingl칠s. Puede usar esos cambios para activar los procesos de ingesti칩n de modo que se ejecuten en funci칩n de la llegada de nuevos datos a la base de datos. O puede ingerir datos de registro para usarlos en ciertas aplicaciones de aprendizaje autom치tico, como la detecci칩n de anomal칤as, si, por ejemplo, est치 ingiriendo datos de registro de sistemas de seguridad. Los registros desempe침an un papel crucial en el seguimiento de lo que ocurri칩 en muchos de los sistemas de software anteriores con los que trabajar치. Esto los convierte en una fuente de datos rica que puede respaldar casos de uso posteriores, como el an치lisis de datos, la resoluci칩n de problemas, la supervisi칩n del rendimiento , las aplicaciones de aprendizaje autom치tico y la automatizaci칩n. Como mencion칠 anteriormente, el registro es un registro de informaci칩n sobre eventos. En general, los datos que se registran primero para cada evento de un registro son la cuenta personal, de sistema o de servicio asociada al evento, como un identificador de usuario y su direcci칩n IP. A continuaci칩n, encontrar치 un registro del evento que ocurri칩 junto con sus metadatos. Por ejemplo, un usuario agreg칩 un producto espec칤fico a su carrito, junto con el estado de esa acci칩n. Por 칰ltimo, encontrar치s un registro de la marca de tiempo del evento. Los datos de registro se pueden registrar como texto simple no estructurado o en formato JSON o CSV, o incluso como datos codificados binarios. Adem치s de los datos que describen la hora y el contenido de un evento, los registros tambi칠n suelen incluir una etiqueta para clasificar el evento asignando lo que se conoce como nivel de registro a cada registro. Los niveles de registro pueden incluir etiquetas como debug, info, warn , error o fatal que permiten saber qu칠 tipo de informaci칩n contiene un registro determinado. Por ejemplo, a un registro que contenga informaci칩n de actividad b치sica se le asignar칤a el nivel de registro de informaci칩n. Bueno, es posible que a un registro que contenga un mensaje de error se le asigne el nivel de registro de errores o si ocurre algo m치s grave, como que los sistemas principales fallan y necesitan atenci칩n urgente. Esto podr칤a incluir el nivel de registro fatal como etiqueta. Hablaremos m치s sobre los niveles de registro m치s adelante, cuando comience a crear registros en sus propias aplicaciones de canalizaci칩n de datos, en lugar de supervisar sus propios sistemas. Como ingeniero de datos, es importante que comprenda c칩mo trabajar con los registros, sus tipos, formatos y aplicaciones. Los registros ser치n una fuente importante de datos para el trabajo que realice y pueden ayudarlo a solucionar problemas, supervisar el rendimiento y atender muchos casos de uso posteriores. Acomp치침ame en el siguiente v칤deo para ver algunos de los sistemas de streaming.



C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "C2W1-4.txt"  
 echo.
) 
En la lecci칩n anterior, analizamos algunos detalles pr치cticos, algunos sistemas fuente comunes con los que trabajar치 como ingeniero de datos. En los laboratorios, tienes algo de pr치ctica en la manipulaci칩n de datos en bases de datos relacionales y no SQL, as칤 como en el almacenamiento de objetos. Cuando se trata de conectarse realmente a los sistemas de origen y a su trabajo como ingeniero de datos, es relativamente com칰n encontrarse con problemas imprevistos que le impiden acceder a los datos que le interesan. Estos problemas pueden deberse a problemas como tener una administraci칩n de identidad y acceso o definiciones de mensajer칤a instant치nea incorrectas , configuraciones de red incorrectas o incluso tener un conjunto incorrecto de credenciales de acceso. Al principio, estos problemas pueden parecer relativamente triviales, pero seg칰n mi propia experiencia, estos problemas ocurren todo el tiempo en la ingenier칤a de datos y pueden ser un obst치culo importante si no sabes c칩mo depurarlos y resolverlos correctamente. De hecho, creo que resolver problemas como estos es una habilidad fundamental para los ingenieros de datos, por lo que cuando entrevisto a nuevos ingenieros de datos, me gusta utilizar una canalizaci칩n de datos funcional que pueda conectarse a los sistemas de origen previstos y, tal vez, romper algunas de las configuraciones de mensajer칤a instant치nea o de red, y luego pedirle al candidato que resuelva los problemas. Esto me demuestra lo bien preparados que est치n para solucionar estos problemas en el trabajo. En esta lecci칩n, empezar칠 por repasar algunos de los detalles de c칩mo conectarse a los diferentes sistemas fuente. Lo demostrar칠 en AWS. Sin embargo, los principios que analizaremos tambi칠n se aplican a otras plataformas en la nube. En el contexto de las funciones y los permisos de IAM, analizaremos la importancia de la seguridad en la nube, donde la IAM es la clave para controlar y administrar el acceso a las fuentes de datos basadas en la nube y a otros componentes de sus canalizaciones de datos. Por 칰ltimo, nos adentraremos en la creaci칩n de redes. Empezar칠 con una descripci칩n general de alto nivel y, a continuaci칩n, Morgan le explicar치 los detalles de las redes en AWS, incluidas las VPC y las subredes, las puertas de enlace, el enrutamiento, los grupos de seguridad y m치s. Por lo tanto, en esta lecci칩n, se basar치 en su conocimiento de los conceptos b치sicos de redes que analizamos en el curso anterior. Despu칠s de esta lecci칩n, pondr치s a prueba tus habilidades. Al igual que hago con los candidatos de ingenier칤a de datos en el proceso de entrevista, he establecido un desaf칤o para ustedes en el pr칩ximo ejercicio de laboratorio. All칤 encontrar치s una canalizaci칩n de datos que deber칤a resultarte familiar en uno de los laboratorios anteriores, pero ahora la he desglosado. En ese ejercicio de laboratorio, tendr치 una idea de lo que es trabajar como ingeniero de datos al que se le asigna la tarea de conectarse a un sistema fuente en la nube. Te prometo que este no ser치 como el ejercicio est치ndar de Hello World, en el que todo funciona como se espera. En el juego, te ver치s envuelto en un escenario en el que, al igual que en el mundo real, las cosas no funcionan como esperabas. Su trabajo consistir치 en solucionar problemas y averiguar la causa del problema y, a continuaci칩n, resolverlo para conectarse a las fuentes de datos que necesita. Acomp치침eme en el siguiente v칤deo, en el que le proporcionar칠 una descripci칩n general de alto nivel sobre las formas de conectar los sistemas fuente.

Cuando crea una canalizaci칩n de datos en una arquitectura basada en la nube, lo que realmente est치 creando es una red de recursos conectados. Por lo tanto, la forma en que configura esa red desempe침a un papel clave para garantizar que los datos fluyan correctamente a lo largo de su canalizaci칩n de datos. En un curso anterior, analizaste los conceptos b치sicos de las redes y c칩mo las redes son en realidad solo un conjunto de dispositivos conectados que pueden compartir datos y comunicarse entre s칤 y a trav칠s de Internet. Aqu칤 me gustar칤a revisar y ampliar algunos de esos principios b치sicos de redes para prepararlo para los problemas que pueden surgir al intentar conectarse a los sistemas fuente. Cuando se trata de redes en la nube, los principios b치sicos son pr치cticamente los mismos en todos los principales proveedores de nube. Aqu칤 repasar칠 los principios en el contexto de AWS, ya que es con lo que est치 trabajando en estos cursos. Pero sepa que los conceptos subyacentes se aplican a cualquier nube en la que est칠 trabajando. El t칠rmino computaci칩n en nube puede sonar un poco abstracto, como si la computaci칩n estuviera ocurriendo de alguna manera en el 칠ter. Pero no se equivoque, la nube y la computaci칩n en nube se componen de centros de datos f칤sicos muy reales que se encuentran repartidos por todo el mundo. Como aprendi칩 en el curso anterior, la nube de AWS es una red global que se extiende por diferentes 치reas geogr치ficas conocidas como regiones. Cada regi칩n contiene cl칰steres de zonas de disponibilidad y cada zona de disponibilidad consta de uno o m치s centros de datos con alimentaci칩n, redes y conectividad redundantes. En muchas aplicaciones de computaci칩n en nube, los datos y los recursos se replican en todas las regiones y zonas de disponibilidad para garantizar que los sistemas sigan funcionando incluso si uno o m치s centros de datos dejan de funcionar. Como ingeniero de datos que est치 creando nuevos recursos en la nube, tendr치 que decidir en qu칠 regi칩n hospedar sus recursos. Al hacerlo, es posible que tengas que considerar aspectos como el cumplimiento legal. Por ejemplo, 쯔lmacenar sus datos en una regi칩n espec칤fica significa que debe cumplir con requisitos normativos o de privacidad de datos 칰nicos? Es posible que tambi칠n debas tener en cuenta otros factores, como la latencia y la disponibilidad. En general, cuanto m치s cerca est칠n los usuarios finales de la regi칩n en la que est치n alojados los recursos, menor ser치 la latencia. Y mientras m치s zonas de disponibilidad est칠n replicados sus recursos, mejor podr치 resistir un desastre o recuperarse de 칠l. Adem치s de estas consideraciones, el costo de los recursos puede variar de una regi칩n a otra, lo que puede ser un factor a la hora de tomar una decisi칩n. Por eso, cuando realmente trabajas con recursos en la nube, es f치cil perder de vista el hecho de que lo que realmente haces es interactuar con una red de dispositivos f칤sicos que est치n repartidos por todo el mundo. Sin embargo, como ingeniero de datos, es importante entender c칩mo se configura esta infraestructura global y c칩mo afecta a aspectos como la latencia, el costo, la confiabilidad y la disponibilidad de los sistemas que construyes. As칤 que volvamos a las cosas importantes que necesita saber sobre las redes en la nube. Dentro de cualquier regi칩n, puede crear nubes privadas virtuales personalizadas, o VPC, que son redes m치s peque침as que abarcan varias zonas de disponibilidad dentro de una regi칩n. La creaci칩n de VPC le permite tener un control m치s detallado sobre qui칠n puede acceder a qu칠 recursos. De este modo, puede dividir el espacio de su VPC en subredes, o subredes para abreviar, que alojan sus recursos de canalizaci칩n de datos reales. Cada subred puede entonces tener sus propias reglas de seguridad, conocidas como lista de control de acceso a la red o ACL de red para abreviar, as칤 como configuraciones de enrutamiento a trav칠s de una puerta de enlace de Internet. Esto le permite crear subredes p칰blicas para los recursos conectados a Internet, como los servidores web, y subredes privadas para los recursos internos, como las bases de datos. En el mundo real, las cosas pueden complicarse muy r치pidamente, especialmente cuando se comienzan a configurar varias VPC, subredes, puertas de enlace y configuraciones de enrutamiento entre recursos. Y es en este contexto en el que el simple hecho de conectarse a una base de datos depende de varias capas de configuraciones de red, sin mencionar los permisos de IAM. Por lo tanto, comprender los detalles de la configuraci칩n de la red es fundamental a la hora de conectarse a los sistemas de origen. Tambi칠n es necesario para la orquestaci칩n y automatizaci칩n exitosas de sus canalizaciones de datos, algo que abordaremos en la 칰ltima semana de este curso. A continuaci칩n, Morgan le explicar치 los detalles de las redes en AWS. Y despu칠s de eso, le mostrar칠 lo que puede esperar en el pr칩ximo laboratorio, donde depurar치 su conexi칩n a una base de datos.



C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "C2W1-5.txt"  
 echo.
) 
Excelente trabajo, he llegado al final de la primera semana de este curso sobre sistemas de origen, ingesti칩n y canalizaciones de datos. Espero que se sienta m치s c칩modo y con m치s conocimientos en lo que respecta a la primera etapa del ciclo de vida de la ingenier칤a de datos, en la que los datos se generan en los sistemas de origen. Entonces, est치 creando un nuevo sistema de datos desde cero o actualizando y manteniendo una canalizaci칩n de datos existente. Deber치 comprender c칩mo funcionan los sistemas fuente. Esta semana, comenzamos analizando los detalles de algunos sistemas fuente comunes, como las bases de datos relacionales y NoSQL, el almacenamiento de objetos para archivos y los sistemas de registro y transmisi칩n de eventos. Tambi칠n analizamos algunas formas de conectarse a estas fuentes de datos, espec칤ficamente en una arquitectura basada en la nube. Analizamos c칩mo las redes permiten conectarse a los sistemas de origen y la importancia de la IAM para garantizar la seguridad de los sistemas de origen y del resto de la canalizaci칩n de datos. A lo largo de la semana, destaqu칠 algunos problemas comunes que pueden surgir al trabajar con los sistemas fuente, y tuviste la oportunidad de solucionar algunos de estos problemas comunes en el laboratorio. La semana que viene, nos centraremos en las diferentes formas de ingerir datos de los sistemas de origen. Profundizaremos en los detalles entre la ingesta por lotes y la ingesta por streaming, y tambi칠n analizaremos los diferentes factores que debes tener en cuenta al dise침ar tu arquitectura de ingesta. Nos vemos all칤.



C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "C2W1-6.txt"  
 echo.
) 
En el primer curso de la especializaci칩n, recopilamos algunos requisitos bas치ndonos en las conversaciones que mantuvimos con varias partes interesadas. Incluyendo a un cient칤fico de datos, el director de datos, un gerente de marketing de productos y un ingeniero de software en una empresa de comercio electr칩nico ficticia. A trav칠s de estas conversaciones, el director de datos le dijo que la empresa tiene como objetivo expandirse a nuevos mercados y aumentar la retenci칩n de los clientes existentes. Adem치s, trabaj칩 con estas partes interesadas para configurar una canalizaci칩n de datos para un sistema de recomendaci칩n. Aqu칤 nos basaremos en ese conjunto de conversaciones y hablaremos con un analista de marketing al que se le ha encomendado la tarea de buscar informaci칩n y tendencias de venta de productos. Por eso, en este v칤deo, interpretar칠 a la ingeniera de datos y mi amiga Colleen interpretar치 a la analista de marketing. Vamos a entrar. Colleen n칰mero cuatro, me alegro de conocerte. >> Tambi칠n me alegro de conocerte. >> Hola, soy Joe, soy un nuevo ingeniero de datos y estoy deseando saber m치s sobre lo que est치is haciendo. >> S칤, por supuesto, estoy muy emocionada de trabajar contigo. As칤 que estoy trabajando en tratar de entender qu칠 tipo de factores externos podr칤an ser se침ales de que podr칤amos conectar con los h치bitos de compra de los clientes , por lo que el equipo de marketing ha estado haciendo una lluvia de ideas sobre qu칠 tipo de cosas podr칤an ser y se nos han ocurrido algunas ideas que nos gustar칤a explorar m치s a fondo. >> Eso suena genial, 쯣or qu칠 no me cuentas m치s sobre eso? >> Claro que s칤. As칤 que pensamos que, en t칠rminos generales, la forma en que alguien se siente, feliz o triste, emocionada o relajada, podr칤a afectar su comportamiento cuando se trata de comprar en l칤nea. Por supuesto, no tenemos forma de saber exactamente c칩mo se sienten nuestros clientes en un d칤a determinado, pero pensamos que podr칤amos explorar algunas ideas. En particular, nos gustar칤a ver qu칠 tipo de m칰sica escucha la gente en las distintas regiones en las que vendemos nuestros productos y, a continuaci칩n, compararlos con las ventas de productos. Ya veo. As칤 que est치s pensando que te gustar칤a obtener datos p칰blicos de algunas fuentes externas para obtener informaci칩n sobre la m칰sica que escucha la gente. >> S칤, exactamente. Lo he estado investigando y parece que Spotify tiene una API p칰blica en la que podr칤amos obtener datos sobre los artistas musicales que est치n de moda en las diferentes regiones y las tendencias de escucha de las personas a lo largo del tiempo. 쯉uena como algo con lo que podr칤as ayudarnos? >> Seguro que soy un gran fan de Nickelback, as칤 que definitivamente me gusta mucho la m칰sica. >> Bonito, debe serlo. >> S칤, s칤, as칤 que tendr칠 que echar un vistazo m치s de cerca a la API de Spotify. Y una vez que averig칲e los detalles, tal vez podamos hablar exactamente de qu칠 tipo de informaci칩n le gustar칤a obtener y c칩mo le gustar칤a que se proporcionara. >> Vale, s칤, suena fant치stico. Mientras tanto, av칤same si hay algo en lo que pueda ayudarte y espero poder hablar m치s contigo una vez que tengamos los detalles resueltos. Muy bien, eso es genial, gracias. Muy bien, ese fue un ejemplo de una conversaci칩n con un analista de marketing en la que describe sus necesidades de datos para un proyecto. Conc칠ntrese en extraer los datos de una API p칰blica que les gustar칤a analizar junto con los datos de ventas de productos. Ahora, reconozco, y tal vez t칰 tambi칠n lo est칠s pensando, que estudiar las tendencias regionales sobre el tipo de m칰sica que escucha la gente puede no parecer un enfoque de marketing particularmente brillante. Y probablemente tengas raz칩n, pero cr칠eme, he visto todo tipo de cosas locas en lo que respecta al tipo de datos que las diferentes partes interesadas quieren tener en sus manos. Por lo tanto, el punto aqu칤 no es insistir en si esto parece un esfuerzo digno, sino identificar los requisitos clave del sistema que necesitar치 construir. En este caso, no cabe duda de que se necesita m치s informaci칩n para saber exactamente cu치l ser치 el mejor enfoque a la hora de crear toda la canalizaci칩n de datos para este proyecto. Pero por el momento, nos vamos a centrar en la parte de la ingesti칩n. Lo m치s importante que aprender치s aqu칤 es que vas a necesitar ingerir datos de una API de terceros. Con el tiempo, tendr치 que considerar otros detalles, como la forma en que, en 칰ltima instancia, almacenar치 y entregar치 los datos al analista, y esto depender치 de lo que necesite el analista. En general, al ingerir datos de una API, te enfrentar치s a alg칰n tipo de proceso de ingesta por lotes, pero su aspecto exacto depender치 de lo que pretendas hacer con los datos. En el siguiente v칤deo, analizaremos m치s de cerca las ventajas y desventajas entre los populares paradigmas de procesamiento de datos por lotes de extracci칩n, transformaci칩n, carga o ETL y extracci칩n, carga, transformaci칩n o ELT en relaci칩n con la ingesta de datos. Y despu칠s de eso, analizaremos la conexi칩n y la ingesta de datos de una API REST. Nos vemos en el siguiente v칤deo.

En el v칤deo anterior, nuestros analistas de marketing compartieron sus objetivos para un proyecto en el que est치n trabajando para incorporar algunos datos externos en su an치lisis de las ventas de productos. Para este proyecto, parece que los analistas se centrar치n principalmente en las tendencias hist칩ricas de los datos y es posible que en el futuro quieran pasar a un an치lisis m치s expl칤cito de los datos actuales, pero no necesariamente en tiempo real o en un sentido urgente. Adem치s, sabe que extraer치 los datos de una API de terceros. Si bien, por lo general, tendr치 cierta flexibilidad en cuanto a la frecuencia o la cantidad de datos que extraiga, estar치 limitado a alg칰n tipo de ingesta por lotes. Esto se debe a que las llamadas a la API funcionan de manera similar a las solicitudes web, en las que env칤as una solicitud de datos y recibes una respuesta, y la cantidad de solicitudes que puedes realizar por vez suele ser limitada. En t칠rminos de ingesta de datos para este proyecto, se trata de un proceso por lotes. En el curso anterior, present칠 brevemente ETL y ELT, que son las siglas de extract transform load y extract load transform respectivamente. Se trata de dos patrones de ingesta por lotes muy comunes y, si bien t칠cnicamente incluyen componentes de las etapas de transformaci칩n y almacenamiento del ciclo de vida de la ingenier칤a de datos , en la pr치ctica, tendr치 que pensar en las ventajas y desventajas entre estos patrones y la etapa de ingesta. Eso es lo que me gustar칤a hacer ahora mismo. En primer lugar, hablar칠 un poco m치s sobre lo que distingue a estos dos procesos y , a continuaci칩n, analizaremos cu치l podr칤a ser el m치s adecuado para el caso de uso de nuestros analistas de marketing. La carga de transformaci칩n de extractos o ETL es en realidad el patr칩n original de ingesti칩n de lotes que gan칩 popularidad en las d칠cadas de 1980 y 1990. El proceso comienza con la extracci칩n de datos sin procesar de un sistema de tienda, lo que se puede hacer consultando directamente una base de datos o utilizando una API, por ejemplo. A continuaci칩n, transforma los datos en un 치rea de preparaci칩n intermedia. A continuaci칩n, carga los datos en un destino de almacenamiento de destino, como una base de datos o un almac칠n de datos. En los a침os 80 y 90, la capacidad de almacenamiento y computaci칩n era extremadamente limitada, por lo que era importante tener un plan para determinar exactamente qu칠 datos quer칤as ingerir y c칩mo quer칤as almacenar esos datos y acceder a ellos, en qu칠 formato, etc. Los almacenes de datos eran costosos de configurar y no eran adecuados para ejecutar consultas pesadas que inclu칤an combinaciones y transformaciones complejas. En aquellos d칤as, no hab칤a m치s remedio que ser muy intencional a la hora de transformar los datos sin procesar durante el proceso de ingesti칩n para garantizar que pudieran almacenarse y ponerse a disposici칩n de manera eficiente. El ETL sigue siendo muy popular hoy en d칤a como patr칩n de ingesti칩n. Pero ahora, con el costo relativamente bajo del almacenamiento en la nube y el aumento de la potencia computacional, ya no es la 칰nica opci칩n. A principios de la d칠cada de 2010, los sistemas de almacenamiento en la nube se volvieron altamente escalables y vimos la aparici칩n de lagos de datos basados en sistemas de almacenamiento de objetos como S3 y almacenes de datos en la nube, como Redshift y Snowflake. Esto hizo posible almacenar enormes cantidades de datos de forma relativamente econ칩mica y realizar todas las transformaciones de datos directamente en su almac칠n de datos. Fue entonces cuando surgi칩 el concepto de ELT o transformaci칩n de carga de extracci칩n. En el proceso ELT, se extraen los datos sin procesar de los sistemas de origen y se cargan directamente en la base de datos o el almac칠n de datos de destino o incluso en el almacenamiento 칩ptico sin realizar ninguna transformaci칩n. La interesante idea del ELT es que no es necesario que decidas por adelantado c칩mo quieres usar tus datos. Esto puede resultar atractivo porque, en cierto sentido, se podr칤a decir que al aplicar transformaciones a los datos sin procesar y almacenar solo los resultados del proceso como se hace con ETL, se pierde parte de la informaci칩n en el proceso. Sin embargo, con ELT, todas las opciones permanecen sobre la mesa, ya que solo tiene que capturar todos los datos y guardarlos para usarlos m치s adelante, y luego puede consultar y transformar los datos sin procesar de la manera que desee, sin que se pierda ninguna informaci칩n. Ahora, por muy atractivo que pueda parecer este paradigma, para ser honesto, cuando escuch칠 por primera vez sobre la idea del ELT, pens칠 que era una idea terrible. 쯇or qu칠 pens칠 que querr칤as acumular un mont칩n de datos sin procesar y almacenamiento sin pensar profundamente en c칩mo quieres usarlos? Como he estado enfatizando a lo largo de estos cursos, el primer paso en cualquier proyecto de ingenier칤a de datos debe ser establecer con firmeza cu치les son sus objetivos finales y, solo entonces, pensar en c칩mo construir un sistema para lograr esos objetivos. Sin embargo, con el tiempo, empec칠 a ver los beneficios potenciales del ELT. Por un lado, el ELT es m치s r치pido de implementar porque no requiere una planificaci칩n detallada y anticipada sobre c칩mo desea transformar exactamente sus datos. Tambi칠n es posible hacer que los datos est칠n disponibles m치s r치pidamente para los usuarios finales, aunque est칠n sin procesar, porque el ELT elimina la necesidad de un servidor provisional y de transformaciones de datos en curso. Con la potencia de procesamiento del almac칠n de datos moderno, las transformaciones a칰n se pueden realizar de manera eficiente una vez que los datos se cargan en el almacenamiento. M치s all치 de eso, como dije antes, cuando desee almacenar todos sus datos sin procesar, puede configurarlos m치s adelante para adoptar diferentes transformaciones o analizar los datos de una manera diferente que podr칤a haber sido posible si solo almacenara los datos de transformaci칩n en primer lugar. 쮺u치l es la desventaja del ELT? En resumen, si no tiene cuidado, su canalizaci칩n puede convertirse simplemente en una canalizaci칩n EL, en la que puede extraer y cargar enormes cantidades de datos sin procesar en el almacenamiento sin tener que averiguar c칩mo transformarlos en algo 칰til. Si no quiere dedicar tiempo por adelantado a planificar c칩mo va a usar sus datos, podr칤a terminar con lo que com칰nmente se conoce como un pantano de datos, que es una situaci칩n en la que los datos se vuelven desorganizados, inadministrables y pr치cticamente in칰tiles. Me gusta mostrar esta imagen cuando surge el tema de los pantanos de datos. Aqu칤 tenemos a un ingeniero de datos sentado en su pantano de datos, o se ha quedado con absolutamente todo lo que pensaba que podr칤a tener alg칰n valor alg칰n d칤a. Pero ahora, por supuesto, incluso si pudiera recordar todo lo que hab칤a all칤, probablemente ni siquiera ser칤a capaz de encontrarlo. A principios de la d칠cada de 2010, los pantanos de datos eran comunes, ya que las empresas descubrieron que era posible conservar literalmente cada fragmento de datos sin procesar por si acaso. Hoy en d칤a, gran parte de esto se ha solucionado debido en parte a las regulaciones que exigen que las empresas almacenen los datos de tal manera que puedan auditarse o eliminarse de manera ordenada; por ejemplo, un usuario solicita que sus datos se eliminen de los sistemas de la empresa. Dicho esto, el costo relativamente bajo del almacenamiento actual, combinado con la potencia de procesamiento de los almacenes de datos modernos y otras abstracciones de almacenamiento, significa que tanto ETL como ELT pueden ser enfoques razonables para el procesamiento por lotes. Sin embargo, sea cual sea el enfoque que adopte, es importante tener un conjunto y objetivos claros en mente y administrar los datos en consecuencia. Pensemos en la conversaci칩n con el analista de marketing. Para este proyecto, ingerir치s datos de una API de terceros. La mayor칤a de las veces, los datos que recibir치 a trav칠s de una conexi칩n de API ser치n datos semiestructurados, tal vez en formato JSON. En algunos casos, es posible que tambi칠n est칠s recuperando datos no estructurados, como textos e im치genes. En este caso, parece que el analista de marketing pretende realizar un an치lisis exploratorio de los datos y no podr칤a decir por adelantado exactamente qu칠 transformaciones podr칤an ser necesarias. Probablemente, un oleoducto ELT sea la elecci칩n correcta para este escenario de ingesti칩n, ya que brinda m치s flexibilidad en las etapas de transformaci칩n y navegaci칩n de este proyecto. El componente principal de este caso de uso de ingesti칩n del que a칰n no hemos hablado en detalle es una parte sobre la ingesta de datos de una API. Ah칤 es hacia donde nos dirigimos ahora. Acomp치침eme en el siguiente v칤deo para ver c칩mo trabajar치 con una API como fuente de datos.

 

	

ETL
	

ELT

Historia
	

- En los a침os 80 y 90, el coste de los almacenes de datos era muy elevado (millones de d칩lares), por lo que los ingenieros quer칤an ser muy cuidadosos con los datos que iban a cargar en el almac칠n de datos

- El volumen de datos a칰n era manejable.
	

- El almac칠n de datos en la nube redujo significativamente el coste de almacenamiento y procesamiento de datos (de millones de d칩lares a s칩lo cientos/miles de d칩lares)

- Volumen de datos disparado.

Procesamiento (transformaci칩n)
	

- Los datos se transforman en un formato predeterminado antes de cargarse en un repositorio de datos. As칤, los ingenieros de datos tienen que modelar cuidadosamente los datos y transformarlos a este formato.

- Las transformaciones dependen de la capacidad de procesamiento de la herramienta de procesamiento que se utiliza para ingerir los datos (sin relaci칩n con el destino de destino)
	

- Los datos en bruto se cargan en el destino final. A continuaci칩n, se transforman justo antes del an치lisis (pueden utilizarse con solicitudes de datos no bien definidas)

- Las transformaciones dependen de la capacidad de procesamiento del repositorio de datos, como el almac칠n de datos.

Tiempo de mantenimiento 
	

Si la transformaci칩n resulta inadecuada, es necesario volver a cargar los datos.
	

Los datos originales est치n intactos y ya cargados y pueden utilizarse cuando sea necesario para una transformaci칩n adicional: Se requiere menos tiempo para el mantenimiento de los datos.

Tiempo de carga y tiempo de transformaci칩n
	

Tiempo de carga: suele llevar m치s tiempo, ya que utiliza un 치rea y un sistema de puesta en escena.

Tiempo de transformaci칩n: depende del tama침o de los datos, de la complejidad de la transformaci칩n y de la herramienta que se utilice para realizarla.

	

Tiempo de carga: no hay transformaci칩n, los datos se cargan directamente en el sistema de destino

Tiempo de transformaci칩n: suele ser m치s r치pido porque se basa en la potencia de procesamiento y la paralelizaci칩n de los almacenes de datos modernos

(generalmente se considera m치s eficiente)

Flexibilidad (tipos de datos)
	

Los ETL suelen estar dise침ados para manejar datos estructurados.
	

Los ETL pueden manejar todo tipo de datos: estructurados, no estructurados, semiestructurados. Una vez cargados los datos en el sistema de destino, puede transformarlos.

Coste
	

Depende de la herramienta ETL/ELT que se utilice y del sistema de destino al que se carguen los datos. (Y, por supuesto, depende del volumen de datos).
	

Depende de la herramienta ETL/ELT que se utilice y del sistema de destino al que se carguen los datos. (Y, por supuesto, depende del volumen de datos).

Escalabilidad
	

Hoy en d칤a, la mayor칤a de las herramientas en la nube son escalables. Sin embargo, el reto aqu칤 es que si tiene muchas fuentes de datos y muchos objetivos, tendr치 que hacer un gran esfuerzo para gestionar el c칩digo y manejar los datos de m칰ltiples fuentes
	

ELT utiliza la potencia de procesamiento escalable del almac칠n de datos para permitir la transformaci칩n a gran escala.

Calidad/seguridad de los datos
	

Garantiza la calidad de los datos limpi치ndolos previamente. Las transformaciones tambi칠n pueden incluir el enmascaramiento de informaci칩n personal.
	

Los datos deben transferirse primero al sistema de destino antes de aplicar transformaciones que mejoren la calidad o la seguridad de los datos.

*Existe un subpatr칩n denominado EtLT, en el que t peque침o no se refiere al modelado de negocio, sino a la transformaci칩n con alcance limitado (enmascarar datos sensibles, deduplicar filas).

En el curso anterior, mencion칠 el llamado mandato de API que lleg칩 en forma de correo electr칩nico de Jeff Bezos a todos los empleados de Amazon en 2002. La esencia de este correo electr칩nico era que, de ahora en adelante, todos los equipos deber치n utilizar interfaces de servicio, tambi칠n conocidas como interfaces de programaci칩n de aplicaciones o API, para comunicarse, as칤 como para ofrecer datos y funciones. El problema que pretend칤a resolver era que, antes de esa fecha, los equipos de Amazon y de cualquier otra organizaci칩n no dispon칤an de una forma uniforme o estable de intercambiar datos y servicios, lo que generaba ineficiencias. Al configurar las API como una interfaz de servicio estable y predecible entre los diferentes equipos, cualquier equipo individual pod칤a proporcionar datos , funciones y comunicaciones a otros equipos, sin importar el tipo de l칤o complicado que pudieran tener los equipos en sus propios sistemas. La otra parte del mandato de la API consist칤a en que todas estas interfaces de servicio o API ten칤an que crearse desde cero para que, finalmente, fueran p칰blicas para los desarrolladores del mundo exterior. Esta reorientaci칩n hacia las interfaces de servicio sent칩 las bases de lo que eventualmente se convertir칤a en Amazon Web Services y marc칩 la direcci칩n de c칩mo las empresas de todo el mundo compartir칤an datos y servicios, tanto interna como externamente. B치sicamente, una API es un conjunto de reglas y especificaciones que le permite comunicarse e intercambiar datos mediante programaci칩n con una aplicaci칩n. Por comunicarse mediante programaci칩n, me refiero a comunicarse mediante la ejecuci칩n de c칩digo. Si ha desarrollado software, es posible que est칠 familiarizado con la conexi칩n a las API. Pero aunque no hayas configurado t칰 mismo las conexiones de API, no cabe duda de que est치s utilizando las API directamente a diario cuando buscas cosas en l칤nea o utilizas las aplicaciones de tu tel칠fono. Esto se debe a que, en la actualidad, las API est치n integradas en la funcionalidad de una amplia gama de aplicaciones de software. Por ejemplo, las aplicaciones de redes sociales utilizan API para obtener y mostrar datos de los servidores web a los usuarios finales. Las API tambi칠n se utilizan para facilitar las transacciones entre sitios web de comercio electr칩nico y sistemas de pago. Muchas empresas ofrecen API p칰blicas para que usted, como desarrollador, pueda acceder a sus datos y servicios e integrarlos en sus propias aplicaciones. Como ingeniero de datos, utilizar치 las API para conectarse y extraer datos de varias fuentes, como servicios web, plataformas en la nube o proveedores externos, enviando solicitudes y recibiendo respuestas en un formato estandarizado. Las API tambi칠n pueden proporcionar funciones de metadatos, documentaci칩n, autenticaci칩n y gesti칩n de errores para facilitar la extracci칩n de datos. El tipo de API m치s com칰n es lo que se conoce como API REST, o REST son las siglas de Representational State Transfer. Las API REST suelen utilizar el Protocolo de transferencia de hipertexto o lo que quiz치s conozcas m치s habitualmente como m칠todos HTTP como base para la comunicaci칩n. Puede pensar que la interacci칩n con las API REST es similar a lo que hace cuando navega por Internet. Al hacer clic en un enlace de su navegador, env칤a una solicitud HTTP a un servidor para un recurso espec칤fico, como una p치gina web, y el servidor responde proporcionando ese recurso. Con una API REST, tambi칠n env칤as una solicitud HTTP para un recurso en particular y la API est치 configurada para responder en funci칩n del contenido de tu solicitud. En la conversaci칩n, tenemos a los analistas de marketing. Nos enteramos de que les gustar칤a analizar los datos que est치n almacenados en una plataforma de terceros, Spotify en este caso, y que est치n disponibles a trav칠s de una API. Este es un escenario muy com칰n con el que se encontrar치 como ingeniero de datos, en el que se puede acceder a trav칠s de una API al sistema de origen del que necesita extraer datos, ya sea un sistema interno o un sistema externo de terceros. Pero la mejor manera de familiarizarse con el funcionamiento de esto es empezar y hacerlo usted mismo, y eso es lo que har치 en el pr칩ximo laboratorio.

En el pr칩ximo laboratorio, practicar치s c칩mo interactuar con la API de Spotify para extraer datos, explorar qu칠 significa la paginaci칩n y aprender a enviar una solicitud de API que requiere autorizaci칩n. En este v칤deo, analizar칠 primero algunos conceptos de API con los que trabajar치s en el laboratorio y, a continuaci칩n, te proporcionar칠 una descripci칩n general de las tareas del laboratorio. Para los ejercicios de laboratorio, necesitar치s tener una cuenta de Spotify para obtener las credenciales que necesitas para extraer datos de la API. Esto se debe a que cualquier solicitud que env칤es a la API de Spotify requiere autorizaci칩n y, para obtener esa autorizaci칩n, necesitas una cuenta. Ahora, que quede claro, no quiero que pienses que estoy promocionando Spotify aqu칤 o que te estoy pidiendo que abras una cuenta sin ning칰n motivo. De hecho, cuando trabajas con API de terceros como ingeniero de datos, es muy com칰n que tengas que registrar una cuenta en esa plataforma de terceros para poder usar la API. Por lo tanto, el prop칩sito de este laboratorio es que sigas ese flujo de trabajo de la misma manera que lo experimentar칤as en el trabajo. Y, por supuesto, si lo deseas, puedes cancelar tu cuenta de Spotify inmediatamente despu칠s de completar el laboratorio. Pero por ahora, si a칰n no tienes una, te animo a que crees una cuenta de Spotify y consultes la documentaci칩n de la API de Spotify para que puedas seguir lo que te voy a mostrar aqu칤. La API web de Spotify es una API RESTful a la que puedes enviar solicitudes para acceder a artistas musicales, 치lbumes y pistas directamente desde el cat치logo de datos de Spotify. Cada elemento de datos espec칤fico, como una lista de reproducci칩n, un artista o un 치lbum, se denomina recurso al que puede acceder enviando una solicitud HTTP al punto final que representa ese recurso. Hay diferentes tipos de solicitudes HTTP, pero las m치s comunes son GET, PUT, POST y DELETE. GET le permite recuperar un recurso, POST le permite crear un recurso, PUT le permite cambiar o reemplazar recursos y DELETE le permite eliminar recursos. Por ejemplo, en la documentaci칩n de la API de Spotify, si haces clic en 츼lbumes, puedes ver todas las solicitudes que puedes usar para interactuar con este recurso. Por ejemplo, puedes realizar una solicitud GET para obtener informaci칩n del cat치logo de Spotify sobre las canciones de un 치lbum usando este punto final. Tambi칠n puedes usar una solicitud GET para recuperar una lista de los lanzamientos de nuevos 치lbumes que aparecen en Spotify usando este punto final. Si la solicitud se realiza correctamente, devuelve una respuesta en formato JSON que contiene informaci칩n sobre el recurso solicitado. Por ejemplo, este es un ejemplo de respuesta a una solicitud GET de canciones de un 치lbum y otro ejemplo de respuesta a una solicitud GET de nuevos lanzamientos. Si la solicitud no se realiza correctamente, devuelve un objeto de error que contiene un c칩digo de estado que explica por qu칠 la solicitud no se ha realizado correctamente. Por ejemplo, un c칩digo de 400 significa una solicitud incorrecta que podr칤a deberse a una sintaxis mal formada, y un c칩digo de 404 significa que no se pudo encontrar el recurso solicitado. Puede encontrar en la documentaci칩n aqu칤 una lista de los c칩digos de estado y el significado de cada uno. Cuando realizas una solicitud a la API web de Spotify, debes especificar el punto final del recurso, as칤 como un token de acceso. El token de acceso es una cadena que contiene las credenciales y los permisos que se utilizan para acceder a un recurso determinado. Para obtener el token de acceso, primero tendr치s que crear una cuenta de Spotify y, desde tu cuenta, podr치s obtener un ID de cliente y un secreto de cliente que podr치s usar en el proceso de autorizaci칩n y la generaci칩n del token de acceso. El token de acceso es v치lido durante una hora. Transcurrido ese tiempo, el token caduca y tendr치s que solicitar uno nuevo. En el laboratorio, se te proporcionar치 un c칩digo que puedes usar para solicitar un token de acceso con tu ID de cliente y el secreto del cliente. Para obtener m치s informaci칩n sobre el proceso de autorizaci칩n con Spotify, puedes consultar la documentaci칩n. Ten en cuenta que es posible que trabajes con otras API que no requieran autorizaci칩n o que el proceso de autorizaci칩n sea diferente al de Spotify, por lo que te sugiero que consultes siempre la documentaci칩n de la API con la que vas a trabajar. Antes de entrar en m치s detalles sobre el laboratorio, no dudes en pausar el v칤deo para consultar la documentaci칩n de Spotify, as칤 como los dos ejemplos de solicitud de API, Get Album Tracks y Get New Releases. Antes de empezar a interactuar con la API de Spotify, debes crear una cuenta para obtener las claves que utilizar치s para generar los tokens de acceso. As칤 que primero aseg칰rate de registrarte y completar la informaci칩n requerida para crear tu cuenta. As칤 que aqu칤 ya he iniciado sesi칩n en mi cuenta. Har칠 clic en el nombre de la cuenta en la esquina superior derecha y, a continuaci칩n, en el panel de control y, a continuaci칩n, en Crear aplicaci칩n. En Nombre de la aplicaci칩n, escribir칠 este nombre. En Descripci칩n de la aplicaci칩n, escribir칠 Spotify App para probar la API. Para los URI de redireccionamiento, especificar칠 este host local. Y aqu칤 voy a elegir Web API. Por 칰ltimo, har칠 clic en Guardar. Si recibes el error de que tu cuenta no est치 lista, puedes cerrar sesi칩n y esperar unos minutos y, a continuaci칩n, volver a iniciar sesi칩n y repetir los pasos. Una vez creada la aplicaci칩n, puede hacer clic en ella para ir a la p치gina de inicio de la aplicaci칩n. Aqu칤 har칠 clic en la configuraci칩n para encontrar el ID de cliente y el secreto del cliente. Tendr치 que copiar estos valores para usarlos en el laboratorio. Una vez que inicie el laboratorio y siga las instrucciones de configuraci칩n del laboratorio, estar치 en este Jupyter Notebook. Aqu칤, en la carpeta SRC o Source, har칠 clic en el archivo env y, a continuaci칩n, pegar칠 el ID de cliente y el secreto del cliente para almacenarlos en estas variables de entorno. Volviendo al Jupyter Notebook, primero ejecutar칠 la celda para importar los paquetes necesarios, luego esta celda para cargar las variables de entorno y, a continuaci칩n, estas dos variables para representar el ID de cliente y el secreto del cliente. Junto a obtener un token de acceso, puedes encontrar en esta celda la funci칩n getToken, que espera que tu ID de cliente y tu secreto de cliente generen el token de acceso. Utilic칠 la documentaci칩n de la API para saber qu칠 detalles deben incluir esta funci칩n. No dudes en consultar este enlace para obtener m치s informaci칩n sobre estos detalles. Ahora llamar칠 a la funci칩n getToken, que devuelve esta respuesta de token. Es un diccionario que contiene tres claves: accessToken, el tipo de token y ExpiresIn. Usar치s la cadena, que es el valor de la clave accessToken, en todas las llamadas a la API de este laboratorio. As칤 que echemos un vistazo a algunos de los ejemplos de llamadas a la API. Para crear tus llamadas a la API en Python, puedes usar el paquete requests, que ya he importado a este cuaderno. Este paquete es una biblioteca popular que proporciona una forma sencilla y f치cil de usar de interactuar con las API. Por ejemplo, para realizar una solicitud GET en la API de Spotify, puedes llamar al m칠todo requests.get. Deber치s introducir el punto final del recurso al que quieres acceder y especificar el accessToken mediante los encabezados de los par치metros asign치ndolo a un diccionario en este formato. En el laboratorio, se le proporciona esta funci칩n que crea autom치ticamente el encabezado de autorizaci칩n dado el accessToken. As칤 que aqu칤, en esta solicitud GET, usar칠 la funci칩n getAuthHeader proporcionada y la asignar칠 al par치metro headers. Para esta funci칩n, usar칠 la respuesta del token para pasar el accessToken. Tenga en cuenta que el segundo GET no es la solicitud HTTP GET. Es el m칠todo que se aplica en un diccionario de Python para extraer el valor correspondiente a una clave. Obtengamos ahora el punto final de getNewReleases y veamos la respuesta devuelta. La respuesta es un objeto de respuesta de Spotify que puedes convertir en un diccionario de Python usando el m칠todo.json, tal y como se muestra aqu칤. Esta es la respuesta. Puedes ver que hay una tecla, 츼lbumes. Y para los 치lbumes, hay otro conjunto de pares de valores clave. Para verificar las claves de cada uno, llame al m칠todo .keys. As칤 que la respuesta en realidad contiene una clave, 츼lbumes. Vamos a comprobar la clave de 츼lbumes. Estas son las claves de los 치lbumes. Repasemos cada una de ellas. href contiene un enlace o el punto final al recurso. Puede ver que se agregaron dos par치metros al punto final, getNewReleases, que son offset y limit. La solicitud getNewReleases espera estos dos par치metros. Por lo tanto, de forma predeterminada, se agregaron un desplazamiento de 0 y un l칤mite de 20 al punto final, lo que significa que la respuesta contiene la informaci칩n de los primeros 20 치lbumes. Puedes encontrar los detalles de los 치lbumes usando la tecla Items. Tambi칠n puede comprobar los valores utilizados para el desfase y el l칤mite y obtener el n칰mero total de elementos, que es 100. Por 칰ltimo, next contiene el mismo punto final pero con valores diferentes para el desfase y el l칤mite. En este caso, se refiere a los 20 elementos siguientes. Por lo tanto, si desea especificar su propio desfase y l칤mite, puede hacerlo mediante este punto final. Aqu칤 eleg칤 40 para offset40 y 20 para l칤mite. En el laboratorio, se te asigna esta funci칩n que realiza los getRequests para los lanzamientos de los nuevos 치lbumes. Toma el offset y el l칤mite como par치metros y los usa para construir el punto final de la getRequest. O puede usar el punto final completo especificado en el 칰ltimo par치metro cuando no est칠 vac칤o. Su tarea consistir치 en completar esta funci칩n. A continuaci칩n, tendr치s que realizar la paginaci칩n para extraer la lista completa de lanzamientos de nuevos 치lbumes. En lugar de extraer los 100 elementos de una llamada a GetRequest, puedes realizar la paginaci칩n para extraer los elementos fragmento por fragmento. Por lo tanto, puede comenzar con la primera llamada a la API, comenzando con una compensaci칩n de 0 y eligiendo el l칤mite que desee. Y luego puedes seguir repitiendo la misma llamada, especificando una compensaci칩n diferente cada vez para seguir leyendo desde donde la dejaste en la llamada a la API anterior. O puedes usar el punto final proporcionado en el siguiente campo de la respuesta devuelta. Por lo tanto, en el laboratorio, se le proporcionan estas dos funciones. El primero realiza la paginaci칩n cambiando manualmente el desplazamiento para cada nueva llamada a la API. Y el segundo usa el siguiente campo proporcionado por la respuesta de la llamada a la API actual. As칤 que tendr치s la tarea de completar estas dos funciones. En la segunda parte del laboratorio, se le proporcionan estas funciones de Python que deber치 completar. Estas funciones te permitir치n crear un proceso de ingesta por lotes que extraiga la informaci칩n del cat치logo de Spotify para cada uno de los nuevos lanzamientos de 치lbumes. Por lo tanto, en esta parte, tendr치s que realizar dos llamadas a la API paginadas. En la primera, utilizar치s la lista de lanzamientos de nuevos 치lbumes con la misma llamada paginada que utilizaste en la primera parte del laboratorio. En la segunda, obtendr치s la informaci칩n del cat치logo de un 치lbum determinado mediante el punto final getAlbumTracks. El archivo Python de autenticaci칩n contiene el script de la funci칩n getToken que devuelve un token de acceso. El archivo de punto final contiene dos funciones. La primera corresponde a la llamada paginada al punto final, getNewAlbums. Y la segunda corresponde a la llamada paginada al punto final, getAlbumTracks. Deber치 completar estas funciones. Tambi칠n tendr치s que completar un fragmento de c칩digo que genere autom치ticamente un nuevo token cuando caduque. Y, por 칰ltimo, en la funci칩n principal, llamar치s a la primera llamada a la API paginada para obtener los ID de los lanzamientos de los nuevos 치lbumes. Luego, para cada ID de 치lbum, llamar치s a la segunda llamada a la API paginada para extraer la informaci칩n de la pista de cada ID de 치lbum. Deber치s completar parte de este c칩digo. Por lo tanto, aseg칰rese de leer las instrucciones detenidamente y de echar un vistazo a estas funciones para comprender c칩mo funcionan juntas. El laboratorio tambi칠n contiene algunas partes opcionales que puede consultar para obtener m치s informaci칩n sobre las llamadas a la API. Una vez m치s, antes de empezar el laboratorio, te animo a que consultes la documentaci칩n de la API de Spotify y crees tu cuenta. Cuando termines el laboratorio, acomp치침ame en la siguiente lecci칩n para explorar los patrones de transmisi칩n y gesti칩n.



C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "C2W2-2.txt"  
 echo.
) 
Ahora es el momento de analizar la ingesta de streaming con m치s detalle. En el curso anterior, trabajaste con un sistema de recomendaci칩n de productos en el 칰ltimo laboratorio, pero en realidad no explicamos en detalle c칩mo se configur칩 la ingesta de streaming para ese sistema. Por eso, en este v칤deo tendremos otra conversaci칩n con las partes interesadas en la que hablar칠 con un ingeniero de software sobre los detalles de ingesta de ese sistema de recomendaci칩n. Despu칠s de eso, construir치 el sistema usted mismo en el siguiente laboratorio. As칤 que pasemos a la conversaci칩n de seguimiento que tendremos con el ingeniero de software. Me alegro de volver a verte, Colleen. >> Tambi칠n me alegro de verte. >> Gracias, estoy trabajando para configurar un nuevo sistema de recomendaci칩n de productos y me gustar칤a trabajar con usted para comprender mejor c칩mo puede funcionar la ingesta de datos en t칠rminos de recibir datos de actividad de los usuarios en tiempo real desde el sitio web. >> Por supuesto, la forma en que funciona el sistema en el sitio web es que registramos continuamente los eventos en el registro del servidor web. Y esos eventos incluyen todo, desde las m칠tricas internas de rendimiento del sistema y cualquier error u otra anomal칤a que se genere, as칤 como la actividad de los usuarios. Al igual que los botones o enlaces en los que hacen clic nuestros usuarios mientras navegan por diferentes productos o realizan compras. >> De acuerdo, bueno, en un escenario ideal, probablemente me gustar칤a ingerir todos los datos de actividad de los usuarios y ninguna de las m칠tricas internas del sistema. 쮺rees que ser칤a posible separar los registros de eventos relacionados con la actividad de los usuarios y guardarlos en un registro separado para que yo pueda ajustarlos? >> S칤, sin duda podemos hacerlo. Me imagino varias formas en las que podr칤a funcionar. Pero si incluimos los mensajes sobre la actividad de los usuarios en un tema de Kafka o en una transmisi칩n de Kinesis, podr칤as incorporarlos directamente desde all칤 a tu canalizaci칩n. >> Vale, genial. S칤, parece una buena soluci칩n. Creo que si pudieras introducir una transmisi칩n de datos de Kinesis, me vendr칤a muy bien. He estado estudiando c칩mo podr칤an funcionar otros aspectos de la canalizaci칩n con la kinesis, por lo que me parece una buena opci칩n por ahora. La otra pregunta que tengo es sobre la carga 칰til de datos en s칤 y la velocidad de mensajes esperada.  Podr칤as decirme m치s sobre lo que puedo esperar en t칠rminos del formato de los mensajes individuales y la velocidad de los mensajes que llegan a la transmisi칩n? >> S칤, por lo que los mensajes se graban en formato JSON. Por lo tanto, la carga 칰til que puede esperar es un JSON que incluye un identificador de sesi칩n y toda la informaci칩n del cliente, como la ubicaci칩n, as칤 como su actividad de navegaci칩n en t칠rminos de los productos que han visto o agregado a su carrito con respecto al tama침o de los mensajes individuales, var칤an un poco, pero generalmente rondan unos cientos de bytes cada uno. La tasa de mensajes que puede esperar variar치 mucho seg칰n el n칰mero de usuarios que haya en la plataforma en un momento dado. Pero como puedes imaginar, un usuario puede generar varios eventos por minuto y, entonces, es posible que tengamos unos 10 000 usuarios en la plataforma en las horas punta. As칤 que eso podr칤a traducirse en hasta 1000 eventos por segundo. >> De acuerdo, genial, veamos, tal vez al final de una estimaci칩n general, entonces podr칤amos tener, digamos, 1000 eventos por segundo con un tama침o de unos cientos de bytes cada uno. Eso es menos de un megabyte por segundo, por lo que deber칤a estar dentro de la capacidad de una transmisi칩n de datos de Kinesis. Creo que pueden gestionar cientos de megabytes por segundo, seg칰n la configuraci칩n. >> Muy bien, la otra cosa que tendr칠 que configurar por mi parte es cu치nto tiempo se conservan los mensajes en la transmisi칩n. Como sabes, la transmisi칩n estar치 b치sicamente en un registro de solo pines, pero lo configuraremos para que los mensajes se eliminen despu칠s de un per칤odo de tiempo. >> Bien, por supuesto, la idea es que usemos los datos en tiempo real para hacer recomendaciones y tambi칠n guardemos las entradas y salidas del modelo de recomendaci칩n para analizarlas m치s adelante. Por lo tanto, si todo va bien, no necesitaremos volver a leer los mensajes de la transmisi칩n. Pero supongo que si algo sale mal, es posible que queramos tener la posibilidad de hacer una copia de seguridad y reproducir la transmisi칩n. Si algo se rompe. 쯈uiz치s podr칤amos retener los mensajes en la transmisi칩n durante un d칤a despu칠s de que se hayan escrito inicialmente? >> Claro, veamos. En un d칤a ajetreado, es posible que escribamos, como dijiste, alrededor de un megabyte por segundo en la transmisi칩n. Y hay una especie de orden de magnitud, alrededor de 100 000 segundos en un d칤a. Entonces, el tama침o total del arroyo podr칤a crecer hasta ser, 쯖u치l ser칤a? 100 gigabytes, en el peor de los casos. As칤 que parece razonable. Vale, bueno, 쯛ay algo m치s que quieras saber o deber칤amos seguir adelante y empezar a construir esto? >> Creo que eso es todo por ahora. Vamos a construirlo. >> Constr칰yelo. Bien, ese fue un ejemplo de una conversaci칩n con un ingeniero de software que ser치 una parte interesada principal y el propietario del sistema fuente del que tendr치s que ingerir datos. Como he recalcado en varios puntos a lo largo de estos cursos hasta ahora, hay otras cosas que deber칤a analizar con los propietarios de los sistemas de origen a la hora de comprender las posibles interrupciones en su canalizaci칩n de datos. Cosas como cambios de esquema o interrupciones. Pero por ahora, nos centraremos en entender los datos en s칤 y el mecanismo de ingesti칩n que utilizar치s en este caso. Acomp치침ame en el siguiente v칤deo, en el que analizaremos algunos de los detalles de esta conversaci칩n y analizaremos m치s de cerca la ingesta de streaming.

En la primera semana de este curso, analizamos los sistemas de transmisi칩n, incluidas las colas de mensajes y las plataformas de transmisi칩n de eventos, desde la perspectiva de que se trata de sistemas fuente, en los que la canalizaci칩n de datos estaba en el extremo consumidor de estas fuentes de datos. Tambi칠n mencion칠 que, seg칰n el sistema con el que trabajes, la fuente real podr칤a ser solo el productor del evento o varios elementos de un sistema de streaming, como varios productores, corredores y consumidores, podr칤an estar antes de tu sistema de ingesta. En la conversaci칩n que acabamos de mantener con el ingeniero de software, el plan que se nos ocurri칩 era que el ingeniero configurara una transmisi칩n de datos de Kinesis y usted consumir치 los mensajes de esa transmisi칩n. En este v칤deo, me gustar칤a hablar un poco m치s sobre los detalles de los flujos de mensajes y, despu칠s, ir치s al laboratorio. Como recordatorio r치pido, las plataformas de transmisi칩n de eventos y las colas de mensajes son las dos modalidades principales de ingesta de streaming. Una cola de mensajes es esencialmente un b칰fer que se utiliza para entregar mensajes de un productor de eventos a un consumidor de forma asincr칩nica. Las colas de mensajes suelen funcionar primero en entrar, primero en salir o Fifo, lo que significa que el consumidor de eventos siempre leer치 primero el mensaje m치s antiguo de la cola y, una vez consumido, se eliminar치 de la cola. Las plataformas de transmisi칩n de eventos, por otro lado, funcionan almacenando los mensajes de forma persistente en un registro que solo se puede adjuntar. El enrutador de eventos distribuye los mensajes del registro a los suscriptores y es posible reproducir o volver a procesar cualquier mensaje del registro. Utilizar치 el servicio Kinesis de Amazon como plataforma de transmisi칩n de eventos en el laboratorio, pero otra plataforma muy utilizada es Apache Kafka. En este v칤deo, utilizar칠 Kafka como plataforma de ejemplo para explicar algunos detalles y establecer paralelismos con Kinesis, donde existan. Y luego, en el siguiente v칤deo, Morgan analizar치 la kinesis con m치s detalle. Por lo tanto, al final de esta semana, puede tener un poco de contexto sobre estas dos soluciones. Por lo tanto, Apache Kafka es una plataforma de transmisi칩n de eventos de c칩digo abierto y, si bien las plataformas de transmisi칩n vienen en diferentes sabores y variedades, los principios de c칩mo se enrutan y almacenan los eventos son similares en todas las plataformas. En un nivel alto, los productores de eventos env칤an o env칤an mensajes a trav칠s de la red a un cl칰ster de Kafka, que contiene uno o m치s servidores, tambi칠n llamados corredores. Luego, los consumidores del evento leen o extraen mensajes de ese grupo de Kafka. Vamos a centrarnos un poco en el c칰mulo de Kafka que tengo aqu칤. Dentro de un cl칰ster de Kafka, los flujos de mensajes se dividen y se dirigen en lo que se denominan temas. Puede pensar en un tema como una categor칤a que contiene una colecci칩n de eventos relacionados, o tal vez en otro sentido, como un camino hacia alg칰n lugar. Por lo tanto, los mensajes se alinean en temas similares a c칩mo se alinean las filas de autom칩viles en diferentes autopistas seg칰n su destino. Un tema puede contener cualquier tipo de mensaje, por ejemplo, alertas de fraude , pedidos de clientes o lecturas de temperatura de dispositivos de IoT. Y es trabajo del productor enviar un mensaje a su tema correspondiente. Cada tema tiene una o m치s particiones, que son solo registros que contienen secuencias ordenadas e inmutables de mensajes a las que se agregan nuevos mensajes continuamente. Utilizando la analog칤a del sistema de carreteras, los tabiques son como los carriles de la carretera. M치s carriles permiten el paso de m치s coches, por lo que cada partici칩n gestiona un subconjunto de mensajes a medida que se a침aden al tema. Esto permite un tr치fico o un flujo de mensajes m치s eficientes. Una vez m치s, es tarea del productor decidir a qu칠 partici칩n enviar cada mensaje. La decisi칩n podr칤a basarse en una estrategia por turnos, por ejemplo, o calculando la partici칩n de destino en funci칩n de la clave del mensaje. Con la kinesis, todos estos conceptos son esencialmente los mismos, pero en lugar de temas, tienes transmisiones, y en lugar de particiones, tienes lo que se llama fragmentos. Ahora, por otro lado, los consumidores est치n agrupados y cada grupo de consumidores puede suscribirse a uno o m치s temas. Los consumidores de un grupo cooperan para consumir los mensajes de todas las particiones de un tema determinado. Cada partici칩n solo se puede asignar a un 칰nico consumidor del grupo y cada consumidor consume mensajes de un subconjunto diferente de particiones. Cuando un productor publica un mensaje en una partici칩n de temas, ese mensaje se entrega a un consumidor de cada grupo de consumidores suscriptor. Una vez que se publica un mensaje en un tema, el cl칰ster de Kafka conserva esa informaci칩n durante un per칤odo de tiempo configurable, independientemente de que el mensaje se haya consumido o no. Esto permite a los consumidores reproducir y volver a procesar los mensajes seg칰n sea necesario. En nuestra conversaci칩n con el ingeniero de software, nos enteramos de que las acciones de los usuarios en el sitio web se registran como mensajes en el registro del servidor web y esos mensajes se env칤an a un flujo de datos de Kinesis. Del mismo modo, esos mensajes podr칤an dirigirse a un tema de Kafka, y podr칤as consumirlos desde all칤 suscribi칠ndote a ese tema en un conjunto diferente de circunstancias. Puede imaginarse un escenario en el que tenga acceso directo para monitorear el registro del servidor web en busca de nuevos mensajes. Entonces, podr칤a tratar el registro del servidor web como el productor de eventos. A partir de ah칤, los eventos podr칤an transferirse a un tema de Kafka o a una transmisi칩n de Kinesis como primer paso del proceso de incorporaci칩n. Tambi칠n puede monitorear la actividad de la base de datos mediante un proceso conocido como captura de datos de cambio continuo o CDC continuo. Al procesar el registro de la base de datos, puede transmitir los cambios de datos a su canalizaci칩n de datos para asegurarse de que los datos de su canalizaci칩n est칠n sincronizados con las actualizaciones de datos de la base de datos de origen. A continuaci칩n, Morgan le explicar치 los detalles de las transmisiones de datos de Amazon Kinesis, que utilizaremos como herramienta de ingesta de transmisiones en el pr칩ximo laboratorio. Y despu칠s de eso, volver칠 para darte un r치pido recorrido por el 칰ltimo laboratorio antes de que te lances a crear tu propia soluci칩n de ingesta de streaming.

Acabas de aprender un poco m치s sobre c칩mo funciona Apache Kafka. Ahora quiero ayudarlo a comprender c칩mo funciona Amazon Kinesis Data Streams antes de que vaya al laboratorio. Como ya sabes, al igual que Kafka, hay productores de eventos que env칤an datos a la transmisi칩n y consumidores que leen los datos de la transmisi칩n. En el pr칩ximo laboratorio, trabajar치 con un Kinesis Data Stream como sistema fuente ascendente. No ser치s responsable de configurar la transmisi칩n en s칤. Sin embargo, tambi칠n ha visto que tambi칠n puede incorporar productores , consumidores y transmisiones en otras partes de sus sistemas de datos. Aqu칤 me gustar칤a explicarte algunos de los detalles de Kinesis que son relevantes cuando tienes el control de todo el sistema, que incluye al productor, el consumidor y la transmisi칩n en este caso. Al igual que en Kafka, los productores env칤an datos a los temas. Cuando trabaja con Kinesis Data Streams, un productor env칤a los datos a una transmisi칩n espec칤fica. Un flujo se compone de muchos fragmentos, que proporcionan las unidades de capacidad del flujo. Como necesitas escalar tu transmisi칩n para ingerir m치s datos, debes agregar m치s fragmentos a la transmisi칩n. Para saber cu치ntos fragmentos necesitar치 para su caso de uso o cu치ndo necesitar치 aumentar el n칰mero de fragmentos, necesitar치 saber el tama침o y la velocidad de las operaciones de escritura y lectura que espera realizar en su proceso. Las operaciones de escritura se producen cuando un productor de eventos escribe datos en la transmisi칩n, y las operaciones de lectura se producen cuando los consumidores intermedios leen los datos de la transmisi칩n. En cuanto a la capacidad, cada acelga puede soportar hasta cinco operaciones de lectura por segundo, y esas cinco operaciones pueden sumar una velocidad total m치xima de lectura de datos de dos megabytes/segundo. Para escribir datos, un productor puede escribir hasta 1000 registros por segundo en un disco duro con una velocidad total m치xima de escritura de datos de un megabyte/segundo. Para determinar la cantidad de fragmentos que necesitar칤a para un caso de uso espec칤fico, se necesitar칤an algunos an치lisis y algunos c치lculos matem치ticos, dado el tama침o y la velocidad de las operaciones de lectura y escritura que espera. A veces puede resultar dif칤cil estimar el n칰mero exacto de operaciones de lectura y escritura. Por ejemplo, en una aplicaci칩n nueva. O en otros casos, lo 칰nico que puede saber con certeza es que espera que el tr치fico de su aplicaci칩n var칤e dr치sticamente con el tiempo, como en una plataforma de comercio electr칩nico u otras aplicaciones p칰blicas. Para esas situaciones, puede usar Kinesis en modo bajo demanda. El modo bajo demanda gestionar치 autom치ticamente el escalado de los fragmentos hacia arriba o hacia abajo seg칰n sea necesario, y solo se le cobrar치 por lo que utilice. Esto puede resultar m치s conveniente desde una perspectiva operativa en comparaci칩n con la alternativa, que es el modo aprovisionado. Con el modo de aprovisionamiento, especific칩 la cantidad de fragmentos necesarios para la aplicaci칩n en funci칩n de la velocidad esperada de solicitudes de escritura y lectura. Luego, depende de usted agregar m치s fragmentos o volver a agregarlos cuando sea necesario. El modo de aprovisionamiento puede ser una buena opci칩n para su trabajo si tiene un tr치fico de aplicaciones predecible o si desea poder controlar los costos con m치s cuidado. Cuando se trata de los datos que se mueven a trav칠s de una transmisi칩n, cada registro de datos que un productor env칤a a la transmisi칩n incluye una clave de partici칩n, un n칰mero de secuencia y los datos en s칤 mismos en forma de lo que se denomina un objeto binario grande o blob para abreviar. Al configurar el generador de datos para su sistema, debe elegir una clave de partici칩n. La clave de partici칩n se usa luego para determinar en qu칠 fragmento se coloca el registro de datos. Luego, la propia Kinesis asigna un n칰mero de secuencia a medida que se escribe cada registro para mantener el orden de los registros dentro del fragmento. Por ejemplo, supongamos que quieres crear un flujo de transacciones desde una plataforma de comercio electr칩nico. A continuaci칩n, es posible que desee utilizar el ID de cliente como clave de partici칩n. En este caso, todas las transacciones de un solo cliente podr칤an almacenarse en el mismo fragmento. Esto facilitar칤a a los consumidores intermedios agrupar los registros relacionados con un solo cliente para su agregaci칩n y an치lisis. Un productor coloca los datos en fragmentos y un consumidor lee los datos de los fragmentos, y es com칰n que varios consumidores lean los datos de un fragmento. De forma predeterminada, los consumidores comparten la capacidad de lectura de Shards, que se denomina salida de ventilador compartida. Esto significa que los consumidores compiten por la capacidad de lectura. Esto puede ser un problema en algunos casos de uso. Para evitar este problema de capacidad, puedes configurar las cosas de manera que cada consumidor pueda leer a la capacidad total de lectura de dos megabytes/segundo del Shard, lo que se denomina ventilaci칩n mejorada. Puede usar servicios gestionados como AWS Lambda, Amazon Managed Service for Apache Flink y ADS Glue para procesar los datos almacenados en las transmisiones de datos de Kinesis, o puede crear sus propios consumidores personalizados mediante la biblioteca de clientes de Amazon Kinesis o KCL. Tambi칠n puede configurar las cosas para que la salida de una transmisi칩n se convierta en la entrada de otra, lo que puede permitirle crear flujos de trabajo de procesamiento de datos en tiempo real m치s complejos. Los consumidores tambi칠n pueden enviar datos a otros servicios de ADS, como la integraci칩n con Amazon Data Firehose para almacenar datos en Amazon S3. Tambi칠n es importante recordar que Kinesis Data Streams permite que varias aplicaciones funcionen con la misma transmisi칩n al mismo tiempo. Cada uno consume los datos de forma independiente y los env칤a de forma descendente a diferentes sistemas. A continuaci칩n, Joe le explicar치 los detalles del pr칩ximo laboratorio. Luego, usted mismo se pondr치 manos a la obra en la ingesti칩n de streaming con Kinesis. Buena suerte y divi칠rtete.

쯈u칠 es la Captura de Datos de Cambios (CDC)?
Estado: Traducido autom치ticamente del Ingl칠s
Traducido autom치ticamente del Ingl칠s
Informaci칩n:
Este elemento incluye contenido que a칰n no se tradujo a tu idioma preferido.
쯈u칠 es CDC? 

Supongamos que ha extra칤do y cargado datos de una base de datos en su sistema de almacenamiento. Al cabo de un tiempo, puede que necesite actualizar los datos almacenados en su sistema de almacenamiento para asegurarse de que est치n sincronizados con los datos del sistema de origen. Existen dos estrategias para ello:

    Instant치neas completas o carga completa: en este enfoque, cada vez que desee actualizar los datos almacenados en su sistema, ingestar치 todos los datos de su sistema de origen, sustituyendo los datos antiguos almacenados por los nuevos datos actualizados. Si sus datos son tabulares, la carga completa de los datos significa que elimina todos los datos antiguos de la tabla almacenada y extrae todas las filas de la tabla de origen cada vez que necesite actualizar sus datos almacenados. Se trata de un enfoque sencillo que garantiza la coherencia entre los datos del sistema de origen y los datos almacenados en su canalizaci칩n de datos. Sin embargo, para datos de gran volumen, puede tardar mucho tiempo en ejecutarse y requerir muchos recursos de procesamiento y memoria. Es m치s adecuado para casos en los que no es necesario actualizar los datos con frecuencia.

    Carga incremental (diferencial): en este enfoque, s칩lo se cargan las actualizaciones y los cambios desde la 칰ltima lectura de los sistemas de origen. Por ejemplo, al cargar actualizaciones de una base de datos de origen, puede utilizar una columna last_updated_at para identificar las filas de datos que se han actualizado desde la 칰ltima lectura de esta base de datos de origen y, a continuaci칩n, cargar s칩lo los datos actualizados de estas filas identificadas. Aunque este m칠todo es m치s r치pido que el de carga completa, especialmente para grandes vol칰menes de datos, su aplicaci칩n puede requerir una l칩gica m치s compleja. Cuando se trabaja con bases de datos, este proceso se conoce como Captura de datos de cambios o (CDC). Seg칰n el libro Fundamentos de la ingenier칤a de datos, "La captura de datos de cambios (CDC) es un m칠todo para extraer cada evento de cambio (inserci칩n, actualizaci칩n, eliminaci칩n) que se produce en una base de datos" y ponerlo a disposici칩n de los sistemas posteriores.

Casos de uso de CDC

    CDC le ayuda a sincronizar datos entre diferentes bases de datos, soportando la replicaci칩n continua de bases de datos. Por ejemplo, es posible que tenga un sistema PostgreSQL de origen que soporte una aplicaci칩n y desee ingerir de forma peri칩dica o continua los cambios de las tablas en un almac칠n de datos para permitir el an치lisis basado en los datos m치s recientes. O si trabaja en una empresa h칤brida, podr칤a necesitar usar CDC para capturar cambios en bases de datos locales y aplicar esos cambios a bases de datos en la nube.

    CDC le ayuda a capturar todos los cambios hist칩ricos con fines de auditor칤a y otros fines empresariales. Por ejemplo, algunas empresas est치n obligadas a mantener informaci칩n hist칩rica completa de las compras de sus clientes con fines normativos, o para extraer informaci칩n que permita a las empresas mejorar.

    CDC permite a los microservicios rastrear cualquier cambio en la base de datos de origen. Por ejemplo, considere un microservicio que gestiona pedidos de compra. Cuando se realiza un nuevo pedido, puede utilizar CDC para transmitir informaci칩n al servicio de env칤os y al servicio de atenci칩n al cliente.

Dos enfoques de CDC

    Push: Este enfoque requiere que implemente alg칰n tipo de l칩gica o proceso para capturar los cambios en la base de datos de origen. A continuaci칩n, depende de la base de datos de origen para empujar cualquier actualizaci칩n de datos al sistema de destino cuando algo cambia en el sistema de origen. Este m칠todo permite que los sistemas de destino se actualicen con los datos m치s recientes casi en tiempo real, pero si no se configura correctamente, se corre el riesgo de perder las actualizaciones de datos si los sistemas de destino est치n inaccesibles cuando los sistemas de origen intentan enviar los cambios.

    Pull: este m칠todo requiere que los sistemas de destino sondeen continuamente la base de datos de origen para comprobar si hay cambios y, a continuaci칩n, extraigan las actualizaciones de datos cuando se produzcan. Este m칠todo suele dar lugar a un retraso antes de que los sistemas de destino introduzcan nuevas actualizaciones de datos, ya que los cambios suelen producirse por lotes entre las solicitudes de extracci칩n.

Modelos de implementaci칩n de CDC

Existen varios m칠todos para que CDC extraiga los cambios de las bases de datos.

    CDC orientado a lotes o basado en consultas (pull-based): En este enfoque, se consulta la propia base de datos para identificar si se ha producido un cambio en los datos. En el caso de las bases de datos relacionales, esto requiere que la base de datos tenga una columna adicional denominada updated_at, last_updated o last_modified que le ayude a encontrar todas las filas actualizadas m치s all치 de un cierto tiempo especificado. Este proceso permite extraer los cambios y actualizar de forma incremental una tabla de destino. Sin embargo, este enfoque puede a침adir sobrecarga computacional al sistema de origen, ya que los sistemas de destino tienen que escanear cada fila de la tabla para identificar los 칰ltimos valores actualizados.

    CDC continuoo basado en registros (pull-based): En lugar de ejecutar consultas peri칩dicas para obtener los cambios de la tabla como un lote, puede tratar cada actualizaci칩n de la base de datos como un evento utilizando CDC continuo. Este tipo de CDC se basa en la comprobaci칩n del registro de la base de datos. Un registro de base de datos registra cada cambio en la base de datos de forma secuencial (por ejemplo, cada creaci칩n, actualizaci칩n, eliminaci칩n) y se utiliza en caso de fallo para restaurar el estado de la base de datos. Puede leer los eventos de este registro (escribiendo su propio c칩digo o utilizando una herramienta CDC como Debezium) y enviarlos a una plataforma de streaming, como Apache Kafka. De esta forma, puede capturar los cambios de datos en tiempo real sin incurrir en ninguna sobrecarga computacional ni requerir la necesidad de una columna adicional en las bases de datos de origen.

    CDC basado en desencadenantes (m칠todo basado en push): Un desencadenante es una funci칩n almacenada que puede configurar para que se ejecute cuando cambie una columna espec칤fica. Los disparadores informan al CDC de los cambios en las bases de datos de origen y, de este modo, se libera al CDC de la detecci칩n de cambios. Sin embargo, un n칰mero excesivo de triggers puede afectar negativamente al rendimiento de escritura de la base de datos de origen.

Herramientas para CDC

Si칠ntase libre de leer m치s sobre algunas de las herramientas comunes utilizadas para implementar CDC

    Debezium

AWS DMS

API de conexi칩n Kafka

CDC basado en registros Airbyte


Resumen: Consideraciones generales para elegir las herramientas de ingesti칩n
Estado: Traducido autom치ticamente del Ingl칠s
Traducido autom치ticamente del Ingl칠s

A la hora de elegir una herramienta de ingesti칩n para sus sistemas de datos, debe tener en cuenta las caracter칤sticas de los datos que va a ingestar, as칤 como la fiabilidad y durabilidad de la herramienta de ingesti칩n.

Caracter칤sticas de los datos

Nota: En el libro "Fundamentos de la ingenier칤a de datos", Joe y Matt se refieren a las caracter칤sticas de los datos como la carga 칰til de los datos, que incluye el tipo de datos (tipo y formato), la forma, el tama침o, el esquema y los tipos de datos, y los metadatos.

    Tipo de datos y estructura: En el curso 1 aprendimos que los datos de los sistemas fuente pueden ser estructurados, no estructurados o semiestructurados. A la hora de decidir c칩mo ingerir los datos y qu칠 herramienta elegir, es necesario conocer el tipo y la estructura de los datos (por ejemplo, una imagen en formato PNG) para poder identificar la herramienta de ingesta adecuada y las transformaciones que podr칤a ser necesario aplicar m치s adelante.

    Volumen de datos: En cuanto al Volumen de datos, hay que tener en cuenta dos cosas:

        El tama침o en bytes de los datos existentes que necesita ingestar: En el caso de la ingesta por lotes, debe tener en cuenta el tama침o de los datos hist칩ricos que necesita ingerir. 쯉e pueden ingerir todos los datos hist칩ricos en un solo lote? Dependiendo de la conexi칩n de red entre el sistema de origen y el sistema de destino, puede ser posible transferir los datos hist칩ricos a trav칠s de la red, pero si tiene un ancho de banda limitado, puede que tenga que dividir la carga masiva en trozos, lo que reduce efectivamente el tama침o de la carga en subsecciones m치s peque침as.En el caso de la ingesta de streaming, debe tener en cuenta el tama침o del mensaje. Debe asegurarse de que la herramienta de ingesta de streaming puede manejar el tama침o m치ximo esperado del mensaje. Por ejemplo, Amazon Kinesis Data Streams admite un tama침o m치ximo de mensaje de 1 MB, mientras que Kafka admite por defecto este tama침o m치ximo, pero se puede configurar para que admita un tama침o m치ximo de datos de 20 MB o m치s.

        El tama침o de los datos futuros que puede ingerir con la misma canalizaci칩n: 쯖칩mo espera que crezcan los datos? 쮺u치l es el crecimiento diario, mensual o anual de los datos? Considerar el tama침o actual y futuro le ayuda a entender c칩mo configurar su herramienta y qu칠 coste anticipar para garantizar que su sistema de ingesta satisface las demandas.

    Requisitos de latencia: A la hora de dise침ar su canalizaci칩n, uno de los requisitos de las partes interesadas que debe tener en cuenta es la latencia: 쯔 qu칠 velocidad quieren operar con los datos? 쮺u치l es el retraso aceptable? 쯅ecesitan extraer informaci칩n de los datos un d칤a despu칠s de que se ingieran, o necesitan informaci칩n casi en tiempo real? Dicho de otro modo, 쯥e trata de un escenario por lotes, en el que los datos deben incorporarse una vez al d칤a, a la semana o al mes? Para cumplir el requisito de latencia, hay que pensar en la rapidez con la que hay que procesar los datos ingeridos una vez que llegan a la canalizaci칩n y comprender tambi칠n la rapidez con la que se generan los datos de origen. La velocidad de los datos influir치 en las herramientas (batch o streaming) que elija para ingerir y procesar los datos.

    Calidad de los datos: 쮼st치n los datos de origen en buen estado para su uso posterior inmediato? 쯈u칠 tratamiento posterior es necesario para servirlos? Dependiendo de los sistemas de origen, los datos pueden estar incompletos o contener informaci칩n incoherente, duplicados o errores. Si no se espera que los datos est칠n en buen estado, es posible que tenga que comprobar la calidad de los datos ingeridos para solucionar cualquier problema. Algunas herramientas de ingesta pueden ayudarle a completar los valores que faltan o a detectar/corregir incoherencias o entradas no v치lidas. Obtendr치 m치s informaci칩n sobre las comprobaciones de calidad en el pr칩ximo curso.

    Cambiosen el esquema:los cambios en el esquema(por ejemplo, a침adir una nueva columna, cambiar el tipo de columna, crear una nueva tabla, cambiar el nombre de una columna) se producen con frecuencia en los sistemas fuente y, por lo general, est치n fuera de su control. Si espera que estos cambios se produzcan con frecuencia, puede que tenga que considerar el uso de herramientas de ingesta que detecten autom치ticamente los cambios de esquema. Sin embargo, la comunicaci칩n entre usted y las partes interesadas es tan importante como la automatizaci칩n que comprueba los cambios de esquema.

Fiabilidad y durabilidad 

La fiabilidad y la durabilidad son dos aspectos importantes en la fase de ingesta. Fiabilidad significa asegurarse de que los sistemas de ingesta cumplen correctamente su funci칩n. Durabilidad significa asegurarse de que los datos no se pierden ni se corrompen. Si dise침a un sistema de ingesti칩n fiable, garantizar치 la durabilidad de los datos ingestados. Por ejemplo, los sistemas de streaming, como los dispositivos IoT, no retienen los eventos indefinidamente, por lo que si no ingiere correctamente sus datos, estos pueden perderse. Aseg칰rate de comprender las caracter칤sticas de los sistemas de origen y las herramientas de ingesta.

Consejos: Eval칰e las compensaciones entre el coste de perder datos y la creaci칩n de un nivel adecuado de redundancia. Para m치s informaci칩n y consideraciones, consulte el cap칤tulo 7 de Fundamentos de la ingenier칤a de datos.


En el curso anterior, implement칩 una canalizaci칩n de streaming similar a la siguiente con un bucket de Kinesis Data Firehose y S3. Se le proporcion칩 un Kinesis Data Stream que transmite las actividades de los usuarios en l칤nea como eventos o registros. Procesas estos registros para calcular las recomendaciones de productos y usar la instancia de Data Firehose para enviar los registros al bucket de S3 de tu canalizaci칩n. En el pr칩ximo laboratorio, obtendr치 m치s informaci칩n sobre c칩mo puede continuar transmitiendo estos registros en proceso y explorar m치s a fondo Kinesis Data Streams como fuente. El laboratorio consta de dos partes. En la primera parte, trabajar치 con un Kinesis Data Stream que act칰a como un enrutador entre un productor simple y un consumidor simple. En la segunda parte, volver치s a trabajar con el escenario del curso 1. Se le proporcionar치 una transmisi칩n de datos de Kinesis como fuente, pero esta vez usar치 otras dos transmisiones de datos de Kinesis para continuar transmitiendo los registros en proceso. De cada uno de estos dos nuevos flujos de datos, un Data Firehose tomar치 los datos y los entregar치 al bucket S3 correspondiente. Repasemos la primera parte de este laboratorio. Para comprender mejor los componentes de una plataforma de transmisi칩n de eventos, primero crear치 una transmisi칩n de datos de Kinesis y, a continuaci칩n, interactuar치 con ella desde el punto de vista del productor y el consumidor. Para ello, se le proporcionan dos scripts de Python, consumer desde la CLI y producer desde la CLI. El script de productor representa una aplicaci칩n de productor simple que escribe un 칰nico registro de datos en Kinesis Data Stream. El registro de datos es una cadena JSON que contiene los detalles de una sesi칩n de usuario, como el identificador de sesi칩n, el n칰mero de cliente, la ciudad, el pa칤s y el historial de navegaci칩n. El script de Python del productor usa Boto3 para interactuar con Kinesis. Puede ejecutar el script desde la terminal, que espera dos argumentos: el nombre de la transmisi칩n de datos de Kinesis y la cadena JSON de un solo registro. En el script del productor, estos dos argumentos se pasan al m칠todo PutRecord de Boto3 para escribir el registro en Kinesis Data Stream. El script de consumidor tambi칠n representa una aplicaci칩n de consumidor sencilla que puede ejecutar desde el terminal especificando el nombre de la transmisi칩n de datos de Kinesis. Cuando ejecutes el script de consumidor, recorrer치 todos los fragmentos del flujo de datos, extraer치 todos los registros de cada fragmento y, a continuaci칩n, imprimir치 informaci칩n en el terminal sobre cada registro. Si compruebas el c칩digo del script de consumo, puedes ver que tambi칠n se usa Boto3. En la funci칩n PullShards, el consumidor recorre continuamente los fragmentos del flujo de datos, edita los registros con el m칠todo getRecords de Boto3 y, a continuaci칩n, imprime este texto en la terminal para explicar qu칠 registro se ley칩 desde qu칠 fragmento y en qu칠 posici칩n dentro del fragmento. En la primera parte del laboratorio, no editar치s los guiones para productores y consumidores, pero tendr치s la tarea de ejecutar estos guiones. Despu칠s de crear Kinesis Data Stream, primero ejecutar치 el script de consumidor en la terminal. Para ello, primero activar치 el entorno de JupyterLab, despu칠s ir치 a la carpeta SourceCLI o srcCLI y, por 칰ltimo, ejecutar치 el script de consumidor como se muestra aqu칤, proporcion치ndole el nombre de la transmisi칩n de Kinesis que cre칩. Observar치 que no se imprimir치 nada en el terminal porque el flujo de datos ahora est치 vac칤o. Mantendr치s este terminal abierto para que el consumidor siga funcionando y, a continuaci칩n, en otro terminal, ejecutar치s el script del productor para escribir un registro en el flujo de datos. De nuevo, en la nueva terminal, navegar치 hasta la carpeta SourceCLI o srcCLI y, a continuaci칩n, ejecutar치 el script del productor especificando el nombre de la transmisi칩n de Kinesis y la cadena JSON que representa el registro. Ahora, si compruebas el primer terminal en el que est치 funcionando el consumidor, ver치s que el consumidor ha le칤do el registro que acabas de enviar al flujo de datos. Cuando hayas terminado con la primera parte del laboratorio, volver치s al escenario original de comercio electr칩nico. Se le proporcionar치 una transmisi칩n de datos de Kinesis que representa su sistema de origen, por lo que estar치 del lado del consumidor al ingerir datos de la fuente de transmisi칩n. Implementar치 una canalizaci칩n de ETL de streaming, en la que primero aplicar치 una transformaci칩n simple en los registros ingeridos y, a continuaci칩n, continuar치 transmitiendo estos registros en su canalizaci칩n. Para ello, configurar치 dos transmisiones de datos de Kinesis. Enviar치 los registros que corresponden a clientes de EE. UU. a un flujo de datos y los que corresponden a clientes internacionales a otro flujo de datos. Suponiendo que su empresa haya notado que los clientes muestran diferentes comportamientos de compra en funci칩n de sus pa칤ses. Por lo tanto, si se encuentran en los EE. UU., sus actividades en l칤nea deben ser procesadas por un determinado motor de recomendaciones. De lo contrario, sus actividades en l칤nea deben ser procesadas por otro motor de recomendaciones. Para cada uno de estos dos flujos de datos, una manguera de datos recopilar치 autom치ticamente los datos y los entregar치 a su respectivo bucket de S3. En primer lugar, crear치 los dos flujos de datos, las dos instancias de Firehose y los dos dep칩sitos con Boto3. A continuaci칩n, escribir치 el c칩digo de transformaci칩n en el script de consumidor que se le proporciona en esta carpeta ETL. Este script tambi칠n se puede ejecutar desde el terminal y espera el nombre del flujo de datos de origen y los nombres de los dos flujos de datos de destino. El script contiene esta funci칩n de extracci칩n de fragmentos, en la que se le proporciona el c칩digo que recorre los fragmentos para extraer los registros. Aqu칤, una vez que se extrae un registro, tendr치 que completar esta parte del c칩digo para transformar el registro. La transformaci칩n consiste en a침adir tres campos, como se muestra aqu칤. El primer campo, cantidad total de productos, representa la suma de las cantidades de productos que aparecen en el historial de navegaci칩n. El segundo campo, en general en el carrito de la compra, representa la suma de las cantidades de productos de los productos que se colocan en el carrito de la compra. Y el tercer campo, el total de productos diferentes, representa el n칰mero de productos que aparecen en el historial de navegaci칩n. Y, por 칰ltimo, enviar치 el registro de transformaci칩n al flujo de datos correspondiente en funci칩n del valor del campo de pa칤s. Tras aplicar estas modificaciones al script del consumidor, ejecutar치 el script en el terminal especificando el nombre del flujo de datos de origen y los nombres de los dos flujos de datos de destino, como se muestra aqu칤. Al ejecutar este comando, el script del consumidor leer치 los registros de la transmisi칩n de datos de origen, los transformar치 y, a continuaci칩n, los enviar치 a las transmisiones de Kinesis correspondientes. Los 칤ndices Firehose de Kinesis entregar치n autom치ticamente los datos a los dep칩sitos de S3. Esta fue una descripci칩n general de las tareas que realizar치 en este laboratorio, y ahora est치 listo para comenzar. Cuando termines el laboratorio, acomp치침ame aqu칤 para ver un resumen r치pido de esta semana.


C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "C2W3_great_expectations.txt"  
 echo.
) 
En el siguiente laboratorio, utilizar치 Great Expectations para aplicar pruebas de calidad de datos. Pero antes de continuar, me gustar칤a ofreceros una visi칩n general de los componentes principales de Great Expectations con un ejemplo de flujo de trabajo. Cuando trabajas con Great Expectations, normalmente comienzas tu flujo de trabajo especificando los datos que deseas probar. A continuaci칩n, define las expectativas o las pruebas que desea realizar con los datos y, por 칰ltimo, valida los datos en funci칩n de sus expectativas. Para implementar un flujo de trabajo de este tipo, debe interactuar con los componentes principales de Great Expectations, que consisten en el contexto de los datos, las fuentes de datos, las expectativas y los puntos de control. Estos componentes se utilizan para acceder, almacenar y gestionar los objetos y procesos necesarios en el flujo de trabajo. Para iniciar el flujo de trabajo, primero debe crear una instancia de un objeto de contexto de datos. Un contexto de datos sirve como punto de entrada para la API Great Expectations, que consta de clases y m칠todos que permiten crear objetos para conectarse a las fuentes de datos, crear expectativas y validar los datos. Con el contexto de datos, puede configurar y acceder a las propiedades, como los objetos, as칤 como a los metadatos de su proyecto Great Expectations. Despu칠s de crear una instancia del objeto de contexto de datos, debe declarar el objeto de la fuente de datos, lo que indica las grandes expectativas de d칩nde obtener los datos que desea validar. La fuente de datos puede ser una base de datos SQL, un sistema de archivos local, un bucket de S3 o incluso un marco de datos de pandas. Despu칠s de conectarse a la fuente de datos, debe indicar a Great Expectations en qu칠 parte de los datos debe centrarse. Para ello, declare sus activos de datos desde la fuente de datos. Un activo de datos es una colecci칩n de registros dentro de una fuente de datos. Puede ser una tabla de una base de datos SQL o un archivo de un sistema de archivos. Tambi칠n puede ser un recurso de consulta que une datos de m치s de una tabla, o puede ser una colecci칩n de archivos que coincidan con un patr칩n de expresiones regulares en particular. Puede dividir a칰n m치s los datos de su activo en lotes. Por ejemplo, si su activo de datos representa los registros que corresponden a las ventas de un a침o determinado en una tabla, puede dividir los registros en lotes mensuales y validar cada lote. O puede particionar sus datos con respecto a los identificadores de la tienda. Tambi칠n puede trabajar con todos los registros de su activo de datos como un solo lote para recuperar los lotes de su activo de datos. Ya sea un lote o varios lotes, debe crear un objeto de solicitud por lotes a partir de su activo. Las solicitudes por lotes son la forma principal de recuperar datos del activo de datos y es lo que debe proporcionar para el resto de los componentes de Great Expectations. A continuaci칩n, debe definir sus expectativas. Una expectativa es una declaraci칩n que puedes usar para verificar si tus datos cumplen una condici칩n determinada. Por ejemplo, puede definir una expectativa para comprobar si una columna no contiene valores nulos. Puede definir su propia expectativa o usar una de las declaraciones disponibles en la galer칤a de expectativas. Por ejemplo, esperar que el m칤nimo de la columna est칠 entre, esperar que los valores de la columna sean 칰nicos y esperar que los valores de la columna sean nulos son ejemplos de pruebas que puedes usar directamente. Ver치s c칩mo puedes llamarlos en el ejemplo de flujo de trabajo en el que trabajaremos. Tambi칠n puede definir m치s de una expectativa para su activo de datos y recopilarlas en un objeto de conjunto de expectativas. Ahora, para validar sus datos, debe crear un objeto validador que espere una solicitud por lotes y su correspondiente conjunto de expectativas. Puede validar manualmente los datos interactuando directamente con el validador, o puede simplificar el proceso de validaci칩n mediante un objeto de punto de control. Un punto de control toma una solicitud por lotes y un conjunto de expectativas y los proporciona autom치ticamente a un validador que genera los resultados de la validaci칩n. A lo largo de este proceso, se generar치n metadatos sobre su proyecto y, con grandes expectativas, los guardar치n en algunos almacenes de back-end. Great Expectations es compatible con diferentes tipos de tiendas. Las tiendas m치s comunes son Expectation Store, donde puede encontrar sus suites de expectativas. El almac칠n de validaci칩n, donde puede encontrar informaci칩n sobre los objetos generados al validar los datos compar치ndolos con el conjunto de expectativas. La tienda Checkpoint, donde puede encontrar las configuraciones de sus puntos de control. Y una tienda de documentos de datos, donde puede encontrar informes sobre expectativas, puntos de control y resultados de validaci칩n. Puede acceder a estos almacenes y a sus ajustes a trav칠s del objeto de contexto de datos. Estos son los pasos de un flujo de trabajo t칤pico de Great Expectations. En el siguiente v칤deo, aplicaremos estos pasos en un conjunto de datos de ejemplo.

En el v칤deo anterior, ha aprendido acerca de los componentes b치sicos de las expectativas de calificaci칩n y c칩mo es un flujo de trabajo de validaci칩n t칤pico. Apliquemos ahora estos pasos en un conjunto de datos de ejemplo, que es la Base de datos de alquiler de DVD, que ha visto en uno de los laboratorios de la primera semana de este curso. S칩lo nos centraremos en comprobar las columnas de la tabla de pagos. En concreto, comprobaremos si la columna id de pago contiene ids 칰nicos, la columna id de cliente no contiene valores nulos, y todos los valores de la columna importe son no negativos. Ya he configurado una base de datos postgresSQL localmente en mi m치quina y cargado los datos en la base de datos. En el terminal har칠 pip install grandes expectativas. Despu칠s, para iniciar el proyecto grandes expectativas, escribir칠 great expectations init. Este comando inicializar치 el objeto de contexto de datos, configurar la estructura de la carpeta de su proyecto como se muestra aqu칤, y crear sus tiendas backend como los puntos de control, las expectativas, datos docs, y las tiendas de validaci칩n como directorios locales. Escribir칠 Y para proceder, siempre se puede cambiar la ubicaci칩n de sus tiendas backend. As칤, por ejemplo, en el laboratorio va a configurar sus tiendas como s tres cubos. En este video ill mantener estos como directorios locales. Ahora, para interactuar con los componentes de grandes expectativas, voy a lanzar un Jupyter notebook en el mismo directorio ra칤z y crear este archivo notebook, ejemplo. Aqu칤 en el archivo notebook, primero importar칠 el paquete de grandes expectativas y luego llamar칠 al m칠todo get context, para obtener el objeto context del proyecto. Usando este objeto, puedes conectarte a la fuente de datos, definir tus expectativas, crear un validador, y luego ejecutar tus puntos de control. As칤 que vamos a utilizar este contexto para crear primero el objeto fuente de datos. Create expectations proporciona diferentes m칠todos que te permiten conectarte a tus diferentes fuentes de datos. Para conectarme a mi base de datos SQL local, llamar칠 al m칠todo context sources add sql, y luego elegir칠 my datasource como nombre para la fuente de datos. Este m칠todo tambi칠n espera una cadena de conexi칩n que incluye la informaci칩n necesaria para conectarse a la base de datos, como el nombre de usuario, la contrase침a, el nombre de host, el n칰mero de puerto y el nombre de la base de datos. Este es el formato de la cadena que puedes usar para conectarte a una base de datos postgresql . Usando este formato, crear칠 la cadena de conexi칩n a mi base de datos usando la informaci칩n de mi base de datos local. A continuaci칩n, desde el origen de datos, crear칠 el activo de datos llamando al m칠todo add tableasset. Como s칩lo nos estamos centrando en la tabla de pagos, elegir칠 el nombre para el activo como payment tb y luego especificar칠 el nombre de la tabla dentro de la base de datos origen, que es payment. En este caso. Ahora, si quieres crear lotes en tu activo, grandes expectativas proporciona un conjunto de m칠todos que te permite dividir tu activo de datos basado en la fecha o un valor de columna. As칤 que por ejemplo, llamar칠 aqu칤 al m칠todo add splitter datetime part en el activo que acabo de crear, y luego indicar칠 el nombre de la columna que contiene la fecha, que es payment date, y especificar칠 month para el segundo argumento datetime parts. Y por 칰ltimo, crear칠 el objeto de solicitud de lote llamando al m칠todo build batch request sobre el objeto asset. Antes de continuar con el flujo de trabajo, echemos un vistazo r치pido a los lotes. Usando el objeto asset, llamar칠 al m칠todo get batch list from batch requests y pasar칠 el objeto de solicitud de lote que acabo de crear y luego iterar칠 sobre los lotes para comprobar la especificaci칩n de cada lote. Puedes ver que los datos contienen cuatro lotes donde cada lote corresponde a un mes. Ahora que tenemos el objeto batch request creado, vamos a definir las expectativas. Lo har칠 de forma interactiva trabajando directamente con un validador. Primero, crear칠 el conjunto de expectativas que contendr치 todas las expectativas que definir치. Sobre el objeto context llamar칠 al m칠todo add o update expectation suite y elegir칠 el nombre mysuite. Ahora, para crear el validador, utilizar칠 el objeto de contexto para llamar al m칠todo get validator y introducir칠 el objeto de solicitud de lote y el nombre del conjunto de expectativas. Ahora, utilizando el validador, puedes llamar a cualquiera de los m칠todos de expectativas desde la galer칤a de expectativas. As칤 que aqu칤 llamar칠 a expect column values to be unique para comprobar la unicidad de los valores dentro de la columna payment id. Los resultados que se muestran aqu칤, corresponden al 칰ltimo lote. Sin embargo, todos los lotes se probaron y, puesto que llegamos al 칰ltimo lote, significa que la prueba se ejecut칩 correctamente en todos los dem치s lotes. Ahora definir칠 dos expectativas m치s. Esperar que los valores de columna no sean nulos en la columna id de cliente para comprobar que la columna no contiene ning칰n valor nulo, y Esperar que el valor m칤nimo de columna est칠 entre en la columna importe para comprobar que todos los valores de la columna son nulos negativos. Puede ver que ambas pruebas se ejecutan correctamente. Ahora estas tres expectativas que he creado interactivamente mientras trabajaba con el validador s칩lo est치n disponibles para esta sesi칩n. Para poder utilizar estas expectativas en otras sesiones, es necesario guardarlas. Usando el validador, llamar칠 al m칠todo guardar conjunto de expectativas y especificar칠 para grandes expectativas que no descarte las expectativas fallidas, este m칠todo guarda el conjunto de expectativas, mi conjunto que contiene las tres expectativas en el almac칠n de expectativas. Ahora, vamos a automatizar el proceso de la validaci칩n utilizando el objeto checkpoint. Para crear el objeto checkpoint, llamar칠 al m칠todo add o update checkpoint en el objeto context y le asignar칠 el nombre mycheckpoint. Este m칠todo espera validaciones como segundo argumento que consiste en una lista de pares de un lote de datos y su correspondiente conjunto de expectativas de para especificar estas validaciones he iterado a trav칠s de los lotes y para cada lote he extra칤do la petici칩n del lote y he usado el nombre del conjunto de expectativas. Ahora ejecutar칠 el punto de comprobaci칩n llamando al m칠todo run, la prueba de los cuatro lotes superada. Para obtener informaci칩n m치s detallada sobre cada resultado, puedes consultar los documentos de datos llamando al m칠todo build data docs utilizando el objeto context. Este m칠todo devuelve un enlace que abrir칠 para consultar los documentos de datos. Aqu칤 puedes encontrar los resultados de las validaciones realizadas en cada lote. Puedes ver que todas las pruebas fueron exitosas. Si haces clic en cualquier fila, puedes encontrar estad칤sticas sobre las expectativas evaluadas, as칤 como las expectativas exitosas y no exitosas. Tambi칠n puedes encontrar las expectativas realizadas en cada columna y el resultado correspondiente. As칤 que vamos a hacer clic en mostrar m치s informaci칩n. Aqu칤 encontrar치s algunos metadatos sobre la ejecuci칩n del punto de control y los lotes utilizados. Ahora, volvamos a la p치gina de inicio y hagamos clic en la pesta침a suites de expectativas. Aqu칤 encontrar치s informaci칩n sobre la suite de expectativas, mi suite. Y ah칤 lo tienes. Ahora ya sabes c칩mo interactuar con los componentes principales de las expectativas de grado. Ahora, es tu turno de practicar el uso de grandes expectativas y validar algunos datos en el laboratorio.

[Opcional] Conversaci칩n y notas adicionales
Estado: Traducido autom치ticamente del Ingl칠s
Traducido autom치ticamente del Ingl칠s
Informaci칩n:
Este elemento incluye contenido que a칰n no se tradujo a tu idioma preferido.
Principales conclusiones de la conversaci칩n con Barr Moses

Los problemas con los datos son inevitables y pueden producirse en cualquier fase del proceso de datos. Cuanto antes se detecten, menos da침o causar치n a la organizaci칩n. Para detectar problemas con los datos, primero hay que elegir indicadores que eval칰en la calidad de los datos, de forma similar a c칩mo los equipos de software controlan los indicadores que eval칰an la salud de la infraestructura de su software.

En su libro(Data Quality Fundamentals

), Barr Moses sugiere empezar con las siguientes preguntas:

    쮼st치n actualizados los datos?

    쮼st치n completos los datos?

    쮼st치n los campos dentro de los rangos esperados?

    쮼s el 칤ndice de nulos mayor o menor de lo que deber칤a?

    쮿a cambiado el esquema?

Formul칩 estas preguntas en 5 pilares para la observabilidad de los datos, cuyo objetivo es describir completamente el estado de los datos. 
Los 5 pilares de Barr Moses

    Distribuci칩n/ Calidad interna: El pilar de la calidad se refiere a las caracter칤sticas internas de los datos, y comprueba m칠tricas como el porcentaje de elementos NULOS, el porcentaje de elementos 칰nicos, las estad칤sticas de resumen y si tus datos est치n dentro de lo esperado. Le ayuda a asegurarse de que sus datos son fiables en funci칩n de sus expectativas de datos.

    Frescura: La frescura de los datos se refiere a lo "frescos" o "actualizados" que est치n los datos dentro del activo final (tabla, informe BI), es decir, cu치ndo se actualizaron los datos por 칰ltima vez y con qu칠 frecuencia se actualizan. Los datos obsoletos suponen una p칠rdida de tiempo y dinero.

    Volumen: Volumen de datos se refiere a comprobar la cantidad de datos ingeridos y buscar picos o ca칤das inesperadas. Las ca칤das repentinas en el volumen de datos pueden indicar problemas como p칠rdida de datos o interrupciones del sistema, y los aumentos repentinos pueden indicar aumentos inesperados en el uso.

    Linaje: Seg칰n Barr

    , "cuando los datos se rompen, la primera pregunta es siempre "쯗칩nde?" El linaje de datos le ayuda a trazar el viaje de los datos desde su origen hasta su destino, visualizando c칩mo se transformaron los datos y d칩nde se almacenaron. De este modo, se puede identificar el origen de errores o anomal칤as.  

    Esquema: El esquema de datos se refiere a la supervisi칩n de los cambios en la estructura o los tipos de datos. Este pilar ayuda a evitar fallos en la canalizaci칩n de datos.

Recursos adicionales

Si desea obtener m치s informaci칩n sobre la observabilidad de los datos, puede consultar los siguientes recursos adicionales.

    Fundamentos de la calidad de datos 

, por Barr Moses, Lior Gavish, Molly Vorweck [libro]

The rise of data downtime

, por Barr Moses [art칤culo]

쯈u칠 es la observabilidad de los datos?
 por Andy Petrella [libro]


C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "cat ...awsaws_console_url.txt"  
 echo.
) 
cat ../.aws/aws_console_url

    username: postgres
    password: postgrespwrd o adminpwrd


aws rds describe-db-instances --db-instance-identifier de-c2w1a1-rds --output text --query "de-c2w1a1-rds.ct4yggiam6pg.us-east-1.rds.amazonaws.com" --region us-east-1

de-c2w1a1-rds.ch668gmkcdla.us-east-1.rds.amazonaws.com
de-c2w1a1-rds.ct4yggiam6pg.us-east-1.rds.amazonaws.com

psql --host=de-c2w1a1-rds.ct4yggiam6pg.us-east-1.rds.amazonaws.com --username=postgres --password --port=5432

s3
de-c2w1a1-395639430833-us-east-1-data



C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "draft.txt"  
 echo.
) 
38084256712
378.190,04


MYSQL_CONNECTION_STRING: mysql+pymysql://admin:adminpwrd@de-c2w3a1-rds.cdi6y0y2yhrw.us-east-1.rds.amazonaws.com:3306/taxi_trips


GXArtifactsS3Bucket
	
de-c2w3a1-891377276567-us-east-1-gx-artifacts
	
This is the S3 Bucket for GX Artifacts


GXDocsS3Bucket
	
de-c2w3a1-891377276567-us-east-1-gx-docs
	
This is the S3 Bucket for GX Docs



C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "resultado.txt"  
 echo.
) 

C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "C2W1-1.txt"  
 echo.
) 
Bienvenido al segundo curso de la espec

C:\Users\Alfredo\Desktop\Alfredo\Cursos\Data Engineering\Curso 2 DE 4 Source Systems Data Ingestion and Pipelines>(
type "user_sessions-py.txt"  
 echo.
) 
import datetime as dt
import json
import logging
from random import randint

import boto3
import requests
from airflow import DAG
from airflow.operators.dummy import EmptyOperator
from airflow.operators.python import PythonOperator

############################# START OF EXERCISE 1 #############################

### START CODE HERE ### (1 line of code)
# Assign the name of your _Raw Data Bucket_ to `DATA_BUCKET` constant
# replacing the placeholder `<RAW-DATA-BUCKET>`
RAW_DATA_BUCKET = "<RAW-DATA-BUCKET>"
### END CODE HERE ###

### START CODE HERE ### (1 line of code)
# Instantiate a boto3 client using method `boto3.client()`, establishing a
# connection with `s3`
client = boto3.client("s3")
### END CODE HERE ###

logger = logging.getLogger()
logger.setLevel("INFO")

# Define the directed acyclic graph:
with DAG(
    dag_id="user_sessions",
    ### START CODE HERE ### (1 line of code)
    # Set the `start_date` as a `datetime` object representing a
    # date 7 days before the date 2020-10-17. This can be done using the function
    # `dt.timedelta()` with the parameter `days` equal to `7`
    start_date=dt.datetime(2020, 10, 17) - dt.timedelta(days=7),
    ### END CODE HERE ###
    end_date=dt.datetime(2020, 10, 17),
    schedule="@daily",
    # Setting `catchup=True` (which is the default) in combination with a
    # `start_date` in the past means that, immediately after the DAG is
    # activated, Airflow will execute all the DAG runs corresponding to the
    # days between `start_date` and the current date. You'll take advantage of
    # that to have the DAG automatically run a few times as soon as you
    # activate it:
    catchup=True,
    # max_active_runs defines how many running concurrent instances of a DAG there are allowed to be
    # running. In this case it is set to 1. This means that the catchup will be done
    # day by day.
    max_active_runs=1,
) as dag:
    ############################## END OF EXERCISE 1 ##############################

    start_task = EmptyOperator(task_id="start")

    ############################# START OF EXERCISE 2 #############################

    def get_new_users(**context):

        # `context['ds']` will correspond to the
        # logical date of the DAG run (the date when it was scheduled to run).
        # Setting `start_date` to `context['ds']` indicates that data will be
        # extracted starting at the same date as the DAG is scheduled.
        start_date = f"{context['ds']}"
        # `context["data_interval_end"]` is another built-in variable in
        # Airflow's task context, representing the end of the interval for
        # which the DAG is being executed. It is a datetime object.
        # The strftime("%Y-%m-%d") method formats this datetime object
        # to a string in the YYYY-MM-DD format, making it suitable for use
        # in the API URL.
        end_date = context["data_interval_end"].strftime("%Y-%m-%d")

        # Make a call to the API to retrieve information
        # about the new users between the start_date and end_date.

        ### START CODE HERE ### (1 line of code)
        # In CloudFormation, search for the Output value associated with
        # APIEndpoint to fill the `<API_ENDPOINT>` placeholder.
        response = requests.get(
            f"http://<API_ENDPOINT>/users?start_date={start_date}&end_date={end_date}"
        )
        ### END CODE HERE ###

        assert response.status_code == 200, response.reason

        # Define the name of the object to be created with the initial
        # information about new users:
        file_name = f"new_users_{start_date}.json"

        logger.info(f"Saving file: {file_name}")

        ### START CODE HERE ### (5 lines of code)
        # Call the `put_object` method of the boto3 client by passing the
        # Raw DataBucket constant to the Bucket parameter, and the content of the
        # API response to the Body parameter with the code `response.content`
        client.put_object(
            Bucket=RAW_DATA_BUCKET,
            Key=f"new_users/{start_date}/{file_name}",
            Body=response.content,
        )
        ### END CODE HERE ###
        logger.info(f"File: {file_name} saved successfully")

    # Define the task that fetches the initial information about new users
    # and stores it in S3:
    get_new_users_task = PythonOperator(
        task_id="get_new_users",
        ### START CODE HERE ### (1 line of code)
        # Pass `get_new_users` function which you defined above
        python_callable=get_new_users,
        ### END CODE HERE ###
        retries=3,  # retry the task in case of failure
        retry_delay=dt.timedelta(seconds=1),
    )

    ############################## END OF EXERCISE 2 ##############################

    ############################# START OF EXERCISE 3 #############################

    def get_session(**context):

        # Make a call to the API to retrieve information
        # about the sessions between the start_date and end_date.
        start_date = f"{context['ds']}"
        end_date = context["data_interval_end"].strftime("%Y-%m-%d")

        ### START CODE HERE ### (1 line of code)
        # In CloudFormation, search for the Output value associated with
        # APIEndpoint to fill the `<API_ENDPOINT>` placeholder.
        response = requests.get(
            f"http://<API_ENDPOINT>/sessions?start_date={start_date}&end_date={end_date}"
        )
        ### END CODE HERE ###

        assert response.status_code == 200, response.reason

        # Define the name of the object to be created with the initial
        # information about the sessions.
        file_name = f"sessions_{start_date}.json"

        logger.info(f"Saving file: {file_name}")

        ### START CODE HERE ### (5 lines of code)
        # Call the `put_object` method of the boto3 client by passing the
        # Raw DataBucket constant to the Bucket parameter, and the content of the
        # API response to the Body parameter with the code `response.content`
        client.put_object(
            Bucket=RAW_DATA_BUCKET,
            Key=f"sessions/{start_date}/{file_name}",
            Body=response.content,
        )
        ### END CODE HERE ###
        logger.info(f"File: {file_name} saved successfully")

    # Define the task that fetches the initial information about a session
    # and stores it in S3:
    get_session_task = PythonOperator(
        task_id="get_session",
        ### START CODE HERE ### (1 line of code)
        # Pass `get_session` function which you defined above
        python_callable=get_session,
        ### END CODE HERE ###
        retries=3,  # retry the task in case of failure
        retry_delay=dt.timedelta(seconds=1),
    )

    ############################## END OF EXERCISE 3 ##############################

    ############################# START OF EXERCISE 4 #############################

    def get_session_info_dict(date: str):
        """
        Fetches the contents of the session information file for the given date
        as a Python dictionary.
        """
        # Remember that the name of the file is of the form
        # "sessions_<LOGICAL_DATE>.json":
        session_info_file_name = f"sessions_{date}.json"

        logger.info(f"Reading the file: {session_info_file_name}")

        ### START CODE HERE ### (4 lines of code)
        # Get the object corresponding to the file using method `get_object()`.
        # Pass the Raw Data Bucket name to the Bucket parameter
        # and the Key with the complete location of the
        # "session_<LOGICAL_DATE>.json" file in the bucket:                
        session_info_file = client.get_object(
            Bucket=RAW_DATA_BUCKET,
            Key=f"sessions/{date}/{session_info_file_name}",
        )
        ### END CODE HERE ###

        logger.info(f"File read: {session_info_file_name}")

        assert (
            session_info_file is not None
        ), f"The file {RAW_DATA_BUCKET}/sessions/{date}/{session_info_file_name} does not exist"

        ### START CODE HERE ### (1 line of code)
        # Use the `"Body"` key in the response dictionary from the client
        # `get_object` method. Then, use the `read()` method to obtain
        # the context of the initial information file as a string:
        session_info_string = session_info_file["Body"].read()
        ### END CODE HERE ###

        # Parse the contents of the initial information file, which is JSON, as
        # a Python dictionary:
        session_info = json.loads(session_info_string)

        return session_info

    ############################## END OF EXERCISE 4 ##############################

    ############################# START OF EXERCISE 5 #############################

    def get_user_info(**context):

        ### START CODE HERE ### (1 line of code)
        # Read the initial information about the sessions for the
        # corresponding day using the `get_session_info_dict` function.
        # Remember that the DAG's logical date can be obtained from
        # `context["ds"]`:
        session_info = get_session_info_dict(context["ds"])
        ### END CODE HERE ###

        # Initialize the list that will contain the users' information:
        user_info = []

        # Fill the list users with the information retrieved from the API.
        ### START CODE HERE ### (1 line of code)
        # In CloudFormation, search for the Output value associated with
        # APIEndpoint to fill the `<API_ENDPOINT>` placeholder.

        for session in session_info:
            user_id = session["user_id"]
            print(f"Getting data from ")
            response = requests.get(
                f"http://<API_ENDPOINT>/users_by_id?user_ids={user_id}"
            )
            user_info.append(response.json()[0])
        ### END CODE HERE ###

        # Serialize the list of authors' names as a string representing a JSON
        # array:
        user_info_string = json.dumps(user_info)

        # Construct the users' information file name
        users_file_name = f"user_info_{context['ds']}.json"

        logger.info(f"Saving file: {users_file_name}")

        ### START CODE HERE ### (5 lines of code)
        # Call the `put_object` method of the boto3 client by passing the
        # Raw DataBucket name to the Bucket parameter, and
        # the user_info_string to the Body parameter
        client.put_object(
            Bucket=RAW_DATA_BUCKET,
            Key=f"user_info/{context['ds']}/{users_file_name}",
            Body=user_info_string,
        )
        ### END CODE HERE ###

        logger.info(f"File: {users_file_name} saved")

    get_users_info_task = PythonOperator(
        task_id="get_users_info",
        ### START CODE HERE ### (1 line맖f code)
        # Pass the `get_user_info` to the `python_callable` parameter
        python_callable=get_user_info,
        ### END CODE HERE ###
    )

    ############################## END OF EXERCISE 5 ##############################

    ############################# START OF EXERCISE 6 #############################

    def save_complete_session(**context):

        ### START CODE HERE ### (1 line맖f code)
        # Read the initial information about the sessions for the
        # corresponding day using the `get_session_info_dict` function.
        # Remember that the DAG's logical date can be obtained from
        # `context["ds"]`:
        session_info = get_session_info_dict(context["ds"])
        ### END CODE HERE ###

        # Read the information about the users as list of strings:
        users_file_name = f"user_info_{context['ds']}.json"
        users_object = client.get_object(
            Bucket=RAW_DATA_BUCKET,
            Key=f"user_info/{context['ds']}/{users_file_name}",
        )
        assert users_object is not None

        users = json.loads(users_object["Body"].read())

        # Convert the list of users to a dictionary for quick lookup
        user_dict = {user["user_id"]: user for user in users}

        # Merge session data with user data
        merged_data = []
        for session in session_info:
            user_id = session["user_id"]
            user_data = user_dict.get(user_id, {})
            user_data_filtered = {
                key: value
                for key, value in user_data.items()
                if key != "user_id"
            }

            # Create a new merged entry
            merged_entry = {**session, **user_data_filtered}

            merged_data.append(merged_entry)

        # Convert to JSON
        complete_sessions_json = json.dumps(merged_data)

        # Prepare the book record file name
        complete_sessions_file_name = f"complete_sessions_{context['ds']}.json"

        ### START CODE HERE ### (5 lines맖f code)
        # Call the `put_object` method of the boto3 client by passing the
        # Raw DataBucket name to the Bucket parameter, and the
        # `complete_sessions_json` to the Body parameter
        client.put_object(
            Bucket=RAW_DATA_BUCKET,
            Key=f"complete_sessions/{context['ds']}/{complete_sessions_file_name}",
            Body=complete_sessions_json,
        )
        ### END CODE HERE ###

    save_complete_session_task = PythonOperator(
        task_id="save_complete_session",
        ### START CODE HERE ### (1 line맖f code)
        # Pass the `save_complete_session` to the `python_callable` parameter
        python_callable=save_complete_session,
        ### END CODE HERE ###
    )

    ############################## END OF EXERCISE 6 ##############################

    ############################# START OF EXERCISE 7 #############################

    def clean_up_intermediate_info(**context):
        # Delete the initial information file of the logical date by using the
        # `delete_object` method of the boto3 client by passing the bucket
        # name and Key with the path to the object:
        session_info_file_name = f"sessions_{context['ds']}.json"

        client.delete_object(
            Bucket=RAW_DATA_BUCKET,
            Key=f"sessions/{context['ds']}/{session_info_file_name}",
        )

        users_file_name = f"user_info_{context['ds']}.json"

        ### START CODE HERE ### (4 lines맖f code)
        # Delete the users' info file of the logical date by using the
        # `delete_object` method of the boto3 client by passing the bucket
        # name and Key with the path to the object:
        client.delete_object(
            Bucket=RAW_DATA_BUCKET,
            Key=f"user_info/{context['ds']}/{users_file_name}",
        )
        ### END CODE HERE ###

    cleanup_task = PythonOperator(
        task_id="cleanup",
        ### START CODE HERE ### (1 line맖f code)
        # Pass the `clean_up_intermediate_info` to the `python_callable` parameter
        python_callable=clean_up_intermediate_info,
        ### END CODE HERE ###
    )

    ############################## END OF EXERCISE 7 ##############################

    ############################# START OF EXERCISE 8 #############################

    ### START CODE HERE ### (1 line맖f code)
    # Define the `end` task as a dummy operator with the `task_id`
    # equal to `"end"`:
    end_task = EmptyOperator(task_id="end")
    ### END CODE HERE ###

    ############################## END OF EXERCISE 8 ##############################

    ############################# START OF EXERCISE 9 #############################

    ### START CODE HERE ### (~ 4 lines맖f code)
    # Define the task dependencies with the `>>` operator to obtain the desired
    # DAG:
    start_task >> [get_new_users_task, get_session_task]
    get_session_task >> get_users_info_task >> save_complete_session_task
    [get_new_users_task, save_complete_session_task] >> cleanup_task
    cleanup_task >> end_task
    ### END CODE HERE ###

############################## END OF EXERCISE 9 ##############################

