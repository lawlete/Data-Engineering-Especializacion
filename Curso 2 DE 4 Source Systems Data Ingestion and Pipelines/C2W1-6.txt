En el primer curso de la especialización, recopilamos algunos requisitos basándonos en las conversaciones que mantuvimos con varias partes interesadas. Incluyendo a un científico de datos, el director de datos, un gerente de marketing de productos y un ingeniero de software en una empresa de comercio electrónico ficticia. A través de estas conversaciones, el director de datos le dijo que la empresa tiene como objetivo expandirse a nuevos mercados y aumentar la retención de los clientes existentes. Además, trabajó con estas partes interesadas para configurar una canalización de datos para un sistema de recomendación. Aquí nos basaremos en ese conjunto de conversaciones y hablaremos con un analista de marketing al que se le ha encomendado la tarea de buscar información y tendencias de venta de productos. Por eso, en este vídeo, interpretaré a la ingeniera de datos y mi amiga Colleen interpretará a la analista de marketing. Vamos a entrar. Colleen número cuatro, me alegro de conocerte. >> También me alegro de conocerte. >> Hola, soy Joe, soy un nuevo ingeniero de datos y estoy deseando saber más sobre lo que estáis haciendo. >> Sí, por supuesto, estoy muy emocionada de trabajar contigo. Así que estoy trabajando en tratar de entender qué tipo de factores externos podrían ser señales de que podríamos conectar con los hábitos de compra de los clientes , por lo que el equipo de marketing ha estado haciendo una lluvia de ideas sobre qué tipo de cosas podrían ser y se nos han ocurrido algunas ideas que nos gustaría explorar más a fondo. >> Eso suena genial, ¿por qué no me cuentas más sobre eso? >> Claro que sí. Así que pensamos que, en términos generales, la forma en que alguien se siente, feliz o triste, emocionada o relajada, podría afectar su comportamiento cuando se trata de comprar en línea. Por supuesto, no tenemos forma de saber exactamente cómo se sienten nuestros clientes en un día determinado, pero pensamos que podríamos explorar algunas ideas. En particular, nos gustaría ver qué tipo de música escucha la gente en las distintas regiones en las que vendemos nuestros productos y, a continuación, compararlos con las ventas de productos. Ya veo. Así que estás pensando que te gustaría obtener datos públicos de algunas fuentes externas para obtener información sobre la música que escucha la gente. >> Sí, exactamente. Lo he estado investigando y parece que Spotify tiene una API pública en la que podríamos obtener datos sobre los artistas musicales que están de moda en las diferentes regiones y las tendencias de escucha de las personas a lo largo del tiempo. ¿Suena como algo con lo que podrías ayudarnos? >> Seguro que soy un gran fan de Nickelback, así que definitivamente me gusta mucho la música. >> Bonito, debe serlo. >> Sí, sí, así que tendré que echar un vistazo más de cerca a la API de Spotify. Y una vez que averigüe los detalles, tal vez podamos hablar exactamente de qué tipo de información le gustaría obtener y cómo le gustaría que se proporcionara. >> Vale, sí, suena fantástico. Mientras tanto, avísame si hay algo en lo que pueda ayudarte y espero poder hablar más contigo una vez que tengamos los detalles resueltos. Muy bien, eso es genial, gracias. Muy bien, ese fue un ejemplo de una conversación con un analista de marketing en la que describe sus necesidades de datos para un proyecto. Concéntrese en extraer los datos de una API pública que les gustaría analizar junto con los datos de ventas de productos. Ahora, reconozco, y tal vez tú también lo estés pensando, que estudiar las tendencias regionales sobre el tipo de música que escucha la gente puede no parecer un enfoque de marketing particularmente brillante. Y probablemente tengas razón, pero créeme, he visto todo tipo de cosas locas en lo que respecta al tipo de datos que las diferentes partes interesadas quieren tener en sus manos. Por lo tanto, el punto aquí no es insistir en si esto parece un esfuerzo digno, sino identificar los requisitos clave del sistema que necesitará construir. En este caso, no cabe duda de que se necesita más información para saber exactamente cuál será el mejor enfoque a la hora de crear toda la canalización de datos para este proyecto. Pero por el momento, nos vamos a centrar en la parte de la ingestión. Lo más importante que aprenderás aquí es que vas a necesitar ingerir datos de una API de terceros. Con el tiempo, tendrá que considerar otros detalles, como la forma en que, en última instancia, almacenará y entregará los datos al analista, y esto dependerá de lo que necesite el analista. En general, al ingerir datos de una API, te enfrentarás a algún tipo de proceso de ingesta por lotes, pero su aspecto exacto dependerá de lo que pretendas hacer con los datos. En el siguiente vídeo, analizaremos más de cerca las ventajas y desventajas entre los populares paradigmas de procesamiento de datos por lotes de extracción, transformación, carga o ETL y extracción, carga, transformación o ELT en relación con la ingesta de datos. Y después de eso, analizaremos la conexión y la ingesta de datos de una API REST. Nos vemos en el siguiente vídeo.

En el vídeo anterior, nuestros analistas de marketing compartieron sus objetivos para un proyecto en el que están trabajando para incorporar algunos datos externos en su análisis de las ventas de productos. Para este proyecto, parece que los analistas se centrarán principalmente en las tendencias históricas de los datos y es posible que en el futuro quieran pasar a un análisis más explícito de los datos actuales, pero no necesariamente en tiempo real o en un sentido urgente. Además, sabe que extraerá los datos de una API de terceros. Si bien, por lo general, tendrá cierta flexibilidad en cuanto a la frecuencia o la cantidad de datos que extraiga, estará limitado a algún tipo de ingesta por lotes. Esto se debe a que las llamadas a la API funcionan de manera similar a las solicitudes web, en las que envías una solicitud de datos y recibes una respuesta, y la cantidad de solicitudes que puedes realizar por vez suele ser limitada. En términos de ingesta de datos para este proyecto, se trata de un proceso por lotes. En el curso anterior, presenté brevemente ETL y ELT, que son las siglas de extract transform load y extract load transform respectivamente. Se trata de dos patrones de ingesta por lotes muy comunes y, si bien técnicamente incluyen componentes de las etapas de transformación y almacenamiento del ciclo de vida de la ingeniería de datos , en la práctica, tendrá que pensar en las ventajas y desventajas entre estos patrones y la etapa de ingesta. Eso es lo que me gustaría hacer ahora mismo. En primer lugar, hablaré un poco más sobre lo que distingue a estos dos procesos y , a continuación, analizaremos cuál podría ser el más adecuado para el caso de uso de nuestros analistas de marketing. La carga de transformación de extractos o ETL es en realidad el patrón original de ingestión de lotes que ganó popularidad en las décadas de 1980 y 1990. El proceso comienza con la extracción de datos sin procesar de un sistema de tienda, lo que se puede hacer consultando directamente una base de datos o utilizando una API, por ejemplo. A continuación, transforma los datos en un área de preparación intermedia. A continuación, carga los datos en un destino de almacenamiento de destino, como una base de datos o un almacén de datos. En los años 80 y 90, la capacidad de almacenamiento y computación era extremadamente limitada, por lo que era importante tener un plan para determinar exactamente qué datos querías ingerir y cómo querías almacenar esos datos y acceder a ellos, en qué formato, etc. Los almacenes de datos eran costosos de configurar y no eran adecuados para ejecutar consultas pesadas que incluían combinaciones y transformaciones complejas. En aquellos días, no había más remedio que ser muy intencional a la hora de transformar los datos sin procesar durante el proceso de ingestión para garantizar que pudieran almacenarse y ponerse a disposición de manera eficiente. El ETL sigue siendo muy popular hoy en día como patrón de ingestión. Pero ahora, con el costo relativamente bajo del almacenamiento en la nube y el aumento de la potencia computacional, ya no es la única opción. A principios de la década de 2010, los sistemas de almacenamiento en la nube se volvieron altamente escalables y vimos la aparición de lagos de datos basados en sistemas de almacenamiento de objetos como S3 y almacenes de datos en la nube, como Redshift y Snowflake. Esto hizo posible almacenar enormes cantidades de datos de forma relativamente económica y realizar todas las transformaciones de datos directamente en su almacén de datos. Fue entonces cuando surgió el concepto de ELT o transformación de carga de extracción. En el proceso ELT, se extraen los datos sin procesar de los sistemas de origen y se cargan directamente en la base de datos o el almacén de datos de destino o incluso en el almacenamiento óptico sin realizar ninguna transformación. La interesante idea del ELT es que no es necesario que decidas por adelantado cómo quieres usar tus datos. Esto puede resultar atractivo porque, en cierto sentido, se podría decir que al aplicar transformaciones a los datos sin procesar y almacenar solo los resultados del proceso como se hace con ETL, se pierde parte de la información en el proceso. Sin embargo, con ELT, todas las opciones permanecen sobre la mesa, ya que solo tiene que capturar todos los datos y guardarlos para usarlos más adelante, y luego puede consultar y transformar los datos sin procesar de la manera que desee, sin que se pierda ninguna información. Ahora, por muy atractivo que pueda parecer este paradigma, para ser honesto, cuando escuché por primera vez sobre la idea del ELT, pensé que era una idea terrible. ¿Por qué pensé que querrías acumular un montón de datos sin procesar y almacenamiento sin pensar profundamente en cómo quieres usarlos? Como he estado enfatizando a lo largo de estos cursos, el primer paso en cualquier proyecto de ingeniería de datos debe ser establecer con firmeza cuáles son sus objetivos finales y, solo entonces, pensar en cómo construir un sistema para lograr esos objetivos. Sin embargo, con el tiempo, empecé a ver los beneficios potenciales del ELT. Por un lado, el ELT es más rápido de implementar porque no requiere una planificación detallada y anticipada sobre cómo desea transformar exactamente sus datos. También es posible hacer que los datos estén disponibles más rápidamente para los usuarios finales, aunque estén sin procesar, porque el ELT elimina la necesidad de un servidor provisional y de transformaciones de datos en curso. Con la potencia de procesamiento del almacén de datos moderno, las transformaciones aún se pueden realizar de manera eficiente una vez que los datos se cargan en el almacenamiento. Más allá de eso, como dije antes, cuando desee almacenar todos sus datos sin procesar, puede configurarlos más adelante para adoptar diferentes transformaciones o analizar los datos de una manera diferente que podría haber sido posible si solo almacenara los datos de transformación en primer lugar. ¿Cuál es la desventaja del ELT? En resumen, si no tiene cuidado, su canalización puede convertirse simplemente en una canalización EL, en la que puede extraer y cargar enormes cantidades de datos sin procesar en el almacenamiento sin tener que averiguar cómo transformarlos en algo útil. Si no quiere dedicar tiempo por adelantado a planificar cómo va a usar sus datos, podría terminar con lo que comúnmente se conoce como un pantano de datos, que es una situación en la que los datos se vuelven desorganizados, inadministrables y prácticamente inútiles. Me gusta mostrar esta imagen cuando surge el tema de los pantanos de datos. Aquí tenemos a un ingeniero de datos sentado en su pantano de datos, o se ha quedado con absolutamente todo lo que pensaba que podría tener algún valor algún día. Pero ahora, por supuesto, incluso si pudiera recordar todo lo que había allí, probablemente ni siquiera sería capaz de encontrarlo. A principios de la década de 2010, los pantanos de datos eran comunes, ya que las empresas descubrieron que era posible conservar literalmente cada fragmento de datos sin procesar por si acaso. Hoy en día, gran parte de esto se ha solucionado debido en parte a las regulaciones que exigen que las empresas almacenen los datos de tal manera que puedan auditarse o eliminarse de manera ordenada; por ejemplo, un usuario solicita que sus datos se eliminen de los sistemas de la empresa. Dicho esto, el costo relativamente bajo del almacenamiento actual, combinado con la potencia de procesamiento de los almacenes de datos modernos y otras abstracciones de almacenamiento, significa que tanto ETL como ELT pueden ser enfoques razonables para el procesamiento por lotes. Sin embargo, sea cual sea el enfoque que adopte, es importante tener un conjunto y objetivos claros en mente y administrar los datos en consecuencia. Pensemos en la conversación con el analista de marketing. Para este proyecto, ingerirás datos de una API de terceros. La mayoría de las veces, los datos que recibirá a través de una conexión de API serán datos semiestructurados, tal vez en formato JSON. En algunos casos, es posible que también estés recuperando datos no estructurados, como textos e imágenes. En este caso, parece que el analista de marketing pretende realizar un análisis exploratorio de los datos y no podría decir por adelantado exactamente qué transformaciones podrían ser necesarias. Probablemente, un oleoducto ELT sea la elección correcta para este escenario de ingestión, ya que brinda más flexibilidad en las etapas de transformación y navegación de este proyecto. El componente principal de este caso de uso de ingestión del que aún no hemos hablado en detalle es una parte sobre la ingesta de datos de una API. Ahí es hacia donde nos dirigimos ahora. Acompáñeme en el siguiente vídeo para ver cómo trabajará con una API como fuente de datos.

 

	

ETL
	

ELT

Historia
	

- En los años 80 y 90, el coste de los almacenes de datos era muy elevado (millones de dólares), por lo que los ingenieros querían ser muy cuidadosos con los datos que iban a cargar en el almacén de datos

- El volumen de datos aún era manejable.
	

- El almacén de datos en la nube redujo significativamente el coste de almacenamiento y procesamiento de datos (de millones de dólares a sólo cientos/miles de dólares)

- Volumen de datos disparado.

Procesamiento (transformación)
	

- Los datos se transforman en un formato predeterminado antes de cargarse en un repositorio de datos. Así, los ingenieros de datos tienen que modelar cuidadosamente los datos y transformarlos a este formato.

- Las transformaciones dependen de la capacidad de procesamiento de la herramienta de procesamiento que se utiliza para ingerir los datos (sin relación con el destino de destino)
	

- Los datos en bruto se cargan en el destino final. A continuación, se transforman justo antes del análisis (pueden utilizarse con solicitudes de datos no bien definidas)

- Las transformaciones dependen de la capacidad de procesamiento del repositorio de datos, como el almacén de datos.

Tiempo de mantenimiento 
	

Si la transformación resulta inadecuada, es necesario volver a cargar los datos.
	

Los datos originales están intactos y ya cargados y pueden utilizarse cuando sea necesario para una transformación adicional: Se requiere menos tiempo para el mantenimiento de los datos.

Tiempo de carga y tiempo de transformación
	

Tiempo de carga: suele llevar más tiempo, ya que utiliza un área y un sistema de puesta en escena.

Tiempo de transformación: depende del tamaño de los datos, de la complejidad de la transformación y de la herramienta que se utilice para realizarla.

	

Tiempo de carga: no hay transformación, los datos se cargan directamente en el sistema de destino

Tiempo de transformación: suele ser más rápido porque se basa en la potencia de procesamiento y la paralelización de los almacenes de datos modernos

(generalmente se considera más eficiente)

Flexibilidad (tipos de datos)
	

Los ETL suelen estar diseñados para manejar datos estructurados.
	

Los ETL pueden manejar todo tipo de datos: estructurados, no estructurados, semiestructurados. Una vez cargados los datos en el sistema de destino, puede transformarlos.

Coste
	

Depende de la herramienta ETL/ELT que se utilice y del sistema de destino al que se carguen los datos. (Y, por supuesto, depende del volumen de datos).
	

Depende de la herramienta ETL/ELT que se utilice y del sistema de destino al que se carguen los datos. (Y, por supuesto, depende del volumen de datos).

Escalabilidad
	

Hoy en día, la mayoría de las herramientas en la nube son escalables. Sin embargo, el reto aquí es que si tiene muchas fuentes de datos y muchos objetivos, tendrá que hacer un gran esfuerzo para gestionar el código y manejar los datos de múltiples fuentes
	

ELT utiliza la potencia de procesamiento escalable del almacén de datos para permitir la transformación a gran escala.

Calidad/seguridad de los datos
	

Garantiza la calidad de los datos limpiándolos previamente. Las transformaciones también pueden incluir el enmascaramiento de información personal.
	

Los datos deben transferirse primero al sistema de destino antes de aplicar transformaciones que mejoren la calidad o la seguridad de los datos.

*Existe un subpatrón denominado EtLT, en el que t pequeño no se refiere al modelado de negocio, sino a la transformación con alcance limitado (enmascarar datos sensibles, deduplicar filas).

En el curso anterior, mencioné el llamado mandato de API que llegó en forma de correo electrónico de Jeff Bezos a todos los empleados de Amazon en 2002. La esencia de este correo electrónico era que, de ahora en adelante, todos los equipos deberán utilizar interfaces de servicio, también conocidas como interfaces de programación de aplicaciones o API, para comunicarse, así como para ofrecer datos y funciones. El problema que pretendía resolver era que, antes de esa fecha, los equipos de Amazon y de cualquier otra organización no disponían de una forma uniforme o estable de intercambiar datos y servicios, lo que generaba ineficiencias. Al configurar las API como una interfaz de servicio estable y predecible entre los diferentes equipos, cualquier equipo individual podía proporcionar datos , funciones y comunicaciones a otros equipos, sin importar el tipo de lío complicado que pudieran tener los equipos en sus propios sistemas. La otra parte del mandato de la API consistía en que todas estas interfaces de servicio o API tenían que crearse desde cero para que, finalmente, fueran públicas para los desarrolladores del mundo exterior. Esta reorientación hacia las interfaces de servicio sentó las bases de lo que eventualmente se convertiría en Amazon Web Services y marcó la dirección de cómo las empresas de todo el mundo compartirían datos y servicios, tanto interna como externamente. Básicamente, una API es un conjunto de reglas y especificaciones que le permite comunicarse e intercambiar datos mediante programación con una aplicación. Por comunicarse mediante programación, me refiero a comunicarse mediante la ejecución de código. Si ha desarrollado software, es posible que esté familiarizado con la conexión a las API. Pero aunque no hayas configurado tú mismo las conexiones de API, no cabe duda de que estás utilizando las API directamente a diario cuando buscas cosas en línea o utilizas las aplicaciones de tu teléfono. Esto se debe a que, en la actualidad, las API están integradas en la funcionalidad de una amplia gama de aplicaciones de software. Por ejemplo, las aplicaciones de redes sociales utilizan API para obtener y mostrar datos de los servidores web a los usuarios finales. Las API también se utilizan para facilitar las transacciones entre sitios web de comercio electrónico y sistemas de pago. Muchas empresas ofrecen API públicas para que usted, como desarrollador, pueda acceder a sus datos y servicios e integrarlos en sus propias aplicaciones. Como ingeniero de datos, utilizará las API para conectarse y extraer datos de varias fuentes, como servicios web, plataformas en la nube o proveedores externos, enviando solicitudes y recibiendo respuestas en un formato estandarizado. Las API también pueden proporcionar funciones de metadatos, documentación, autenticación y gestión de errores para facilitar la extracción de datos. El tipo de API más común es lo que se conoce como API REST, o REST son las siglas de Representational State Transfer. Las API REST suelen utilizar el Protocolo de transferencia de hipertexto o lo que quizás conozcas más habitualmente como métodos HTTP como base para la comunicación. Puede pensar que la interacción con las API REST es similar a lo que hace cuando navega por Internet. Al hacer clic en un enlace de su navegador, envía una solicitud HTTP a un servidor para un recurso específico, como una página web, y el servidor responde proporcionando ese recurso. Con una API REST, también envías una solicitud HTTP para un recurso en particular y la API está configurada para responder en función del contenido de tu solicitud. En la conversación, tenemos a los analistas de marketing. Nos enteramos de que les gustaría analizar los datos que están almacenados en una plataforma de terceros, Spotify en este caso, y que están disponibles a través de una API. Este es un escenario muy común con el que se encontrará como ingeniero de datos, en el que se puede acceder a través de una API al sistema de origen del que necesita extraer datos, ya sea un sistema interno o un sistema externo de terceros. Pero la mejor manera de familiarizarse con el funcionamiento de esto es empezar y hacerlo usted mismo, y eso es lo que hará en el próximo laboratorio.

En el próximo laboratorio, practicarás cómo interactuar con la API de Spotify para extraer datos, explorar qué significa la paginación y aprender a enviar una solicitud de API que requiere autorización. En este vídeo, analizaré primero algunos conceptos de API con los que trabajarás en el laboratorio y, a continuación, te proporcionaré una descripción general de las tareas del laboratorio. Para los ejercicios de laboratorio, necesitarás tener una cuenta de Spotify para obtener las credenciales que necesitas para extraer datos de la API. Esto se debe a que cualquier solicitud que envíes a la API de Spotify requiere autorización y, para obtener esa autorización, necesitas una cuenta. Ahora, que quede claro, no quiero que pienses que estoy promocionando Spotify aquí o que te estoy pidiendo que abras una cuenta sin ningún motivo. De hecho, cuando trabajas con API de terceros como ingeniero de datos, es muy común que tengas que registrar una cuenta en esa plataforma de terceros para poder usar la API. Por lo tanto, el propósito de este laboratorio es que sigas ese flujo de trabajo de la misma manera que lo experimentarías en el trabajo. Y, por supuesto, si lo deseas, puedes cancelar tu cuenta de Spotify inmediatamente después de completar el laboratorio. Pero por ahora, si aún no tienes una, te animo a que crees una cuenta de Spotify y consultes la documentación de la API de Spotify para que puedas seguir lo que te voy a mostrar aquí. La API web de Spotify es una API RESTful a la que puedes enviar solicitudes para acceder a artistas musicales, álbumes y pistas directamente desde el catálogo de datos de Spotify. Cada elemento de datos específico, como una lista de reproducción, un artista o un álbum, se denomina recurso al que puede acceder enviando una solicitud HTTP al punto final que representa ese recurso. Hay diferentes tipos de solicitudes HTTP, pero las más comunes son GET, PUT, POST y DELETE. GET le permite recuperar un recurso, POST le permite crear un recurso, PUT le permite cambiar o reemplazar recursos y DELETE le permite eliminar recursos. Por ejemplo, en la documentación de la API de Spotify, si haces clic en Álbumes, puedes ver todas las solicitudes que puedes usar para interactuar con este recurso. Por ejemplo, puedes realizar una solicitud GET para obtener información del catálogo de Spotify sobre las canciones de un álbum usando este punto final. También puedes usar una solicitud GET para recuperar una lista de los lanzamientos de nuevos álbumes que aparecen en Spotify usando este punto final. Si la solicitud se realiza correctamente, devuelve una respuesta en formato JSON que contiene información sobre el recurso solicitado. Por ejemplo, este es un ejemplo de respuesta a una solicitud GET de canciones de un álbum y otro ejemplo de respuesta a una solicitud GET de nuevos lanzamientos. Si la solicitud no se realiza correctamente, devuelve un objeto de error que contiene un código de estado que explica por qué la solicitud no se ha realizado correctamente. Por ejemplo, un código de 400 significa una solicitud incorrecta que podría deberse a una sintaxis mal formada, y un código de 404 significa que no se pudo encontrar el recurso solicitado. Puede encontrar en la documentación aquí una lista de los códigos de estado y el significado de cada uno. Cuando realizas una solicitud a la API web de Spotify, debes especificar el punto final del recurso, así como un token de acceso. El token de acceso es una cadena que contiene las credenciales y los permisos que se utilizan para acceder a un recurso determinado. Para obtener el token de acceso, primero tendrás que crear una cuenta de Spotify y, desde tu cuenta, podrás obtener un ID de cliente y un secreto de cliente que podrás usar en el proceso de autorización y la generación del token de acceso. El token de acceso es válido durante una hora. Transcurrido ese tiempo, el token caduca y tendrás que solicitar uno nuevo. En el laboratorio, se te proporcionará un código que puedes usar para solicitar un token de acceso con tu ID de cliente y el secreto del cliente. Para obtener más información sobre el proceso de autorización con Spotify, puedes consultar la documentación. Ten en cuenta que es posible que trabajes con otras API que no requieran autorización o que el proceso de autorización sea diferente al de Spotify, por lo que te sugiero que consultes siempre la documentación de la API con la que vas a trabajar. Antes de entrar en más detalles sobre el laboratorio, no dudes en pausar el vídeo para consultar la documentación de Spotify, así como los dos ejemplos de solicitud de API, Get Album Tracks y Get New Releases. Antes de empezar a interactuar con la API de Spotify, debes crear una cuenta para obtener las claves que utilizarás para generar los tokens de acceso. Así que primero asegúrate de registrarte y completar la información requerida para crear tu cuenta. Así que aquí ya he iniciado sesión en mi cuenta. Haré clic en el nombre de la cuenta en la esquina superior derecha y, a continuación, en el panel de control y, a continuación, en Crear aplicación. En Nombre de la aplicación, escribiré este nombre. En Descripción de la aplicación, escribiré Spotify App para probar la API. Para los URI de redireccionamiento, especificaré este host local. Y aquí voy a elegir Web API. Por último, haré clic en Guardar. Si recibes el error de que tu cuenta no está lista, puedes cerrar sesión y esperar unos minutos y, a continuación, volver a iniciar sesión y repetir los pasos. Una vez creada la aplicación, puede hacer clic en ella para ir a la página de inicio de la aplicación. Aquí haré clic en la configuración para encontrar el ID de cliente y el secreto del cliente. Tendrá que copiar estos valores para usarlos en el laboratorio. Una vez que inicie el laboratorio y siga las instrucciones de configuración del laboratorio, estará en este Jupyter Notebook. Aquí, en la carpeta SRC o Source, haré clic en el archivo env y, a continuación, pegaré el ID de cliente y el secreto del cliente para almacenarlos en estas variables de entorno. Volviendo al Jupyter Notebook, primero ejecutaré la celda para importar los paquetes necesarios, luego esta celda para cargar las variables de entorno y, a continuación, estas dos variables para representar el ID de cliente y el secreto del cliente. Junto a obtener un token de acceso, puedes encontrar en esta celda la función getToken, que espera que tu ID de cliente y tu secreto de cliente generen el token de acceso. Utilicé la documentación de la API para saber qué detalles deben incluir esta función. No dudes en consultar este enlace para obtener más información sobre estos detalles. Ahora llamaré a la función getToken, que devuelve esta respuesta de token. Es un diccionario que contiene tres claves: accessToken, el tipo de token y ExpiresIn. Usarás la cadena, que es el valor de la clave accessToken, en todas las llamadas a la API de este laboratorio. Así que echemos un vistazo a algunos de los ejemplos de llamadas a la API. Para crear tus llamadas a la API en Python, puedes usar el paquete requests, que ya he importado a este cuaderno. Este paquete es una biblioteca popular que proporciona una forma sencilla y fácil de usar de interactuar con las API. Por ejemplo, para realizar una solicitud GET en la API de Spotify, puedes llamar al método requests.get. Deberás introducir el punto final del recurso al que quieres acceder y especificar el accessToken mediante los encabezados de los parámetros asignándolo a un diccionario en este formato. En el laboratorio, se le proporciona esta función que crea automáticamente el encabezado de autorización dado el accessToken. Así que aquí, en esta solicitud GET, usaré la función getAuthHeader proporcionada y la asignaré al parámetro headers. Para esta función, usaré la respuesta del token para pasar el accessToken. Tenga en cuenta que el segundo GET no es la solicitud HTTP GET. Es el método que se aplica en un diccionario de Python para extraer el valor correspondiente a una clave. Obtengamos ahora el punto final de getNewReleases y veamos la respuesta devuelta. La respuesta es un objeto de respuesta de Spotify que puedes convertir en un diccionario de Python usando el método.json, tal y como se muestra aquí. Esta es la respuesta. Puedes ver que hay una tecla, Álbumes. Y para los álbumes, hay otro conjunto de pares de valores clave. Para verificar las claves de cada uno, llame al método .keys. Así que la respuesta en realidad contiene una clave, Álbumes. Vamos a comprobar la clave de Álbumes. Estas son las claves de los álbumes. Repasemos cada una de ellas. href contiene un enlace o el punto final al recurso. Puede ver que se agregaron dos parámetros al punto final, getNewReleases, que son offset y limit. La solicitud getNewReleases espera estos dos parámetros. Por lo tanto, de forma predeterminada, se agregaron un desplazamiento de 0 y un límite de 20 al punto final, lo que significa que la respuesta contiene la información de los primeros 20 álbumes. Puedes encontrar los detalles de los álbumes usando la tecla Items. También puede comprobar los valores utilizados para el desfase y el límite y obtener el número total de elementos, que es 100. Por último, next contiene el mismo punto final pero con valores diferentes para el desfase y el límite. En este caso, se refiere a los 20 elementos siguientes. Por lo tanto, si desea especificar su propio desfase y límite, puede hacerlo mediante este punto final. Aquí elegí 40 para offset40 y 20 para límite. En el laboratorio, se te asigna esta función que realiza los getRequests para los lanzamientos de los nuevos álbumes. Toma el offset y el límite como parámetros y los usa para construir el punto final de la getRequest. O puede usar el punto final completo especificado en el último parámetro cuando no esté vacío. Su tarea consistirá en completar esta función. A continuación, tendrás que realizar la paginación para extraer la lista completa de lanzamientos de nuevos álbumes. En lugar de extraer los 100 elementos de una llamada a GetRequest, puedes realizar la paginación para extraer los elementos fragmento por fragmento. Por lo tanto, puede comenzar con la primera llamada a la API, comenzando con una compensación de 0 y eligiendo el límite que desee. Y luego puedes seguir repitiendo la misma llamada, especificando una compensación diferente cada vez para seguir leyendo desde donde la dejaste en la llamada a la API anterior. O puedes usar el punto final proporcionado en el siguiente campo de la respuesta devuelta. Por lo tanto, en el laboratorio, se le proporcionan estas dos funciones. El primero realiza la paginación cambiando manualmente el desplazamiento para cada nueva llamada a la API. Y el segundo usa el siguiente campo proporcionado por la respuesta de la llamada a la API actual. Así que tendrás la tarea de completar estas dos funciones. En la segunda parte del laboratorio, se le proporcionan estas funciones de Python que deberá completar. Estas funciones te permitirán crear un proceso de ingesta por lotes que extraiga la información del catálogo de Spotify para cada uno de los nuevos lanzamientos de álbumes. Por lo tanto, en esta parte, tendrás que realizar dos llamadas a la API paginadas. En la primera, utilizarás la lista de lanzamientos de nuevos álbumes con la misma llamada paginada que utilizaste en la primera parte del laboratorio. En la segunda, obtendrás la información del catálogo de un álbum determinado mediante el punto final getAlbumTracks. El archivo Python de autenticación contiene el script de la función getToken que devuelve un token de acceso. El archivo de punto final contiene dos funciones. La primera corresponde a la llamada paginada al punto final, getNewAlbums. Y la segunda corresponde a la llamada paginada al punto final, getAlbumTracks. Deberá completar estas funciones. También tendrás que completar un fragmento de código que genere automáticamente un nuevo token cuando caduque. Y, por último, en la función principal, llamarás a la primera llamada a la API paginada para obtener los ID de los lanzamientos de los nuevos álbumes. Luego, para cada ID de álbum, llamarás a la segunda llamada a la API paginada para extraer la información de la pista de cada ID de álbum. Deberás completar parte de este código. Por lo tanto, asegúrese de leer las instrucciones detenidamente y de echar un vistazo a estas funciones para comprender cómo funcionan juntas. El laboratorio también contiene algunas partes opcionales que puede consultar para obtener más información sobre las llamadas a la API. Una vez más, antes de empezar el laboratorio, te animo a que consultes la documentación de la API de Spotify y crees tu cuenta. Cuando termines el laboratorio, acompáñame en la siguiente lección para explorar los patrones de transmisión y gestión.

