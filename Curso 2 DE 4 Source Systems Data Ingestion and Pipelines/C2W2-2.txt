Ahora es el momento de analizar la ingesta de streaming con más detalle. En el curso anterior, trabajaste con un sistema de recomendación de productos en el último laboratorio, pero en realidad no explicamos en detalle cómo se configuró la ingesta de streaming para ese sistema. Por eso, en este vídeo tendremos otra conversación con las partes interesadas en la que hablaré con un ingeniero de software sobre los detalles de ingesta de ese sistema de recomendación. Después de eso, construirá el sistema usted mismo en el siguiente laboratorio. Así que pasemos a la conversación de seguimiento que tendremos con el ingeniero de software. Me alegro de volver a verte, Colleen. >> También me alegro de verte. >> Gracias, estoy trabajando para configurar un nuevo sistema de recomendación de productos y me gustaría trabajar con usted para comprender mejor cómo puede funcionar la ingesta de datos en términos de recibir datos de actividad de los usuarios en tiempo real desde el sitio web. >> Por supuesto, la forma en que funciona el sistema en el sitio web es que registramos continuamente los eventos en el registro del servidor web. Y esos eventos incluyen todo, desde las métricas internas de rendimiento del sistema y cualquier error u otra anomalía que se genere, así como la actividad de los usuarios. Al igual que los botones o enlaces en los que hacen clic nuestros usuarios mientras navegan por diferentes productos o realizan compras. >> De acuerdo, bueno, en un escenario ideal, probablemente me gustaría ingerir todos los datos de actividad de los usuarios y ninguna de las métricas internas del sistema. ¿Crees que sería posible separar los registros de eventos relacionados con la actividad de los usuarios y guardarlos en un registro separado para que yo pueda ajustarlos? >> Sí, sin duda podemos hacerlo. Me imagino varias formas en las que podría funcionar. Pero si incluimos los mensajes sobre la actividad de los usuarios en un tema de Kafka o en una transmisión de Kinesis, podrías incorporarlos directamente desde allí a tu canalización. >> Vale, genial. Sí, parece una buena solución. Creo que si pudieras introducir una transmisión de datos de Kinesis, me vendría muy bien. He estado estudiando cómo podrían funcionar otros aspectos de la canalización con la kinesis, por lo que me parece una buena opción por ahora. La otra pregunta que tengo es sobre la carga útil de datos en sí y la velocidad de mensajes esperada. ¿ Podrías decirme más sobre lo que puedo esperar en términos del formato de los mensajes individuales y la velocidad de los mensajes que llegan a la transmisión? >> Sí, por lo que los mensajes se graban en formato JSON. Por lo tanto, la carga útil que puede esperar es un JSON que incluye un identificador de sesión y toda la información del cliente, como la ubicación, así como su actividad de navegación en términos de los productos que han visto o agregado a su carrito con respecto al tamaño de los mensajes individuales, varían un poco, pero generalmente rondan unos cientos de bytes cada uno. La tasa de mensajes que puede esperar variará mucho según el número de usuarios que haya en la plataforma en un momento dado. Pero como puedes imaginar, un usuario puede generar varios eventos por minuto y, entonces, es posible que tengamos unos 10 000 usuarios en la plataforma en las horas punta. Así que eso podría traducirse en hasta 1000 eventos por segundo. >> De acuerdo, genial, veamos, tal vez al final de una estimación general, entonces podríamos tener, digamos, 1000 eventos por segundo con un tamaño de unos cientos de bytes cada uno. Eso es menos de un megabyte por segundo, por lo que debería estar dentro de la capacidad de una transmisión de datos de Kinesis. Creo que pueden gestionar cientos de megabytes por segundo, según la configuración. >> Muy bien, la otra cosa que tendré que configurar por mi parte es cuánto tiempo se conservan los mensajes en la transmisión. Como sabes, la transmisión estará básicamente en un registro de solo pines, pero lo configuraremos para que los mensajes se eliminen después de un período de tiempo. >> Bien, por supuesto, la idea es que usemos los datos en tiempo real para hacer recomendaciones y también guardemos las entradas y salidas del modelo de recomendación para analizarlas más adelante. Por lo tanto, si todo va bien, no necesitaremos volver a leer los mensajes de la transmisión. Pero supongo que si algo sale mal, es posible que queramos tener la posibilidad de hacer una copia de seguridad y reproducir la transmisión. Si algo se rompe. ¿Quizás podríamos retener los mensajes en la transmisión durante un día después de que se hayan escrito inicialmente? >> Claro, veamos. En un día ajetreado, es posible que escribamos, como dijiste, alrededor de un megabyte por segundo en la transmisión. Y hay una especie de orden de magnitud, alrededor de 100 000 segundos en un día. Entonces, el tamaño total del arroyo podría crecer hasta ser, ¿cuál sería? 100 gigabytes, en el peor de los casos. Así que parece razonable. Vale, bueno, ¿hay algo más que quieras saber o deberíamos seguir adelante y empezar a construir esto? >> Creo que eso es todo por ahora. Vamos a construirlo. >> Constrúyelo. Bien, ese fue un ejemplo de una conversación con un ingeniero de software que será una parte interesada principal y el propietario del sistema fuente del que tendrás que ingerir datos. Como he recalcado en varios puntos a lo largo de estos cursos hasta ahora, hay otras cosas que debería analizar con los propietarios de los sistemas de origen a la hora de comprender las posibles interrupciones en su canalización de datos. Cosas como cambios de esquema o interrupciones. Pero por ahora, nos centraremos en entender los datos en sí y el mecanismo de ingestión que utilizarás en este caso. Acompáñame en el siguiente vídeo, en el que analizaremos algunos de los detalles de esta conversación y analizaremos más de cerca la ingesta de streaming.

En la primera semana de este curso, analizamos los sistemas de transmisión, incluidas las colas de mensajes y las plataformas de transmisión de eventos, desde la perspectiva de que se trata de sistemas fuente, en los que la canalización de datos estaba en el extremo consumidor de estas fuentes de datos. También mencioné que, según el sistema con el que trabajes, la fuente real podría ser solo el productor del evento o varios elementos de un sistema de streaming, como varios productores, corredores y consumidores, podrían estar antes de tu sistema de ingesta. En la conversación que acabamos de mantener con el ingeniero de software, el plan que se nos ocurrió era que el ingeniero configurara una transmisión de datos de Kinesis y usted consumirá los mensajes de esa transmisión. En este vídeo, me gustaría hablar un poco más sobre los detalles de los flujos de mensajes y, después, irás al laboratorio. Como recordatorio rápido, las plataformas de transmisión de eventos y las colas de mensajes son las dos modalidades principales de ingesta de streaming. Una cola de mensajes es esencialmente un búfer que se utiliza para entregar mensajes de un productor de eventos a un consumidor de forma asincrónica. Las colas de mensajes suelen funcionar primero en entrar, primero en salir o Fifo, lo que significa que el consumidor de eventos siempre leerá primero el mensaje más antiguo de la cola y, una vez consumido, se eliminará de la cola. Las plataformas de transmisión de eventos, por otro lado, funcionan almacenando los mensajes de forma persistente en un registro que solo se puede adjuntar. El enrutador de eventos distribuye los mensajes del registro a los suscriptores y es posible reproducir o volver a procesar cualquier mensaje del registro. Utilizará el servicio Kinesis de Amazon como plataforma de transmisión de eventos en el laboratorio, pero otra plataforma muy utilizada es Apache Kafka. En este vídeo, utilizaré Kafka como plataforma de ejemplo para explicar algunos detalles y establecer paralelismos con Kinesis, donde existan. Y luego, en el siguiente vídeo, Morgan analizará la kinesis con más detalle. Por lo tanto, al final de esta semana, puede tener un poco de contexto sobre estas dos soluciones. Por lo tanto, Apache Kafka es una plataforma de transmisión de eventos de código abierto y, si bien las plataformas de transmisión vienen en diferentes sabores y variedades, los principios de cómo se enrutan y almacenan los eventos son similares en todas las plataformas. En un nivel alto, los productores de eventos envían o envían mensajes a través de la red a un clúster de Kafka, que contiene uno o más servidores, también llamados corredores. Luego, los consumidores del evento leen o extraen mensajes de ese grupo de Kafka. Vamos a centrarnos un poco en el cúmulo de Kafka que tengo aquí. Dentro de un clúster de Kafka, los flujos de mensajes se dividen y se dirigen en lo que se denominan temas. Puede pensar en un tema como una categoría que contiene una colección de eventos relacionados, o tal vez en otro sentido, como un camino hacia algún lugar. Por lo tanto, los mensajes se alinean en temas similares a cómo se alinean las filas de automóviles en diferentes autopistas según su destino. Un tema puede contener cualquier tipo de mensaje, por ejemplo, alertas de fraude , pedidos de clientes o lecturas de temperatura de dispositivos de IoT. Y es trabajo del productor enviar un mensaje a su tema correspondiente. Cada tema tiene una o más particiones, que son solo registros que contienen secuencias ordenadas e inmutables de mensajes a las que se agregan nuevos mensajes continuamente. Utilizando la analogía del sistema de carreteras, los tabiques son como los carriles de la carretera. Más carriles permiten el paso de más coches, por lo que cada partición gestiona un subconjunto de mensajes a medida que se añaden al tema. Esto permite un tráfico o un flujo de mensajes más eficientes. Una vez más, es tarea del productor decidir a qué partición enviar cada mensaje. La decisión podría basarse en una estrategia por turnos, por ejemplo, o calculando la partición de destino en función de la clave del mensaje. Con la kinesis, todos estos conceptos son esencialmente los mismos, pero en lugar de temas, tienes transmisiones, y en lugar de particiones, tienes lo que se llama fragmentos. Ahora, por otro lado, los consumidores están agrupados y cada grupo de consumidores puede suscribirse a uno o más temas. Los consumidores de un grupo cooperan para consumir los mensajes de todas las particiones de un tema determinado. Cada partición solo se puede asignar a un único consumidor del grupo y cada consumidor consume mensajes de un subconjunto diferente de particiones. Cuando un productor publica un mensaje en una partición de temas, ese mensaje se entrega a un consumidor de cada grupo de consumidores suscriptor. Una vez que se publica un mensaje en un tema, el clúster de Kafka conserva esa información durante un período de tiempo configurable, independientemente de que el mensaje se haya consumido o no. Esto permite a los consumidores reproducir y volver a procesar los mensajes según sea necesario. En nuestra conversación con el ingeniero de software, nos enteramos de que las acciones de los usuarios en el sitio web se registran como mensajes en el registro del servidor web y esos mensajes se envían a un flujo de datos de Kinesis. Del mismo modo, esos mensajes podrían dirigirse a un tema de Kafka, y podrías consumirlos desde allí suscribiéndote a ese tema en un conjunto diferente de circunstancias. Puede imaginarse un escenario en el que tenga acceso directo para monitorear el registro del servidor web en busca de nuevos mensajes. Entonces, podría tratar el registro del servidor web como el productor de eventos. A partir de ahí, los eventos podrían transferirse a un tema de Kafka o a una transmisión de Kinesis como primer paso del proceso de incorporación. También puede monitorear la actividad de la base de datos mediante un proceso conocido como captura de datos de cambio continuo o CDC continuo. Al procesar el registro de la base de datos, puede transmitir los cambios de datos a su canalización de datos para asegurarse de que los datos de su canalización estén sincronizados con las actualizaciones de datos de la base de datos de origen. A continuación, Morgan le explicará los detalles de las transmisiones de datos de Amazon Kinesis, que utilizaremos como herramienta de ingesta de transmisiones en el próximo laboratorio. Y después de eso, volveré para darte un rápido recorrido por el último laboratorio antes de que te lances a crear tu propia solución de ingesta de streaming.

Acabas de aprender un poco más sobre cómo funciona Apache Kafka. Ahora quiero ayudarlo a comprender cómo funciona Amazon Kinesis Data Streams antes de que vaya al laboratorio. Como ya sabes, al igual que Kafka, hay productores de eventos que envían datos a la transmisión y consumidores que leen los datos de la transmisión. En el próximo laboratorio, trabajará con un Kinesis Data Stream como sistema fuente ascendente. No serás responsable de configurar la transmisión en sí. Sin embargo, también ha visto que también puede incorporar productores , consumidores y transmisiones en otras partes de sus sistemas de datos. Aquí me gustaría explicarte algunos de los detalles de Kinesis que son relevantes cuando tienes el control de todo el sistema, que incluye al productor, el consumidor y la transmisión en este caso. Al igual que en Kafka, los productores envían datos a los temas. Cuando trabaja con Kinesis Data Streams, un productor envía los datos a una transmisión específica. Un flujo se compone de muchos fragmentos, que proporcionan las unidades de capacidad del flujo. Como necesitas escalar tu transmisión para ingerir más datos, debes agregar más fragmentos a la transmisión. Para saber cuántos fragmentos necesitará para su caso de uso o cuándo necesitará aumentar el número de fragmentos, necesitará saber el tamaño y la velocidad de las operaciones de escritura y lectura que espera realizar en su proceso. Las operaciones de escritura se producen cuando un productor de eventos escribe datos en la transmisión, y las operaciones de lectura se producen cuando los consumidores intermedios leen los datos de la transmisión. En cuanto a la capacidad, cada acelga puede soportar hasta cinco operaciones de lectura por segundo, y esas cinco operaciones pueden sumar una velocidad total máxima de lectura de datos de dos megabytes/segundo. Para escribir datos, un productor puede escribir hasta 1000 registros por segundo en un disco duro con una velocidad total máxima de escritura de datos de un megabyte/segundo. Para determinar la cantidad de fragmentos que necesitaría para un caso de uso específico, se necesitarían algunos análisis y algunos cálculos matemáticos, dado el tamaño y la velocidad de las operaciones de lectura y escritura que espera. A veces puede resultar difícil estimar el número exacto de operaciones de lectura y escritura. Por ejemplo, en una aplicación nueva. O en otros casos, lo único que puede saber con certeza es que espera que el tráfico de su aplicación varíe drásticamente con el tiempo, como en una plataforma de comercio electrónico u otras aplicaciones públicas. Para esas situaciones, puede usar Kinesis en modo bajo demanda. El modo bajo demanda gestionará automáticamente el escalado de los fragmentos hacia arriba o hacia abajo según sea necesario, y solo se le cobrará por lo que utilice. Esto puede resultar más conveniente desde una perspectiva operativa en comparación con la alternativa, que es el modo aprovisionado. Con el modo de aprovisionamiento, especificó la cantidad de fragmentos necesarios para la aplicación en función de la velocidad esperada de solicitudes de escritura y lectura. Luego, depende de usted agregar más fragmentos o volver a agregarlos cuando sea necesario. El modo de aprovisionamiento puede ser una buena opción para su trabajo si tiene un tráfico de aplicaciones predecible o si desea poder controlar los costos con más cuidado. Cuando se trata de los datos que se mueven a través de una transmisión, cada registro de datos que un productor envía a la transmisión incluye una clave de partición, un número de secuencia y los datos en sí mismos en forma de lo que se denomina un objeto binario grande o blob para abreviar. Al configurar el generador de datos para su sistema, debe elegir una clave de partición. La clave de partición se usa luego para determinar en qué fragmento se coloca el registro de datos. Luego, la propia Kinesis asigna un número de secuencia a medida que se escribe cada registro para mantener el orden de los registros dentro del fragmento. Por ejemplo, supongamos que quieres crear un flujo de transacciones desde una plataforma de comercio electrónico. A continuación, es posible que desee utilizar el ID de cliente como clave de partición. En este caso, todas las transacciones de un solo cliente podrían almacenarse en el mismo fragmento. Esto facilitaría a los consumidores intermedios agrupar los registros relacionados con un solo cliente para su agregación y análisis. Un productor coloca los datos en fragmentos y un consumidor lee los datos de los fragmentos, y es común que varios consumidores lean los datos de un fragmento. De forma predeterminada, los consumidores comparten la capacidad de lectura de Shards, que se denomina salida de ventilador compartida. Esto significa que los consumidores compiten por la capacidad de lectura. Esto puede ser un problema en algunos casos de uso. Para evitar este problema de capacidad, puedes configurar las cosas de manera que cada consumidor pueda leer a la capacidad total de lectura de dos megabytes/segundo del Shard, lo que se denomina ventilación mejorada. Puede usar servicios gestionados como AWS Lambda, Amazon Managed Service for Apache Flink y ADS Glue para procesar los datos almacenados en las transmisiones de datos de Kinesis, o puede crear sus propios consumidores personalizados mediante la biblioteca de clientes de Amazon Kinesis o KCL. También puede configurar las cosas para que la salida de una transmisión se convierta en la entrada de otra, lo que puede permitirle crear flujos de trabajo de procesamiento de datos en tiempo real más complejos. Los consumidores también pueden enviar datos a otros servicios de ADS, como la integración con Amazon Data Firehose para almacenar datos en Amazon S3. También es importante recordar que Kinesis Data Streams permite que varias aplicaciones funcionen con la misma transmisión al mismo tiempo. Cada uno consume los datos de forma independiente y los envía de forma descendente a diferentes sistemas. A continuación, Joe le explicará los detalles del próximo laboratorio. Luego, usted mismo se pondrá manos a la obra en la ingestión de streaming con Kinesis. Buena suerte y diviértete.

¿Qué es la Captura de Datos de Cambios (CDC)?
Estado: Traducido automáticamente del Inglés
Traducido automáticamente del Inglés
Información:
Este elemento incluye contenido que aún no se tradujo a tu idioma preferido.
¿Qué es CDC? 

Supongamos que ha extraído y cargado datos de una base de datos en su sistema de almacenamiento. Al cabo de un tiempo, puede que necesite actualizar los datos almacenados en su sistema de almacenamiento para asegurarse de que están sincronizados con los datos del sistema de origen. Existen dos estrategias para ello:

    Instantáneas completas o carga completa: en este enfoque, cada vez que desee actualizar los datos almacenados en su sistema, ingestará todos los datos de su sistema de origen, sustituyendo los datos antiguos almacenados por los nuevos datos actualizados. Si sus datos son tabulares, la carga completa de los datos significa que elimina todos los datos antiguos de la tabla almacenada y extrae todas las filas de la tabla de origen cada vez que necesite actualizar sus datos almacenados. Se trata de un enfoque sencillo que garantiza la coherencia entre los datos del sistema de origen y los datos almacenados en su canalización de datos. Sin embargo, para datos de gran volumen, puede tardar mucho tiempo en ejecutarse y requerir muchos recursos de procesamiento y memoria. Es más adecuado para casos en los que no es necesario actualizar los datos con frecuencia.

    Carga incremental (diferencial): en este enfoque, sólo se cargan las actualizaciones y los cambios desde la última lectura de los sistemas de origen. Por ejemplo, al cargar actualizaciones de una base de datos de origen, puede utilizar una columna last_updated_at para identificar las filas de datos que se han actualizado desde la última lectura de esta base de datos de origen y, a continuación, cargar sólo los datos actualizados de estas filas identificadas. Aunque este método es más rápido que el de carga completa, especialmente para grandes volúmenes de datos, su aplicación puede requerir una lógica más compleja. Cuando se trabaja con bases de datos, este proceso se conoce como Captura de datos de cambios o (CDC). Según el libro Fundamentos de la ingeniería de datos, "La captura de datos de cambios (CDC) es un método para extraer cada evento de cambio (inserción, actualización, eliminación) que se produce en una base de datos" y ponerlo a disposición de los sistemas posteriores.

Casos de uso de CDC

    CDC le ayuda a sincronizar datos entre diferentes bases de datos, soportando la replicación continua de bases de datos. Por ejemplo, es posible que tenga un sistema PostgreSQL de origen que soporte una aplicación y desee ingerir de forma periódica o continua los cambios de las tablas en un almacén de datos para permitir el análisis basado en los datos más recientes. O si trabaja en una empresa híbrida, podría necesitar usar CDC para capturar cambios en bases de datos locales y aplicar esos cambios a bases de datos en la nube.

    CDC le ayuda a capturar todos los cambios históricos con fines de auditoría y otros fines empresariales. Por ejemplo, algunas empresas están obligadas a mantener información histórica completa de las compras de sus clientes con fines normativos, o para extraer información que permita a las empresas mejorar.

    CDC permite a los microservicios rastrear cualquier cambio en la base de datos de origen. Por ejemplo, considere un microservicio que gestiona pedidos de compra. Cuando se realiza un nuevo pedido, puede utilizar CDC para transmitir información al servicio de envíos y al servicio de atención al cliente.

Dos enfoques de CDC

    Push: Este enfoque requiere que implemente algún tipo de lógica o proceso para capturar los cambios en la base de datos de origen. A continuación, depende de la base de datos de origen para empujar cualquier actualización de datos al sistema de destino cuando algo cambia en el sistema de origen. Este método permite que los sistemas de destino se actualicen con los datos más recientes casi en tiempo real, pero si no se configura correctamente, se corre el riesgo de perder las actualizaciones de datos si los sistemas de destino están inaccesibles cuando los sistemas de origen intentan enviar los cambios.

    Pull: este método requiere que los sistemas de destino sondeen continuamente la base de datos de origen para comprobar si hay cambios y, a continuación, extraigan las actualizaciones de datos cuando se produzcan. Este método suele dar lugar a un retraso antes de que los sistemas de destino introduzcan nuevas actualizaciones de datos, ya que los cambios suelen producirse por lotes entre las solicitudes de extracción.

Modelos de implementación de CDC

Existen varios métodos para que CDC extraiga los cambios de las bases de datos.

    CDC orientado a lotes o basado en consultas (pull-based): En este enfoque, se consulta la propia base de datos para identificar si se ha producido un cambio en los datos. En el caso de las bases de datos relacionales, esto requiere que la base de datos tenga una columna adicional denominada updated_at, last_updated o last_modified que le ayude a encontrar todas las filas actualizadas más allá de un cierto tiempo especificado. Este proceso permite extraer los cambios y actualizar de forma incremental una tabla de destino. Sin embargo, este enfoque puede añadir sobrecarga computacional al sistema de origen, ya que los sistemas de destino tienen que escanear cada fila de la tabla para identificar los últimos valores actualizados.

    CDC continuoo basado en registros (pull-based): En lugar de ejecutar consultas periódicas para obtener los cambios de la tabla como un lote, puede tratar cada actualización de la base de datos como un evento utilizando CDC continuo. Este tipo de CDC se basa en la comprobación del registro de la base de datos. Un registro de base de datos registra cada cambio en la base de datos de forma secuencial (por ejemplo, cada creación, actualización, eliminación) y se utiliza en caso de fallo para restaurar el estado de la base de datos. Puede leer los eventos de este registro (escribiendo su propio código o utilizando una herramienta CDC como Debezium) y enviarlos a una plataforma de streaming, como Apache Kafka. De esta forma, puede capturar los cambios de datos en tiempo real sin incurrir en ninguna sobrecarga computacional ni requerir la necesidad de una columna adicional en las bases de datos de origen.

    CDC basado en desencadenantes (método basado en push): Un desencadenante es una función almacenada que puede configurar para que se ejecute cuando cambie una columna específica. Los disparadores informan al CDC de los cambios en las bases de datos de origen y, de este modo, se libera al CDC de la detección de cambios. Sin embargo, un número excesivo de triggers puede afectar negativamente al rendimiento de escritura de la base de datos de origen.

Herramientas para CDC

Siéntase libre de leer más sobre algunas de las herramientas comunes utilizadas para implementar CDC

    Debezium

AWS DMS

API de conexión Kafka

CDC basado en registros Airbyte


Resumen: Consideraciones generales para elegir las herramientas de ingestión
Estado: Traducido automáticamente del Inglés
Traducido automáticamente del Inglés

A la hora de elegir una herramienta de ingestión para sus sistemas de datos, debe tener en cuenta las características de los datos que va a ingestar, así como la fiabilidad y durabilidad de la herramienta de ingestión.

Características de los datos

Nota: En el libro "Fundamentos de la ingeniería de datos", Joe y Matt se refieren a las características de los datos como la carga útil de los datos, que incluye el tipo de datos (tipo y formato), la forma, el tamaño, el esquema y los tipos de datos, y los metadatos.

    Tipo de datos y estructura: En el curso 1 aprendimos que los datos de los sistemas fuente pueden ser estructurados, no estructurados o semiestructurados. A la hora de decidir cómo ingerir los datos y qué herramienta elegir, es necesario conocer el tipo y la estructura de los datos (por ejemplo, una imagen en formato PNG) para poder identificar la herramienta de ingesta adecuada y las transformaciones que podría ser necesario aplicar más adelante.

    Volumen de datos: En cuanto al Volumen de datos, hay que tener en cuenta dos cosas:

        El tamaño en bytes de los datos existentes que necesita ingestar: En el caso de la ingesta por lotes, debe tener en cuenta el tamaño de los datos históricos que necesita ingerir. ¿Se pueden ingerir todos los datos históricos en un solo lote? Dependiendo de la conexión de red entre el sistema de origen y el sistema de destino, puede ser posible transferir los datos históricos a través de la red, pero si tiene un ancho de banda limitado, puede que tenga que dividir la carga masiva en trozos, lo que reduce efectivamente el tamaño de la carga en subsecciones más pequeñas.En el caso de la ingesta de streaming, debe tener en cuenta el tamaño del mensaje. Debe asegurarse de que la herramienta de ingesta de streaming puede manejar el tamaño máximo esperado del mensaje. Por ejemplo, Amazon Kinesis Data Streams admite un tamaño máximo de mensaje de 1 MB, mientras que Kafka admite por defecto este tamaño máximo, pero se puede configurar para que admita un tamaño máximo de datos de 20 MB o más.

        El tamaño de los datos futuros que puede ingerir con la misma canalización: ¿cómo espera que crezcan los datos? ¿Cuál es el crecimiento diario, mensual o anual de los datos? Considerar el tamaño actual y futuro le ayuda a entender cómo configurar su herramienta y qué coste anticipar para garantizar que su sistema de ingesta satisface las demandas.

    Requisitos de latencia: A la hora de diseñar su canalización, uno de los requisitos de las partes interesadas que debe tener en cuenta es la latencia: ¿a qué velocidad quieren operar con los datos? ¿Cuál es el retraso aceptable? ¿Necesitan extraer información de los datos un día después de que se ingieran, o necesitan información casi en tiempo real? Dicho de otro modo, ¿se trata de un escenario por lotes, en el que los datos deben incorporarse una vez al día, a la semana o al mes? Para cumplir el requisito de latencia, hay que pensar en la rapidez con la que hay que procesar los datos ingeridos una vez que llegan a la canalización y comprender también la rapidez con la que se generan los datos de origen. La velocidad de los datos influirá en las herramientas (batch o streaming) que elija para ingerir y procesar los datos.

    Calidad de los datos: ¿Están los datos de origen en buen estado para su uso posterior inmediato? ¿Qué tratamiento posterior es necesario para servirlos? Dependiendo de los sistemas de origen, los datos pueden estar incompletos o contener información incoherente, duplicados o errores. Si no se espera que los datos estén en buen estado, es posible que tenga que comprobar la calidad de los datos ingeridos para solucionar cualquier problema. Algunas herramientas de ingesta pueden ayudarle a completar los valores que faltan o a detectar/corregir incoherencias o entradas no válidas. Obtendrá más información sobre las comprobaciones de calidad en el próximo curso.

    Cambiosen el esquema:los cambios en el esquema(por ejemplo, añadir una nueva columna, cambiar el tipo de columna, crear una nueva tabla, cambiar el nombre de una columna) se producen con frecuencia en los sistemas fuente y, por lo general, están fuera de su control. Si espera que estos cambios se produzcan con frecuencia, puede que tenga que considerar el uso de herramientas de ingesta que detecten automáticamente los cambios de esquema. Sin embargo, la comunicación entre usted y las partes interesadas es tan importante como la automatización que comprueba los cambios de esquema.

Fiabilidad y durabilidad 

La fiabilidad y la durabilidad son dos aspectos importantes en la fase de ingesta. Fiabilidad significa asegurarse de que los sistemas de ingesta cumplen correctamente su función. Durabilidad significa asegurarse de que los datos no se pierden ni se corrompen. Si diseña un sistema de ingestión fiable, garantizará la durabilidad de los datos ingestados. Por ejemplo, los sistemas de streaming, como los dispositivos IoT, no retienen los eventos indefinidamente, por lo que si no ingiere correctamente sus datos, estos pueden perderse. Asegúrate de comprender las características de los sistemas de origen y las herramientas de ingesta.

Consejos: Evalúe las compensaciones entre el coste de perder datos y la creación de un nivel adecuado de redundancia. Para más información y consideraciones, consulte el capítulo 7 de Fundamentos de la ingeniería de datos.


En el curso anterior, implementó una canalización de streaming similar a la siguiente con un bucket de Kinesis Data Firehose y S3. Se le proporcionó un Kinesis Data Stream que transmite las actividades de los usuarios en línea como eventos o registros. Procesas estos registros para calcular las recomendaciones de productos y usar la instancia de Data Firehose para enviar los registros al bucket de S3 de tu canalización. En el próximo laboratorio, obtendrá más información sobre cómo puede continuar transmitiendo estos registros en proceso y explorar más a fondo Kinesis Data Streams como fuente. El laboratorio consta de dos partes. En la primera parte, trabajará con un Kinesis Data Stream que actúa como un enrutador entre un productor simple y un consumidor simple. En la segunda parte, volverás a trabajar con el escenario del curso 1. Se le proporcionará una transmisión de datos de Kinesis como fuente, pero esta vez usará otras dos transmisiones de datos de Kinesis para continuar transmitiendo los registros en proceso. De cada uno de estos dos nuevos flujos de datos, un Data Firehose tomará los datos y los entregará al bucket S3 correspondiente. Repasemos la primera parte de este laboratorio. Para comprender mejor los componentes de una plataforma de transmisión de eventos, primero creará una transmisión de datos de Kinesis y, a continuación, interactuará con ella desde el punto de vista del productor y el consumidor. Para ello, se le proporcionan dos scripts de Python, consumer desde la CLI y producer desde la CLI. El script de productor representa una aplicación de productor simple que escribe un único registro de datos en Kinesis Data Stream. El registro de datos es una cadena JSON que contiene los detalles de una sesión de usuario, como el identificador de sesión, el número de cliente, la ciudad, el país y el historial de navegación. El script de Python del productor usa Boto3 para interactuar con Kinesis. Puede ejecutar el script desde la terminal, que espera dos argumentos: el nombre de la transmisión de datos de Kinesis y la cadena JSON de un solo registro. En el script del productor, estos dos argumentos se pasan al método PutRecord de Boto3 para escribir el registro en Kinesis Data Stream. El script de consumidor también representa una aplicación de consumidor sencilla que puede ejecutar desde el terminal especificando el nombre de la transmisión de datos de Kinesis. Cuando ejecutes el script de consumidor, recorrerá todos los fragmentos del flujo de datos, extraerá todos los registros de cada fragmento y, a continuación, imprimirá información en el terminal sobre cada registro. Si compruebas el código del script de consumo, puedes ver que también se usa Boto3. En la función PullShards, el consumidor recorre continuamente los fragmentos del flujo de datos, edita los registros con el método getRecords de Boto3 y, a continuación, imprime este texto en la terminal para explicar qué registro se leyó desde qué fragmento y en qué posición dentro del fragmento. En la primera parte del laboratorio, no editarás los guiones para productores y consumidores, pero tendrás la tarea de ejecutar estos guiones. Después de crear Kinesis Data Stream, primero ejecutará el script de consumidor en la terminal. Para ello, primero activará el entorno de JupyterLab, después irá a la carpeta SourceCLI o srcCLI y, por último, ejecutará el script de consumidor como se muestra aquí, proporcionándole el nombre de la transmisión de Kinesis que creó. Observará que no se imprimirá nada en el terminal porque el flujo de datos ahora está vacío. Mantendrás este terminal abierto para que el consumidor siga funcionando y, a continuación, en otro terminal, ejecutarás el script del productor para escribir un registro en el flujo de datos. De nuevo, en la nueva terminal, navegará hasta la carpeta SourceCLI o srcCLI y, a continuación, ejecutará el script del productor especificando el nombre de la transmisión de Kinesis y la cadena JSON que representa el registro. Ahora, si compruebas el primer terminal en el que está funcionando el consumidor, verás que el consumidor ha leído el registro que acabas de enviar al flujo de datos. Cuando hayas terminado con la primera parte del laboratorio, volverás al escenario original de comercio electrónico. Se le proporcionará una transmisión de datos de Kinesis que representa su sistema de origen, por lo que estará del lado del consumidor al ingerir datos de la fuente de transmisión. Implementará una canalización de ETL de streaming, en la que primero aplicará una transformación simple en los registros ingeridos y, a continuación, continuará transmitiendo estos registros en su canalización. Para ello, configurará dos transmisiones de datos de Kinesis. Enviará los registros que corresponden a clientes de EE. UU. a un flujo de datos y los que corresponden a clientes internacionales a otro flujo de datos. Suponiendo que su empresa haya notado que los clientes muestran diferentes comportamientos de compra en función de sus países. Por lo tanto, si se encuentran en los EE. UU., sus actividades en línea deben ser procesadas por un determinado motor de recomendaciones. De lo contrario, sus actividades en línea deben ser procesadas por otro motor de recomendaciones. Para cada uno de estos dos flujos de datos, una manguera de datos recopilará automáticamente los datos y los entregará a su respectivo bucket de S3. En primer lugar, creará los dos flujos de datos, las dos instancias de Firehose y los dos depósitos con Boto3. A continuación, escribirá el código de transformación en el script de consumidor que se le proporciona en esta carpeta ETL. Este script también se puede ejecutar desde el terminal y espera el nombre del flujo de datos de origen y los nombres de los dos flujos de datos de destino. El script contiene esta función de extracción de fragmentos, en la que se le proporciona el código que recorre los fragmentos para extraer los registros. Aquí, una vez que se extrae un registro, tendrá que completar esta parte del código para transformar el registro. La transformación consiste en añadir tres campos, como se muestra aquí. El primer campo, cantidad total de productos, representa la suma de las cantidades de productos que aparecen en el historial de navegación. El segundo campo, en general en el carrito de la compra, representa la suma de las cantidades de productos de los productos que se colocan en el carrito de la compra. Y el tercer campo, el total de productos diferentes, representa el número de productos que aparecen en el historial de navegación. Y, por último, enviará el registro de transformación al flujo de datos correspondiente en función del valor del campo de país. Tras aplicar estas modificaciones al script del consumidor, ejecutará el script en el terminal especificando el nombre del flujo de datos de origen y los nombres de los dos flujos de datos de destino, como se muestra aquí. Al ejecutar este comando, el script del consumidor leerá los registros de la transmisión de datos de origen, los transformará y, a continuación, los enviará a las transmisiones de Kinesis correspondientes. Los índices Firehose de Kinesis entregarán automáticamente los datos a los depósitos de S3. Esta fue una descripción general de las tareas que realizará en este laboratorio, y ahora está listo para comenzar. Cuando termines el laboratorio, acompáñame aquí para ver un resumen rápido de esta semana.
